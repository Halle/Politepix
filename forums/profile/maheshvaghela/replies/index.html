<!DOCTYPE html>
<html lang="en-US">
<head>

<meta charset="UTF-8">

<title>maheshvaghela&#039;s Replies | Politepix</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link href="/wp-content/uploads/omgf/omgf-stylesheet-56/omgf-stylesheet-56.css?ver=1668781124" rel="stylesheet" type="text/css">

<link rel="stylesheet" type="text/css" href="/wp-content/themes/politepix-pixelpress-child-theme/style.css" media="screen">

<link rel="pingback" href="/xmlrpc.php">
<meta name="robots" content="max-image-preview:large">
<script>window._wca = window._wca || [];</script>
<link rel="dns-prefetch" href="//stats.wp.com">
<link rel="dns-prefetch" href="//i0.wp.com">
<link rel="dns-prefetch" href="//c0.wp.com">
<link rel="alternate" type="application/rss+xml" title="Politepix &raquo; Feed" href="http://feeds.feedburner.com/politepixblog">
<link rel="alternate" type="application/rss+xml" title="Politepix &raquo; Comments Feed" href="/comments/feed/">
<script type="text/javascript">
/* <![CDATA[ */
window._wpemojiSettings = {"baseUrl":"https:\/\/s.w.org\/images\/core\/emoji\/15.0.3\/72x72\/","ext":".png","svgUrl":"https:\/\/s.w.org\/images\/core\/emoji\/15.0.3\/svg\/","svgExt":".svg","source":{"concatemoji":"\/wp-includes\/js\/wp-emoji-release.min.js?ver=6.5.2"}};
/*! This file is auto-generated */
!function(i,n){var o,s,e;function c(e){try{var t={supportTests:e,timestamp:(new Date).valueOf()};sessionStorage.setItem(o,JSON.stringify(t))}catch(e){}}function p(e,t,n){e.clearRect(0,0,e.canvas.width,e.canvas.height),e.fillText(t,0,0);var t=new Uint32Array(e.getImageData(0,0,e.canvas.width,e.canvas.height).data),r=(e.clearRect(0,0,e.canvas.width,e.canvas.height),e.fillText(n,0,0),new Uint32Array(e.getImageData(0,0,e.canvas.width,e.canvas.height).data));return t.every(function(e,t){return e===r[t]})}function u(e,t,n){switch(t){case"flag":return n(e,"🏳️‍⚧️","🏳️​⚧️")?!1:!n(e,"🇺🇳","🇺​🇳")&&!n(e,"🏴󠁧󠁢󠁥󠁮󠁧󠁿","🏴​󠁧​󠁢​󠁥​󠁮​󠁧​󠁿");case"emoji":return!n(e,"🐦‍⬛","🐦​⬛")}return!1}function f(e,t,n){var r="undefined"!=typeof WorkerGlobalScope&&self instanceof WorkerGlobalScope?new OffscreenCanvas(300,150):i.createElement("canvas"),a=r.getContext("2d",{willReadFrequently:!0}),o=(a.textBaseline="top",a.font="600 32px Arial",{});return e.forEach(function(e){o[e]=t(a,e,n)}),o}function t(e){var t=i.createElement("script");t.src=e,t.defer=!0,i.head.appendChild(t)}"undefined"!=typeof Promise&&(o="wpEmojiSettingsSupports",s=["flag","emoji"],n.supports={everything:!0,everythingExceptFlag:!0},e=new Promise(function(e){i.addEventListener("DOMContentLoaded",e,{once:!0})}),new Promise(function(t){var n=function(){try{var e=JSON.parse(sessionStorage.getItem(o));if("object"==typeof e&&"number"==typeof e.timestamp&&(new Date).valueOf()<e.timestamp+604800&&"object"==typeof e.supportTests)return e.supportTests}catch(e){}return null}();if(!n){if("undefined"!=typeof Worker&&"undefined"!=typeof OffscreenCanvas&&"undefined"!=typeof URL&&URL.createObjectURL&&"undefined"!=typeof Blob)try{var e="postMessage("+f.toString()+"("+[JSON.stringify(s),u.toString(),p.toString()].join(",")+"));",r=new Blob([e],{type:"text/javascript"}),a=new Worker(URL.createObjectURL(r),{name:"wpTestEmojiSupports"});return void(a.onmessage=function(e){c(n=e.data),a.terminate(),t(n)})}catch(e){}c(n=f(s,u,p))}t(n)}).then(function(e){for(var t in e)n.supports[t]=e[t],n.supports.everything=n.supports.everything&&n.supports[t],"flag"!==t&&(n.supports.everythingExceptFlag=n.supports.everythingExceptFlag&&n.supports[t]);n.supports.everythingExceptFlag=n.supports.everythingExceptFlag&&!n.supports.flag,n.DOMReady=!1,n.readyCallback=function(){n.DOMReady=!0}}).then(function(){return e}).then(function(){var e;n.supports.everything||(n.readyCallback(),(e=n.source||{}).concatemoji?t(e.concatemoji):e.wpemoji&&e.twemoji&&(t(e.twemoji),t(e.wpemoji)))}))}((window,document),window._wpemojiSettings);
/* ]]> */
</script>
<link rel="stylesheet" id="woo-layout-css" href="/wp-content/themes/pixelpress/css/layout.css?ver=6.5.2" type="text/css" media="all">
<link rel="stylesheet" id="woocommerce-css" href="/wp-content/themes/pixelpress/css/woocommerce.css?ver=6.5.2" type="text/css" media="all">
<style id="wp-emoji-styles-inline-css" type="text/css">img.wp-smiley, img.emoji {
		display: inline !important;
		border: none !important;
		box-shadow: none !important;
		height: 1em !important;
		width: 1em !important;
		margin: 0 0.07em !important;
		vertical-align: -0.1em !important;
		background: none !important;
		padding: 0 !important;
	}</style>
<link rel="stylesheet" id="wp-block-library-css" href="https://c0.wp.com/c/6.5.2/wp-includes/css/dist/block-library/style.min.css" type="text/css" media="all">
<style id="wp-block-library-inline-css" type="text/css">.has-text-align-justify{text-align:justify;}</style>
<link rel="stylesheet" id="mediaelement-css" href="https://c0.wp.com/c/6.5.2/wp-includes/js/mediaelement/mediaelementplayer-legacy.min.css" type="text/css" media="all">
<link rel="stylesheet" id="wp-mediaelement-css" href="https://c0.wp.com/c/6.5.2/wp-includes/js/mediaelement/wp-mediaelement.min.css" type="text/css" media="all">
<style id="jetpack-sharing-buttons-style-inline-css" type="text/css">.jetpack-sharing-buttons__services-list{display:flex;flex-direction:row;flex-wrap:wrap;gap:0;list-style-type:none;margin:5px;padding:0}.jetpack-sharing-buttons__services-list.has-small-icon-size{font-size:12px}.jetpack-sharing-buttons__services-list.has-normal-icon-size{font-size:16px}.jetpack-sharing-buttons__services-list.has-large-icon-size{font-size:24px}.jetpack-sharing-buttons__services-list.has-huge-icon-size{font-size:36px}@media print{.jetpack-sharing-buttons__services-list{display:none!important}}.editor-styles-wrapper .wp-block-jetpack-sharing-buttons{gap:0;padding-inline-start:0}ul.jetpack-sharing-buttons__services-list.has-background{padding:1.25em 2.375em}</style>
<style id="classic-theme-styles-inline-css" type="text/css">/*! This file is auto-generated */
.wp-block-button__link{color:#fff;background-color:#32373c;border-radius:9999px;box-shadow:none;text-decoration:none;padding:calc(.667em + 2px) calc(1.333em + 2px);font-size:1.125em}.wp-block-file__button{background:#32373c;color:#fff;text-decoration:none}</style>
<style id="global-styles-inline-css" type="text/css">body{--wp--preset--color--black: #000000;--wp--preset--color--cyan-bluish-gray: #abb8c3;--wp--preset--color--white: #ffffff;--wp--preset--color--pale-pink: #f78da7;--wp--preset--color--vivid-red: #cf2e2e;--wp--preset--color--luminous-vivid-orange: #ff6900;--wp--preset--color--luminous-vivid-amber: #fcb900;--wp--preset--color--light-green-cyan: #7bdcb5;--wp--preset--color--vivid-green-cyan: #00d084;--wp--preset--color--pale-cyan-blue: #8ed1fc;--wp--preset--color--vivid-cyan-blue: #0693e3;--wp--preset--color--vivid-purple: #9b51e0;--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple: linear-gradient(135deg,rgba(6,147,227,1) 0%,rgb(155,81,224) 100%);--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan: linear-gradient(135deg,rgb(122,220,180) 0%,rgb(0,208,130) 100%);--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange: linear-gradient(135deg,rgba(252,185,0,1) 0%,rgba(255,105,0,1) 100%);--wp--preset--gradient--luminous-vivid-orange-to-vivid-red: linear-gradient(135deg,rgba(255,105,0,1) 0%,rgb(207,46,46) 100%);--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray: linear-gradient(135deg,rgb(238,238,238) 0%,rgb(169,184,195) 100%);--wp--preset--gradient--cool-to-warm-spectrum: linear-gradient(135deg,rgb(74,234,220) 0%,rgb(151,120,209) 20%,rgb(207,42,186) 40%,rgb(238,44,130) 60%,rgb(251,105,98) 80%,rgb(254,248,76) 100%);--wp--preset--gradient--blush-light-purple: linear-gradient(135deg,rgb(255,206,236) 0%,rgb(152,150,240) 100%);--wp--preset--gradient--blush-bordeaux: linear-gradient(135deg,rgb(254,205,165) 0%,rgb(254,45,45) 50%,rgb(107,0,62) 100%);--wp--preset--gradient--luminous-dusk: linear-gradient(135deg,rgb(255,203,112) 0%,rgb(199,81,192) 50%,rgb(65,88,208) 100%);--wp--preset--gradient--pale-ocean: linear-gradient(135deg,rgb(255,245,203) 0%,rgb(182,227,212) 50%,rgb(51,167,181) 100%);--wp--preset--gradient--electric-grass: linear-gradient(135deg,rgb(202,248,128) 0%,rgb(113,206,126) 100%);--wp--preset--gradient--midnight: linear-gradient(135deg,rgb(2,3,129) 0%,rgb(40,116,252) 100%);--wp--preset--font-size--small: 13px;--wp--preset--font-size--medium: 20px;--wp--preset--font-size--large: 36px;--wp--preset--font-size--x-large: 42px;--wp--preset--spacing--20: 0.44rem;--wp--preset--spacing--30: 0.67rem;--wp--preset--spacing--40: 1rem;--wp--preset--spacing--50: 1.5rem;--wp--preset--spacing--60: 2.25rem;--wp--preset--spacing--70: 3.38rem;--wp--preset--spacing--80: 5.06rem;--wp--preset--shadow--natural: 6px 6px 9px rgba(0, 0, 0, 0.2);--wp--preset--shadow--deep: 12px 12px 50px rgba(0, 0, 0, 0.4);--wp--preset--shadow--sharp: 6px 6px 0px rgba(0, 0, 0, 0.2);--wp--preset--shadow--outlined: 6px 6px 0px -3px rgba(255, 255, 255, 1), 6px 6px rgba(0, 0, 0, 1);--wp--preset--shadow--crisp: 6px 6px 0px rgba(0, 0, 0, 1);}:where(.is-layout-flex){gap: 0.5em;}:where(.is-layout-grid){gap: 0.5em;}body .is-layout-flow > .alignleft{float: left;margin-inline-start: 0;margin-inline-end: 2em;}body .is-layout-flow > .alignright{float: right;margin-inline-start: 2em;margin-inline-end: 0;}body .is-layout-flow > .aligncenter{margin-left: auto !important;margin-right: auto !important;}body .is-layout-constrained > .alignleft{float: left;margin-inline-start: 0;margin-inline-end: 2em;}body .is-layout-constrained > .alignright{float: right;margin-inline-start: 2em;margin-inline-end: 0;}body .is-layout-constrained > .aligncenter{margin-left: auto !important;margin-right: auto !important;}body .is-layout-constrained > :where(:not(.alignleft):not(.alignright):not(.alignfull)){max-width: var(--wp--style--global--content-size);margin-left: auto !important;margin-right: auto !important;}body .is-layout-constrained > .alignwide{max-width: var(--wp--style--global--wide-size);}body .is-layout-flex{display: flex;}body .is-layout-flex{flex-wrap: wrap;align-items: center;}body .is-layout-flex > *{margin: 0;}body .is-layout-grid{display: grid;}body .is-layout-grid > *{margin: 0;}:where(.wp-block-columns.is-layout-flex){gap: 2em;}:where(.wp-block-columns.is-layout-grid){gap: 2em;}:where(.wp-block-post-template.is-layout-flex){gap: 1.25em;}:where(.wp-block-post-template.is-layout-grid){gap: 1.25em;}.has-black-color{color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-color{color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-color{color: var(--wp--preset--color--white) !important;}.has-pale-pink-color{color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-color{color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-color{color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-color{color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-color{color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-color{color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-color{color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-color{color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-color{color: var(--wp--preset--color--vivid-purple) !important;}.has-black-background-color{background-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-background-color{background-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-background-color{background-color: var(--wp--preset--color--white) !important;}.has-pale-pink-background-color{background-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-background-color{background-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-background-color{background-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-background-color{background-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-background-color{background-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-background-color{background-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-background-color{background-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-background-color{background-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-background-color{background-color: var(--wp--preset--color--vivid-purple) !important;}.has-black-border-color{border-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-border-color{border-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-border-color{border-color: var(--wp--preset--color--white) !important;}.has-pale-pink-border-color{border-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-border-color{border-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-border-color{border-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-border-color{border-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-border-color{border-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-border-color{border-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-border-color{border-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-border-color{border-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-border-color{border-color: var(--wp--preset--color--vivid-purple) !important;}.has-vivid-cyan-blue-to-vivid-purple-gradient-background{background: var(--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple) !important;}.has-light-green-cyan-to-vivid-green-cyan-gradient-background{background: var(--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan) !important;}.has-luminous-vivid-amber-to-luminous-vivid-orange-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange) !important;}.has-luminous-vivid-orange-to-vivid-red-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-orange-to-vivid-red) !important;}.has-very-light-gray-to-cyan-bluish-gray-gradient-background{background: var(--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray) !important;}.has-cool-to-warm-spectrum-gradient-background{background: var(--wp--preset--gradient--cool-to-warm-spectrum) !important;}.has-blush-light-purple-gradient-background{background: var(--wp--preset--gradient--blush-light-purple) !important;}.has-blush-bordeaux-gradient-background{background: var(--wp--preset--gradient--blush-bordeaux) !important;}.has-luminous-dusk-gradient-background{background: var(--wp--preset--gradient--luminous-dusk) !important;}.has-pale-ocean-gradient-background{background: var(--wp--preset--gradient--pale-ocean) !important;}.has-electric-grass-gradient-background{background: var(--wp--preset--gradient--electric-grass) !important;}.has-midnight-gradient-background{background: var(--wp--preset--gradient--midnight) !important;}.has-small-font-size{font-size: var(--wp--preset--font-size--small) !important;}.has-medium-font-size{font-size: var(--wp--preset--font-size--medium) !important;}.has-large-font-size{font-size: var(--wp--preset--font-size--large) !important;}.has-x-large-font-size{font-size: var(--wp--preset--font-size--x-large) !important;}
.wp-block-navigation a:where(:not(.wp-element-button)){color: inherit;}
:where(.wp-block-post-template.is-layout-flex){gap: 1.25em;}:where(.wp-block-post-template.is-layout-grid){gap: 1.25em;}
:where(.wp-block-columns.is-layout-flex){gap: 2em;}:where(.wp-block-columns.is-layout-grid){gap: 2em;}
.wp-block-pullquote{font-size: 1.5em;line-height: 1.6;}</style>
<link rel="stylesheet" id="bbp-default-css" href="/wp-content/plugins/bbpress/templates/default/css/bbpress.min.css?ver=2.6.9" type="text/css" media="all">
<link rel="stylesheet" id="currency_converter_styles-css" href="/wp-content/plugins/woocommerce-currency-converter-widget/assets/css/converter.css?ver=2.2.2" type="text/css" media="all">
<style id="woocommerce-inline-inline-css" type="text/css">.woocommerce form .form-row .required { visibility: visible; }</style>
<link rel="stylesheet" id="cmplz-general-css" href="/wp-content/plugins/complianz-gdpr/assets/css/cookieblocker.min.css?ver=1710283454" type="text/css" media="all">
<link rel="stylesheet" id="jetpack_css-css" href="https://c0.wp.com/p/jetpack/13.3.1/css/jetpack.css" type="text/css" media="all">
<link rel="stylesheet" id="prettyPhoto-css" href="/wp-content/themes/pixelpress/includes/css/prettyPhoto.css?ver=6.5.2" type="text/css" media="all">
<script type="text/template" id="tmpl-variation-template">
	<div class="woocommerce-variation-description">{{{ data.variation.variation_description }}}<\/div>
	<div class="woocommerce-variation-price">{{{ data.variation.price_html }}}<\/div>
	<div class="woocommerce-variation-availability">{{{ data.variation.availability_html }}}<\/div>
</script>
<script type="text/template" id="tmpl-unavailable-variation-template">
	<p>Sorry, this product is unavailable. Please choose a different combination.<\/p>
</script>
<script type="text/javascript" src="https://c0.wp.com/c/6.5.2/wp-includes/js/dist/vendor/wp-polyfill-inert.min.js" id="wp-polyfill-inert-js"></script>
<script type="text/javascript" src="https://c0.wp.com/c/6.5.2/wp-includes/js/dist/vendor/regenerator-runtime.min.js" id="regenerator-runtime-js"></script>
<script type="text/javascript" src="https://c0.wp.com/c/6.5.2/wp-includes/js/dist/vendor/wp-polyfill.min.js" id="wp-polyfill-js"></script>
<script type="text/javascript" src="https://c0.wp.com/c/6.5.2/wp-includes/js/dist/hooks.min.js" id="wp-hooks-js"></script>
<script data-service="jetpack-statistics" data-category="statistics" type="text/plain" data-cmplz-src="https://stats.wp.com/w.js?ver=202417" id="woo-tracks-js"></script>
<script type="text/javascript" src="https://c0.wp.com/c/6.5.2/wp-includes/js/jquery/jquery.min.js" id="jquery-core-js"></script>
<script type="text/javascript" src="https://c0.wp.com/c/6.5.2/wp-includes/js/jquery/jquery-migrate.min.js" id="jquery-migrate-js"></script>
<script type="text/javascript" src="https://c0.wp.com/c/6.5.2/wp-includes/js/underscore.min.js" id="underscore-js"></script>
<script type="text/javascript" id="wp-util-js-extra">
/* <![CDATA[ */
var _wpUtilSettings = {"ajax":{"url":"\/wp-admin\/admin-ajax.php"}};
/* ]]> */
</script>
<script type="text/javascript" src="https://c0.wp.com/c/6.5.2/wp-includes/js/wp-util.min.js" id="wp-util-js"></script>
<script type="text/javascript" src="https://c0.wp.com/p/woocommerce/8.8.2/assets/js/jquery-blockui/jquery.blockUI.min.js" id="jquery-blockui-js" defer data-wp-strategy="defer"></script>
<script type="text/javascript" id="wc-add-to-cart-variation-js-extra">
/* <![CDATA[ */
var wc_add_to_cart_variation_params = {"wc_ajax_url":"\/?wc-ajax=%%endpoint%%","i18n_no_matching_variations_text":"Sorry, no products matched your selection. Please choose a different combination.","i18n_make_a_selection_text":"Please select some product options before adding this product to your cart.","i18n_unavailable_text":"Sorry, this product is unavailable. Please choose a different combination."};
/* ]]> */
</script>
<script type="text/javascript" src="https://c0.wp.com/p/woocommerce/8.8.2/assets/js/frontend/add-to-cart-variation.min.js" id="wc-add-to-cart-variation-js" defer data-wp-strategy="defer"></script>
<script type="text/javascript" src="https://c0.wp.com/p/woocommerce/8.8.2/assets/js/jquery-cookie/jquery.cookie.min.js" id="jquery-cookie-js" defer data-wp-strategy="defer"></script>
<script data-service="jetpack-statistics" data-category="statistics" type="text/plain" data-cmplz-src="https://stats.wp.com/s-202417.js" id="woocommerce-analytics-js" defer data-wp-strategy="defer"></script>
<script type="text/javascript" src="/wp-content/themes/pixelpress/includes/js/third-party.js?ver=6.5.2" id="third party-js"></script>
<script type="text/javascript" src="https://c0.wp.com/p/woocommerce/8.8.2/assets/js/prettyPhoto/jquery.prettyPhoto.min.js" id="prettyPhoto-js" data-wp-strategy="defer"></script>
<script type="text/javascript" src="/wp-content/themes/pixelpress/includes/js/general.js?ver=6.5.2" id="general-js"></script>
<script type="text/javascript" src="/wp-content/themes/pixelpress/includes/js/uniform.js?ver=6.5.2" id="uniform-js"></script>
<link rel="https://api.w.org/" href="/wp-json/">
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="/xmlrpc.php?rsd">
<meta name="generator" content="WordPress 6.5.2">
<meta name="generator" content="WooCommerce 8.8.2">
	<style>img#wpstats{display:none}</style>
					<style>.cmplz-hidden {
					display: none !important;
				}</style>		<script>( function() {
				window.onpageshow = function( event ) {
					// Defined window.wpforms means that a form exists on a page.
					// If so and back/forward button has been clicked,
					// force reload a page to prevent the submit button state stuck.
					if ( typeof window.wpforms !== 'undefined' && event.persisted ) {
						window.location.reload();
					}
				};
			}() );</script>
		
<!-- Theme version -->
<meta name="generator" content="PixelPress Politepix 1.0">
<meta name="generator" content="PixelPress 1.5.4">
<meta name="generator" content="WooFramework 6.2.9">

<!-- Always force latest IE rendering engine (even in intranet) & Chrome Frame -->
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

<!--  Mobile viewport scale | Disable user zooming as the layout is optimised -->
<meta content="initial-scale=1.0; maximum-scale=1.0; user-scalable=no" name="viewport">
		<!--[if lt IE 9]>
			<script src="https://html5shim.googlecode.com/svn/trunk/html5.js"></script>
		<![endif]-->
			<noscript><style>.woocommerce-product-gallery{ opacity: 1 !important; }</style></noscript>
	
<!-- Google Webfonts -->
<link href="/wp-content/uploads/omgf/omgf-stylesheet-165/omgf-stylesheet-165.css?ver=1668781124" rel="stylesheet" type="text/css">

<!-- Alt Stylesheet -->
<link href="/wp-content/themes/pixelpress/styles/default.css" rel="stylesheet" type="text/css">

<!-- Custom Favicon -->
<link rel="shortcut icon" href="/wp-content/uploads/favicon.png">

<!-- Woo Shortcodes CSS -->
<link href="/wp-content/themes/pixelpress/functions/css/shortcodes.css" rel="stylesheet" type="text/css">

<!-- Custom Stylesheet -->
<link href="/wp-content/themes/pixelpress/custom.css" rel="stylesheet" type="text/css">

<!-- Custom Stylesheet In Child Theme -->
<link href="/wp-content/themes/politepix-pixelpress-child-theme/custom.css" rel="stylesheet" type="text/css">

</head>

<body data-cmplz="1" class="bbp-user-page single singular bbpress no-js theme-pixelpress woocommerce-no-js unknown alt-style-default has-lightbox layout-left-content">

<div id="wrapper">

	    
        
    <div id="header-wrap">

		<header id="header" class="col-full">
		
			<div id="logo" class="fl">
												    <a id="logo" href="/" title="iOS Frameworks for speech recognition, text to speech and more">
				    	<img src="/wp-content/uploads/logo1.png" alt="Politepix">
				    </a>
			    			    
			    <hgroup>
			        
					<h1 class="site-title"><a href="/">Politepix</a></h1>
					<h2 class="site-description">iOS Frameworks for speech recognition, text to speech and more</h2>
					<h3 class="nav-toggle"><a href="#navigation">Navigation</a></h3>
				      	
				</hgroup>
			</div>
<!-- /#logo -->		
	        
	        	
	        <div id="header-right" class="fr">

				<nav id="navigation" role="navigation">
					
					<ul id="main-nav" class="nav fl">
<li id="menu-item-1175" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-1175">
<a href="/openears/">OpenEars</a>
<ul class="sub-menu">
	<li id="menu-item-11108" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-11108"><a href="/openears/tutorial/">OpenEars Tutorials</a></li>
	<li id="menu-item-1171" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1171"><a href="/openears/support/">OpenEars FAQ/Support</a></li>
	<li id="menu-item-1189" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-1189"><a href="/forums/forum/openearsforum/">OpenEars Support Forum</a></li>
	<li id="menu-item-14947" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-14947"><a href="/openearsplatform/">About the OpenEars Platform</a></li>
</ul>
</li>
<li id="menu-item-11706" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-11706">
<a href="/neatspeech">NeatSpeech</a>
<ul class="sub-menu">
	<li id="menu-item-11707" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-11707"><a href="/forums/forum/openears-plugins/">NeatSpeech support forum</a></li>
</ul>
</li>
<li id="menu-item-9871" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-9871">
<a href="/rapidears/">RapidEars</a>
<ul class="sub-menu">
	<li id="menu-item-10389" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-10389"><a href="/forums/forum/openears-plugins">RapidEars Support Forum</a></li>
</ul>
</li>
<li id="menu-item-11030" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-11030">
<a href="/rejecto/">Rejecto</a>
<ul class="sub-menu">
	<li id="menu-item-11031" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-11031"><a href="/forums/forum/openears-plugins/">Rejecto Support Forum</a></li>
</ul>
</li>
<li id="menu-item-13071" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-13071"><a href="/savethatwave/">SaveThatWave</a></li>
<li id="menu-item-1021000" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-1021000">
<a href="/ruleorama/">RuleORama</a>
<ul class="sub-menu">
	<li id="menu-item-9002" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-9002"><a href="/forums/forum/openears-plugins/">RuleORama Free Support Forum</a></li>
</ul>
</li>
<li id="menu-item-1178" class="menu-item menu-item-type-post_type menu-item-object-page current_page_parent menu-item-1178"><a title="Blog" href="/blog/">Blog</a></li>
<li id="menu-item-1015870" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-1015870">
<a href="/shop">Shop</a>
<ul class="sub-menu">
	<li id="menu-item-8499" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-8499"><a href="/shop">View all products</a></li>
	<li id="menu-item-8486" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-8486"><a href="/cart/">Cart</a></li>
	<li id="menu-item-8498" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-8498"><a href="/order-tracking/">Track your order</a></li>
	<li id="menu-item-8539" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-8539"><a href="/termsandconditions/">AGB/Terms and Conditions</a></li>
</ul>
</li>
</ul>			        
			        			        <ul class="mini-cart">
						<li>
							<a href="/cart/" title="View your shopping cart" class="cart-parent">
								<span> 
							<span class="woocommerce-Price-amount amount"><bdi><span class="woocommerce-Price-currencySymbol">&euro;</span>0.00</bdi></span><mark>0</mark>							</span>
							</a>
							<ul class="cart_list"><li class="empty">No products in the cart.</li></ul>						</li>
					</ul>
			      				      	
				</nav><!-- /#navigation -->

				
<div id="header-social" class="social fr">
				<a href="http://feeds.feedburner.com/politepixblog" class="subscribe" title="RSS"></a>

				<a href="http://www.twitter.com/politepix" class="twitter" title="Follow me on Twitter"></a>

		</div>
<!-- /.social -->
				

			</div>
<!-- /#header-right -->
			
					
		</header><!-- /#header -->
	
	</div>
<!-- /#header-wrap -->
	
	
	       
    <div id="content" class="page col-full">
    
    	    	
		<section id="main" class="col-left"> 			

                                                                   
            <article class="post-0  type- status-publish hentry">
				
				<header>
			    	<h1>maheshvaghela</h1>
				</header>
				
                <section class="entry">
                	
<div id="bbpress-forums" class="bbpress-wrapper">

	
	
	<div id="bbp-user-wrapper">

		
<div id="bbp-single-user-details">
	<div id="bbp-user-avatar">
		<span class="vcard">
			<a class="url fn n" href="/forums/profile/maheshvaghela/" title="maheshvaghela" rel="me">
							</a>
		</span>
	</div>

	
	<div id="bbp-user-navigation">
		<ul>
			<li class="">
				<span class="vcard bbp-user-profile-link">
					<a class="url fn n" href="/forums/profile/maheshvaghela/" title="maheshvaghela&#039;s Profile" rel="me">Profile</a>
				</span>
			</li>

			<li class="">
				<span class="bbp-user-topics-created-link">
					<a href="/forums/profile/maheshvaghela/topics/" title="maheshvaghela&#039;s Topics Started">Topics Started</a>
				</span>
			</li>

			<li class="current">
				<span class="bbp-user-replies-created-link">
					<a href="/forums/profile/maheshvaghela/replies/" title="maheshvaghela&#039;s Replies Created">Replies Created</a>
				</span>
			</li>

							<li class="">
					<span class="bbp-user-engagements-created-link">
						<a href="/forums/profile/maheshvaghela/engagements/" title="maheshvaghela&#039;s Engagements">Engagements</a>
					</span>
				</li>
			
							<li class="">
					<span class="bbp-user-favorites-link">
						<a href="/forums/profile/maheshvaghela/favorites/" title="maheshvaghela&#039;s Favorites">Favorites</a>
					</span>
				</li>
			
			
		</ul>

		
	</div>
</div>


		<div id="bbp-user-body">
															
<div id="bbp-user-replies-created" class="bbp-user-replies-created">

	
	<div class="bbp-search-form">
		<form role="search" method="get" id="bbp-reply-search-form">
			<div>
				<label class="screen-reader-text hidden" for="rs">Search replies:</label>
				<input type="text" value="" name="rs" id="rs">
				<input class="button" type="submit" id="bbp_search_submit" value="Search">
			</div>
		</form>
	</div>


	<h2 class="entry-title">Forum Replies Created</h2>
	<div class="bbp-user-section">

		
			
<div class="bbp-pagination">
	<div class="bbp-pagination-count">Viewing 8 posts - 1 through 8 (of 8 total)</div>
	<div class="bbp-pagination-links"></div>
</div>


			<div class="burma">Advertisement: <a href="/rejecto">&ldquo;Don't want OpenEars&#8482; to guess one of your vocabulary words when it hears an unknown word? Rejecto can help!&rdquo;</a>
</div>
<p>
</p>
<ul id="topic-0-replies" class="forums bbp-replies">

	<li class="bbp-header">
		<div class="bbp-reply-author">Author</div>
<!-- .bbp-reply-author -->
		<div class="bbp-reply-content">Posts</div>
<!-- .bbp-reply-content -->
	</li>
<!-- .bbp-header -->

	<li class="bbp-body">

		
			
				
<div id="post-1024456" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">January 20, 2015 at 12:09 pm</span>

		
			<span class="bbp-header">
				in reply to: 				<a class="bbp-topic-permalink" href="/forums/topic/openears-detect-the-word-without-speaking-it/">openEars detect the word without speaking it</a>
			</span>

		
		<a href="/forums/topic/openears-detect-the-word-without-speaking-it/#post-1024456" class="bbp-reply-permalink">#1024456</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1024456 -->

<div class="loop-item-0 user-id-1752 bbp-parent-forum-3654 bbp-parent-topic-1024409 bbp-reply-position-18 odd topic-author  post-1024456 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/maheshvaghela/" title="View maheshvaghela&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">maheshvaghela</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>It is immediate after uncommented two lines you can see. Before importing words array. And yes have written set vadThreshold after startListening but in first time. when you told me to uncomment that two lines then I have written set vadThreshold immediate after it. Then I fill Array. Then startListening is called.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1024454" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">January 20, 2015 at 11:52 am</span>

		
			<span class="bbp-header">
				in reply to: 				<a class="bbp-topic-permalink" href="/forums/topic/openears-detect-the-word-without-speaking-it/">openEars detect the word without speaking it</a>
			</span>

		
		<a href="/forums/topic/openears-detect-the-word-without-speaking-it/#post-1024454" class="bbp-reply-permalink">#1024454</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1024454 -->

<div class="loop-item-1 user-id-1752 bbp-parent-forum-3654 bbp-parent-topic-1024409 bbp-reply-position-16 even topic-author  post-1024454 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/maheshvaghela/" title="View maheshvaghela&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">maheshvaghela</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Ok, I have updated version to 2.03 and also uncomment that two lines.<br>
This is a log information.</p>
<pre><code>2015-01-20 16:12:19.509 OpenEarsSampleApp[5469:2180532] Starting OpenEars logging for OpenEars version 2.03 on 32-bit device (or build): iPad running iOS version: 8.000000
2015-01-20 16:12:19.512 OpenEarsSampleApp[5469:2180532] Creating shared instance of OEPocketsphinxController
2015-01-20 16:12:19.669 OpenEarsSampleApp[5469:2180532] Starting dynamic language model generation

INFO: cmd_ln.c(702): Parsing command line:
sphinx_lm_convert \
	-i /var/mobile/Containers/Data/Application/6A690B2A-2799-46D9-BF15-9037FDC56EF7/Library/Caches/FirstOpenEarsDynamicLanguageModel.arpa \
	-o /var/mobile/Containers/Data/Application/6A690B2A-2799-46D9-BF15-9037FDC56EF7/Library/Caches/FirstOpenEarsDynamicLanguageModel.DMP 

Current configuration:
[NAME]		[DEFLT]	[VALUE]
-case			
-debug			0
-help		no	no
-i			/var/mobile/Containers/Data/Application/6A690B2A-2799-46D9-BF15-9037FDC56EF7/Library/Caches/FirstOpenEarsDynamicLanguageModel.arpa
-ienc			
-ifmt			
-logbase	1.0001	1.000100e+00
-mmap		no	no
-o			/var/mobile/Containers/Data/Application/6A690B2A-2799-46D9-BF15-9037FDC56EF7/Library/Caches/FirstOpenEarsDynamicLanguageModel.DMP
-oenc		utf8	utf8
-ofmt			

INFO: ngram_model_arpa.c(504): ngrams 1=22, 2=46, 3=32
INFO: ngram_model_arpa.c(137): Reading unigrams
INFO: ngram_model_arpa.c(543):       22 = #unigrams created
INFO: ngram_model_arpa.c(197): Reading bigrams
INFO: ngram_model_arpa.c(561):       46 = #bigrams created
INFO: ngram_model_arpa.c(562):        6 = #prob2 entries
INFO: ngram_model_arpa.c(570):        5 = #bo_wt2 entries
INFO: ngram_model_arpa.c(294): Reading trigrams
INFO: ngram_model_arpa.c(583):       32 = #trigrams created
INFO: ngram_model_arpa.c(584):        3 = #prob3 entries
INFO: ngram_model_dmp.c(518): Building DMP model...
INFO: ngram_model_dmp.c(548):       22 = #unigrams created
INFO: ngram_model_dmp.c(649):       46 = #bigrams created
INFO: ngram_model_dmp.c(650):        6 = #prob2 entries
INFO: ngram_model_dmp.c(657):        5 = #bo_wt2 entries
INFO: ngram_model_dmp.c(661):       32 = #trigrams created
INFO: ngram_model_dmp.c(662):        3 = #prob3 entries
2015-01-20 16:12:19.745 OpenEarsSampleApp[5469:2180532] Done creating language model with CMUCLMTK in 0.075357 seconds.
2015-01-20 16:12:19.912 OpenEarsSampleApp[5469:2180532] I&#039;m done running performDictionaryLookup and it took 0.134176 seconds
2015-01-20 16:12:19.922 OpenEarsSampleApp[5469:2180532] I&#039;m done running dynamic language model generation and it took 0.304282 seconds
2015-01-20 16:12:19.929 OpenEarsSampleApp[5469:2180532] Starting dynamic language model generation

INFO: cmd_ln.c(702): Parsing command line:
sphinx_lm_convert \
	-i /var/mobile/Containers/Data/Application/6A690B2A-2799-46D9-BF15-9037FDC56EF7/Library/Caches/SecondOpenEarsDynamicLanguageModel.arpa \
	-o /var/mobile/Containers/Data/Application/6A690B2A-2799-46D9-BF15-9037FDC56EF7/Library/Caches/SecondOpenEarsDynamicLanguageModel.DMP 

Current configuration:
[NAME]		[DEFLT]	[VALUE]
-case			
-debug			0
-help		no	no
-i			/var/mobile/Containers/Data/Application/6A690B2A-2799-46D9-BF15-9037FDC56EF7/Library/Caches/SecondOpenEarsDynamicLanguageModel.arpa
-ienc			
-ifmt			
-logbase	1.0001	1.000100e+00
-mmap		no	no
-o			/var/mobile/Containers/Data/Application/6A690B2A-2799-46D9-BF15-9037FDC56EF7/Library/Caches/SecondOpenEarsDynamicLanguageModel.DMP
-oenc		utf8	utf8
-ofmt			

INFO: ngram_model_arpa.c(504): ngrams 1=12, 2=19, 3=10
INFO: ngram_model_arpa.c(137): Reading unigrams
INFO: ngram_model_arpa.c(543):       12 = #unigrams created
INFO: ngram_model_arpa.c(197): Reading bigrams
INFO: ngram_model_arpa.c(561):       19 = #bigrams created
INFO: ngram_model_arpa.c(562):        3 = #prob2 entries
INFO: ngram_model_arpa.c(570):        3 = #bo_wt2 entries
INFO: ngram_model_arpa.c(294): Reading trigrams
INFO: ngram_model_arpa.c(583):       10 = #trigrams created
INFO: ngram_model_arpa.c(584):        2 = #prob3 entries
INFO: ngram_model_dmp.c(518): Building DMP model...
INFO: ngram_model_dmp.c(548):       12 = #unigrams created
INFO: ngram_model_dmp.c(649):       19 = #bigrams created
INFO: ngram_model_dmp.c(650):        3 = #prob2 entries
INFO: ngram_model_dmp.c(657):        3 = #bo_wt2 entries
INFO: ngram_model_dmp.c(661):       10 = #trigrams created
INFO: ngram_model_dmp.c(662):        2 = #prob3 entries
2015-01-20 16:12:20.005 OpenEarsSampleApp[5469:2180532] Done creating language model with CMUCLMTK in 0.074512 seconds.
2015-01-20 16:12:20.178 OpenEarsSampleApp[5469:2180532] The word QUIDNUNC was not found in the dictionary /private/var/mobile/Containers/Bundle/Application/03075EFA-2B35-4613-BBF0-2E3BD17986E8/OpenEarsSampleApp.app/AcousticModelEnglish.bundle/LanguageModelGeneratorLookupList.text/LanguageModelGeneratorLookupList.text.
2015-01-20 16:12:20.179 OpenEarsSampleApp[5469:2180532] Now using the fallback method to look up the word QUIDNUNC
2015-01-20 16:12:20.179 OpenEarsSampleApp[5469:2180532] If this is happening more frequently than you would expect, the most likely cause for it is since you are using the English phonetic lookup dictionary is that your words are not in English or aren&#039;t dictionary words, or that you are submitting the words in lowercase when they need to be entirely written in uppercase. This can also happen if you submit words with punctuation attached – consider removing punctuation from language models or grammars you create before submitting them.
2015-01-20 16:12:20.181 OpenEarsSampleApp[5469:2180532] Using convertGraphemes for the word or phrase QUIDNUNC which doesn&#039;t appear in the dictionary
2015-01-20 16:12:20.239 OpenEarsSampleApp[5469:2180532] I&#039;m done running performDictionaryLookup and it took 0.204421 seconds
2015-01-20 16:12:20.249 OpenEarsSampleApp[5469:2180532] I&#039;m done running dynamic language model generation and it took 0.325570 seconds
2015-01-20 16:12:20.250 OpenEarsSampleApp[5469:2180532] Attempting to start listening session from startListeningWithLanguageModelAtPath:
2015-01-20 16:12:20.259 OpenEarsSampleApp[5469:2180532] User gave mic permission for this app.
2015-01-20 16:12:20.260 OpenEarsSampleApp[5469:2180532] Valid setSecondsOfSilence value of 0.200000 will be used.
2015-01-20 16:12:20.262 OpenEarsSampleApp[5469:2180532] Successfully started listening session from startListeningWithLanguageModelAtPath:
2015-01-20 16:12:20.262 OpenEarsSampleApp[5469:2180540] Starting listening.
2015-01-20 16:12:20.263 OpenEarsSampleApp[5469:2180540] about to set up audio session
2015-01-20 16:12:20.340 OpenEarsSampleApp[5469:2180549] Audio route has changed for the following reason:
2015-01-20 16:12:20.355 OpenEarsSampleApp[5469:2180549] There was a category change. The new category is AVAudioSessionCategoryPlayAndRecord
2015-01-20 16:12:20.724 OpenEarsSampleApp[5469:2180540] done starting audio unit
INFO: cmd_ln.c(702): Parsing command line:
\
	-lm /var/mobile/Containers/Data/Application/6A690B2A-2799-46D9-BF15-9037FDC56EF7/Library/Caches/FirstOpenEarsDynamicLanguageModel.DMP \
	-vad_prespeech 10 \
	-vad_postspeech 20 \
	-vad_threshold 3.000000 \
	-remove_noise yes \
	-remove_silence yes \
	-bestpath yes \
	-lw 6.500000 \
	-dict /var/mobile/Containers/Data/Application/6A690B2A-2799-46D9-BF15-9037FDC56EF7/Library/Caches/FirstOpenEarsDynamicLanguageModel.dic \
	-hmm /private/var/mobile/Containers/Bundle/Application/03075EFA-2B35-4613-BBF0-2E3BD17986E8/OpenEarsSampleApp.app/AcousticModelEnglish.bundle 

Current configuration:
[NAME]		[DEFLT]		[VALUE]
-agc		none		none
-agcthresh	2.0		2.000000e+00
-allphone			
-allphone_ci	no		no
-alpha		0.97		9.700000e-01
-argfile			
-ascale		20.0		2.000000e+01
-aw		1		1
-backtrace	no		no
-beam		1e-48		1.000000e-48
-bestpath	yes		yes
-bestpathlw	9.5		9.500000e+00
-bghist		no		no
-ceplen		13		13
-cmn		current		current
-cmninit	8.0		8.0
-compallsen	no		no
-debug				0
-dict				/var/mobile/Containers/Data/Application/6A690B2A-2799-46D9-BF15-9037FDC56EF7/Library/Caches/FirstOpenEarsDynamicLanguageModel.dic
-dictcase	no		no
-dither		no		no
-doublebw	no		no
-ds		1		1
-fdict				
-feat		1s_c_d_dd	1s_c_d_dd
-featparams			
-fillprob	1e-8		1.000000e-08
-frate		100		100
-fsg				
-fsgusealtpron	yes		yes
-fsgusefiller	yes		yes
-fwdflat	yes		yes
-fwdflatbeam	1e-64		1.000000e-64
-fwdflatefwid	4		4
-fwdflatlw	8.5		8.500000e+00
-fwdflatsfwin	25		25
-fwdflatwbeam	7e-29		7.000000e-29
-fwdtree	yes		yes
-hmm				/private/var/mobile/Containers/Bundle/Application/03075EFA-2B35-4613-BBF0-2E3BD17986E8/OpenEarsSampleApp.app/AcousticModelEnglish.bundle
-input_endian	little		little
-jsgf				
-kdmaxbbi	-1		-1
-kdmaxdepth	0		0
-kdtree				
-keyphrase			
-kws				
-kws_plp	1e-1		1.000000e-01
-kws_threshold	1		1.000000e+00
-latsize	5000		5000
-lda				
-ldadim		0		0
-lextreedump	0		0
-lifter		0		0
-lm				/var/mobile/Containers/Data/Application/6A690B2A-2799-46D9-BF15-9037FDC56EF7/Library/Caches/FirstOpenEarsDynamicLanguageModel.DMP
-lmctl				
-lmname				
-logbase	1.0001		1.000100e+00
-logfn				
-logspec	no		no
-lowerf		133.33334	1.333333e+02
-lpbeam		1e-40		1.000000e-40
-lponlybeam	7e-29		7.000000e-29
-lw		6.5		6.500000e+00
-maxhmmpf	10000		10000
-maxnewoov	20		20
-maxwpf		-1		-1
-mdef				
-mean				
-mfclogdir			
-min_endfr	0		0
-mixw				
-mixwfloor	0.0000001	1.000000e-07
-mllr				
-mmap		yes		yes
-ncep		13		13
-nfft		512		512
-nfilt		40		40
-nwpen		1.0		1.000000e+00
-pbeam		1e-48		1.000000e-48
-pip		1.0		1.000000e+00
-pl_beam	1e-10		1.000000e-10
-pl_pbeam	1e-5		1.000000e-05
-pl_window	0		0
-rawlogdir			
-remove_dc	no		no
-remove_noise	yes		yes
-remove_silence	yes		yes
-round_filters	yes		yes
-samprate	16000		1.600000e+04
-seed		-1		-1
-sendump			
-senlogdir			
-senmgau			
-silprob	0.005		5.000000e-03
-smoothspec	no		no
-svspec				
-tmat				
-tmatfloor	0.0001		1.000000e-04
-topn		4		4
-topn_beam	0		0
-toprule			
-transform	legacy		legacy
-unit_area	yes		yes
-upperf		6855.4976	6.855498e+03
-usewdphones	no		no
-uw		1.0		1.000000e+00
-vad_postspeech	50		20
-vad_prespeech	10		10
-vad_threshold	2.0		3.000000e+00
-var				
-varfloor	0.0001		1.000000e-04
-varnorm	no		no
-verbose	no		no
-warp_params			
-warp_type	inverse_linear	inverse_linear
-wbeam		7e-29		7.000000e-29
-wip		0.65		6.500000e-01
-wlen		0.025625	2.562500e-02

2015-01-20 16:12:20.731 OpenEarsSampleApp[5469:2180549] This is not a case in which OpenEars notifies of a route change. At the close of this function, the new audio route is ---SpeakerMicrophoneBuiltIn---. The previous route before changing to this route was &lt;AVAudioSessionRouteDescription: 0x17e6a8e0, 
inputs = (null); 
outputs = (
    &quot;&lt;AVAudioSessionPortDescription: 0x17e69120, type = Speaker; name = Speaker; UID = Built-In Speaker; selectedDataSource = (null)&gt;&quot;
)&gt;.
INFO: cmd_ln.c(702): Parsing command line:
\
	-nfilt 25 \
	-lowerf 130 \
	-upperf 6800 \
	-feat 1s_c_d_dd \
	-svspec 0-12/13-25/26-38 \
	-agc none \
	-cmn current \
	-varnorm no \
	-transform dct \
	-lifter 22 \
	-cmninit 40 

Current configuration:
[NAME]		[DEFLT]		[VALUE]
-agc		none		none
-agcthresh	2.0		2.000000e+00
-alpha		0.97		9.700000e-01
-ceplen		13		13
-cmn		current		current
-cmninit	8.0		40
-dither		no		no
-doublebw	no		no
-feat		1s_c_d_dd	1s_c_d_dd
-frate		100		100
-input_endian	little		little
-lda				
-ldadim		0		0
-lifter		0		22
-logspec	no		no
-lowerf		133.33334	1.300000e+02
-ncep		13		13
-nfft		512		512
-nfilt		40		25
-remove_dc	no		no
-remove_noise	yes		yes
-remove_silence	yes		yes
-round_filters	yes		yes
-samprate	16000		1.600000e+04
-seed		-1		-1
-smoothspec	no		no
-svspec				0-12/13-25/26-38
-transform	legacy		dct
-unit_area	yes		yes
-upperf		6855.4976	6.800000e+03
-vad_postspeech	50		20
-vad_prespeech	10		10
-vad_threshold	2.0		3.000000e+00
-varnorm	no		no
-verbose	no		no
-warp_params			
-warp_type	inverse_linear	inverse_linear
-wlen		0.025625	2.562500e-02

INFO: acmod.c(252): Parsed model-specific feature parameters from /private/var/mobile/Containers/Bundle/Application/03075EFA-2B35-4613-BBF0-2E3BD17986E8/OpenEarsSampleApp.app/AcousticModelEnglish.bundle/feat.params
INFO: feat.c(715): Initializing feature stream to type: &#039;1s_c_d_dd&#039;, ceplen=13, CMN=&#039;current&#039;, VARNORM=&#039;no&#039;, AGC=&#039;none&#039;
INFO: cmn.c(143): mean[0]= 12.00, mean[1..12]= 0.0
INFO: acmod.c(171): Using subvector specification 0-12/13-25/26-38
INFO: mdef.c(518): Reading model definition: /private/var/mobile/Containers/Bundle/Application/03075EFA-2B35-4613-BBF0-2E3BD17986E8/OpenEarsSampleApp.app/AcousticModelEnglish.bundle/mdef
INFO: mdef.c(531): Found byte-order mark BMDF, assuming this is a binary mdef file
INFO: bin_mdef.c(336): Reading binary model definition: /private/var/mobile/Containers/Bundle/Application/03075EFA-2B35-4613-BBF0-2E3BD17986E8/OpenEarsSampleApp.app/AcousticModelEnglish.bundle/mdef
INFO: bin_mdef.c(516): 46 CI-phone, 168344 CD-phone, 3 emitstate/phone, 138 CI-sen, 6138 Sen, 32881 Sen-Seq
INFO: tmat.c(206): Reading HMM transition probability matrices: /private/var/mobile/Containers/Bundle/Application/03075EFA-2B35-4613-BBF0-2E3BD17986E8/OpenEarsSampleApp.app/AcousticModelEnglish.bundle/transition_matrices
INFO: acmod.c(124): Attempting to use SCHMM computation module
INFO: ms_gauden.c(198): Reading mixture gaussian parameter: /private/var/mobile/Containers/Bundle/Application/03075EFA-2B35-4613-BBF0-2E3BD17986E8/OpenEarsSampleApp.app/AcousticModelEnglish.bundle/means
INFO: ms_gauden.c(292): 1 codebook, 3 feature, size: 
INFO: ms_gauden.c(294):  512x13
INFO: ms_gauden.c(294):  512x13
INFO: ms_gauden.c(294):  512x13
INFO: ms_gauden.c(198): Reading mixture gaussian parameter: /private/var/mobile/Containers/Bundle/Application/03075EFA-2B35-4613-BBF0-2E3BD17986E8/OpenEarsSampleApp.app/AcousticModelEnglish.bundle/variances
INFO: ms_gauden.c(292): 1 codebook, 3 feature, size: 
INFO: ms_gauden.c(294):  512x13
INFO: ms_gauden.c(294):  512x13
INFO: ms_gauden.c(294):  512x13
INFO: ms_gauden.c(354): 0 variance values floored
INFO: s2_semi_mgau.c(904): Loading senones from dump file /private/var/mobile/Containers/Bundle/Application/03075EFA-2B35-4613-BBF0-2E3BD17986E8/OpenEarsSampleApp.app/AcousticModelEnglish.bundle/sendump
INFO: s2_semi_mgau.c(928): BEGIN FILE FORMAT DESCRIPTION
INFO: s2_semi_mgau.c(991): Rows: 512, Columns: 6138
INFO: s2_semi_mgau.c(1023): Using memory-mapped I/O for senones
INFO: s2_semi_mgau.c(1294): Maximum top-N: 4 Top-N beams: 0 0 0
INFO: dict.c(320): Allocating 4128 * 20 bytes (80 KiB) for word entries
INFO: dict.c(333): Reading main dictionary: /var/mobile/Containers/Data/Application/6A690B2A-2799-46D9-BF15-9037FDC56EF7/Library/Caches/FirstOpenEarsDynamicLanguageModel.dic
INFO: dict.c(213): Allocated 0 KiB for strings, 0 KiB for phones
INFO: dict.c(336): 23 words read
INFO: dict.c(342): Reading filler dictionary: /private/var/mobile/Containers/Bundle/Application/03075EFA-2B35-4613-BBF0-2E3BD17986E8/OpenEarsSampleApp.app/AcousticModelEnglish.bundle/noisedict
INFO: dict.c(213): Allocated 0 KiB for strings, 0 KiB for phones
INFO: dict.c(345): 9 words read
INFO: dict2pid.c(396): Building PID tables for dictionary
INFO: dict2pid.c(406): Allocating 46^3 * 2 bytes (190 KiB) for word-initial triphones
INFO: dict2pid.c(132): Allocated 25576 bytes (24 KiB) for word-final triphones
INFO: dict2pid.c(196): Allocated 25576 bytes (24 KiB) for single-phone word triphones
INFO: ngram_model_arpa.c(79): No \data\ mark in LM file
INFO: ngram_model_dmp.c(166): Will use memory-mapped I/O for LM file
INFO: ngram_model_dmp.c(220): ngrams 1=22, 2=46, 3=32
INFO: ngram_model_dmp.c(266):       22 = LM.unigrams(+trailer) read
INFO: ngram_model_dmp.c(312):       46 = LM.bigrams(+trailer) read
INFO: ngram_model_dmp.c(338):       32 = LM.trigrams read
INFO: ngram_model_dmp.c(363):        6 = LM.prob2 entries read
INFO: ngram_model_dmp.c(383):        5 = LM.bo_wt2 entries read
INFO: ngram_model_dmp.c(403):        3 = LM.prob3 entries read
INFO: ngram_model_dmp.c(431):        1 = LM.tseg_base entries read
INFO: ngram_model_dmp.c(487):       22 = ascii word strings read
INFO: ngram_search_fwdtree.c(99): 16 unique initial diphones
INFO: ngram_search_fwdtree.c(148): 0 root, 0 non-root channels, 10 single-phone words
INFO: ngram_search_fwdtree.c(186): Creating search tree
INFO: ngram_search_fwdtree.c(192): before: 0 root, 0 non-root channels, 10 single-phone words
INFO: ngram_search_fwdtree.c(326): after: max nonroot chan increased to 177
INFO: ngram_search_fwdtree.c(339): after: 16 root, 49 non-root channels, 9 single-phone words
INFO: ngram_search_fwdflat.c(157): fwdflat: min_ef_width = 4, max_sf_win = 25
2015-01-20 16:12:20.903 OpenEarsSampleApp[5469:2180540] There is no CMN plist so we are using the fresh CMN value 42.000000.
2015-01-20 16:12:20.904 OpenEarsSampleApp[5469:2180540] Listening.
2015-01-20 16:12:20.906 OpenEarsSampleApp[5469:2180540] Project has these words or phrases in its dictionary:
EIGHT
EIGHTEEN
ELEVEN
ELEVEN(2)
FIFTEEN
FIVE
FOUR
FOURTEEN
NINE
NINETEEN
ONE
ONE(2)
SEVEN
SEVENTEEN
SIX
SIXTEEN
TEN
THIRTEEN
THREE
TWELVE
TWENTY
TWENTY(2)
TWO
2015-01-20 16:12:20.907 OpenEarsSampleApp[5469:2180540] Recognition loop has started
2015-01-20 16:12:48.623 OpenEarsSampleApp[5469:2180544] Speech detected...
2015-01-20 16:12:48.881 OpenEarsSampleApp[5469:2180544] End of speech detected...
INFO: cmn_prior.c(131): cmn_prior_update: from &lt; 42.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 &gt;
INFO: cmn_prior.c(149): cmn_prior_update: to   &lt; 64.65  9.76  3.14  5.08 -10.27 13.92 -8.86  0.30 -4.35 -7.43  7.40 -6.46 -4.60 &gt;
INFO: ngram_search_fwdtree.c(1550):      385 words recognized (10/fr)
INFO: ngram_search_fwdtree.c(1552):     8544 senones evaluated (219/fr)
INFO: ngram_search_fwdtree.c(1556):     4700 channels searched (120/fr), 560 1st, 3013 last
INFO: ngram_search_fwdtree.c(1559):      431 words for which last channels evaluated (11/fr)
INFO: ngram_search_fwdtree.c(1561):      334 candidate words for entering last phone (8/fr)
INFO: ngram_search_fwdtree.c(1564): fwdtree 1.45 CPU 3.724 xRT
INFO: ngram_search_fwdtree.c(1567): fwdtree 27.81 wall 71.296 xRT
INFO: ngram_search_fwdflat.c(302): Utterance vocabulary contains 6 words
INFO: ngram_search_fwdflat.c(938):      298 words recognized (8/fr)
INFO: ngram_search_fwdflat.c(940):     8125 senones evaluated (208/fr)
INFO: ngram_search_fwdflat.c(942):     5526 channels searched (141/fr)
INFO: ngram_search_fwdflat.c(944):      453 words searched (11/fr)
INFO: ngram_search_fwdflat.c(947):      172 word transitions (4/fr)
INFO: ngram_search_fwdflat.c(950): fwdflat 0.02 CPU 0.043 xRT
INFO: ngram_search_fwdflat.c(953): fwdflat 0.02 wall 0.056 xRT
INFO: ngram_search.c(1215): &lt;/s&gt; not found in last frame, using TWO.37 instead
INFO: ngram_search.c(1268): lattice start node &lt;s&gt;.0 end node TWO.2
INFO: ngram_search.c(1294): Eliminated 54 nodes before end node
INFO: ngram_search.c(1399): Lattice has 63 nodes, 1 links
INFO: ps_lattice.c(1368): Normalizer P(O) = alpha(TWO:2:37) = -356386
INFO: ps_lattice.c(1403): Joint P(O,S) = -356386 P(S|O) = 0
INFO: ngram_search.c(890): bestpath 0.00 CPU 0.000 xRT
INFO: ngram_search.c(893): bestpath 0.00 wall 0.001 xRT
2015-01-20 16:12:48.906 OpenEarsSampleApp[5469:2180544] Pocketsphinx heard &quot;TWO&quot; with a score of (0) and an utterance ID of 0.
2015-01-20 16:12:48.908 OpenEarsSampleApp[5469:2180532] Flite sending interrupt speech request.
2015-01-20 16:12:48.910 OpenEarsSampleApp[5469:2180532] I&#039;m running flite
2015-01-20 16:12:49.110 OpenEarsSampleApp[5469:2180532] I&#039;m done running flite and it took 0.198875 seconds
2015-01-20 16:12:49.111 OpenEarsSampleApp[5469:2180532] Flite audio player was nil when referenced so attempting to allocate a new audio player.
2015-01-20 16:12:49.112 OpenEarsSampleApp[5469:2180532] Loading speech data for Flite concluded successfully.
2015-01-20 16:12:49.146 OpenEarsSampleApp[5469:2180532] Flite sending suspend recognition notification.
2015-01-20 16:12:50.355 OpenEarsSampleApp[5469:2180532] AVAudioPlayer did finish playing with success flag of 1
2015-01-20 16:12:50.508 OpenEarsSampleApp[5469:2180532] Flite sending resume recognition notification.
2015-01-20 16:12:51.016 OpenEarsSampleApp[5469:2180532] Valid setSecondsOfSilence value of 0.200000 will be used.
INFO: cmn_prior.c(131): cmn_prior_update: from &lt; 64.65  9.76  3.14  5.08 -10.27 13.92 -8.86  0.30 -4.35 -7.43  7.40 -6.46 -4.60 &gt;
INFO: cmn_prior.c(149): cmn_prior_update: to   &lt; 64.65  9.76  3.14  5.08 -10.27 13.92 -8.86  0.30 -4.35 -7.43  7.40 -6.46 -4.60 &gt;
INFO: ngram_search_fwdflat.c(302): Utterance vocabulary contains 0 words
2015-01-20 16:13:05.254 OpenEarsSampleApp[5469:2180540] Speech detected...
2015-01-20 16:13:05.494 OpenEarsSampleApp[5469:2180540] End of speech detected...
INFO: cmn_prior.c(131): cmn_prior_update: from &lt; 64.65  9.76  3.14  5.08 -10.27 13.92 -8.86  0.30 -4.35 -7.43  7.40 -6.46 -4.60 &gt;
INFO: cmn_prior.c(149): cmn_prior_update: to   &lt; 69.03  2.31 -5.25  0.93 -12.55 10.68 -7.56 -3.24 -14.34 -1.36  8.05 -6.54 -1.84 &gt;
INFO: ngram_search_fwdtree.c(1550):      228 words recognized (8/fr)
INFO: ngram_search_fwdtree.c(1552):     4294 senones evaluated (148/fr)
INFO: ngram_search_fwdtree.c(1556):     1923 channels searched (66/fr), 400 1st, 909 last
INFO: ngram_search_fwdtree.c(1559):      256 words for which last channels evaluated (8/fr)
INFO: ngram_search_fwdtree.c(1561):      143 candidate words for entering last phone (4/fr)
INFO: ngram_search_fwdtree.c(1564): fwdtree 0.75 CPU 2.584 xRT
INFO: ngram_search_fwdtree.c(1567): fwdtree 14.33 wall 49.420 xRT
INFO: ngram_search_fwdflat.c(302): Utterance vocabulary contains 3 words
INFO: ngram_search_fwdflat.c(938):      186 words recognized (6/fr)
INFO: ngram_search_fwdflat.c(940):     3059 senones evaluated (105/fr)
INFO: ngram_search_fwdflat.c(942):     1589 channels searched (54/fr)
INFO: ngram_search_fwdflat.c(944):      255 words searched (8/fr)
INFO: ngram_search_fwdflat.c(947):       82 word transitions (2/fr)
INFO: ngram_search_fwdflat.c(950): fwdflat 0.01 CPU 0.045 xRT
INFO: ngram_search_fwdflat.c(953): fwdflat 0.01 wall 0.040 xRT
INFO: ngram_search.c(1215): &lt;/s&gt; not found in last frame, using [COUGH].27 instead
INFO: ngram_search.c(1268): lattice start node &lt;s&gt;.0 end node [COUGH].2
INFO: ngram_search.c(1294): Eliminated 62 nodes before end node
INFO: ngram_search.c(1399): Lattice has 66 nodes, 1 links
INFO: ps_lattice.c(1368): Normalizer P(O) = alpha([COUGH]:2:27) = -4795305
INFO: ps_lattice.c(1403): Joint P(O,S) = -4795306 P(S|O) = -1
INFO: ngram_search.c(890): bestpath 0.00 CPU 0.000 xRT
INFO: ngram_search.c(893): bestpath 0.00 wall 0.001 xRT
2015-01-20 16:13:05.508 OpenEarsSampleApp[5469:2180540] Pocketsphinx heard &quot;&quot; with a score of (-1) and an utterance ID of 1.
2015-01-20 16:13:05.509 OpenEarsSampleApp[5469:2180540] Hypothesis was null so we aren&#039;t returning it. If you want null hypotheses to also be returned, set OEPocketsphinxController&#039;s property returnNullHypotheses to TRUE before starting OEPocketsphinxController.
2015-01-20 16:13:11.532 OpenEarsSampleApp[5469:2180544] Speech detected...
2015-01-20 16:13:12.026 OpenEarsSampleApp[5469:2180544] End of speech detected...
INFO: cmn_prior.c(131): cmn_prior_update: from &lt; 69.03  2.31 -5.25  0.93 -12.55 10.68 -7.56 -3.24 -14.34 -1.36  8.05 -6.54 -1.84 &gt;
INFO: cmn_prior.c(149): cmn_prior_update: to   &lt; 66.26  8.59 -4.91 -2.19 -13.70 11.91 -10.44 -1.99 -5.42  0.23  6.81 -2.35 -2.18 &gt;
INFO: ngram_search_fwdtree.c(1550):      800 words recognized (12/fr)
INFO: ngram_search_fwdtree.c(1552):    18873 senones evaluated (282/fr)
INFO: ngram_search_fwdtree.c(1556):    10567 channels searched (157/fr), 1008 1st, 7260 last
INFO: ngram_search_fwdtree.c(1559):      878 words for which last channels evaluated (13/fr)
INFO: ngram_search_fwdtree.c(1561):      594 candidate words for entering last phone (8/fr)
INFO: ngram_search_fwdtree.c(1564): fwdtree 0.38 CPU 0.571 xRT
INFO: ngram_search_fwdtree.c(1567): fwdtree 6.52 wall 9.732 xRT
INFO: ngram_search_fwdflat.c(302): Utterance vocabulary contains 10 words
INFO: ngram_search_fwdflat.c(938):      642 words recognized (10/fr)
INFO: ngram_search_fwdflat.c(940):    21996 senones evaluated (328/fr)
INFO: ngram_search_fwdflat.c(942):    15153 channels searched (226/fr)
INFO: ngram_search_fwdflat.c(944):     1039 words searched (15/fr)
INFO: ngram_search_fwdflat.c(947):      426 word transitions (6/fr)
INFO: ngram_search_fwdflat.c(950): fwdflat 0.03 CPU 0.051 xRT
INFO: ngram_search_fwdflat.c(953): fwdflat 0.03 wall 0.051 xRT
INFO: ngram_search.c(1215): &lt;/s&gt; not found in last frame, using &lt;sil&gt;.65 instead
INFO: ngram_search.c(1268): lattice start node &lt;s&gt;.0 end node &lt;sil&gt;.54
INFO: ngram_search.c(1294): Eliminated 13 nodes before end node
INFO: ngram_search.c(1399): Lattice has 115 nodes, 683 links
INFO: ps_lattice.c(1368): Normalizer P(O) = alpha(&lt;sil&gt;:54:65) = -486219
INFO: ps_lattice.c(1403): Joint P(O,S) = -486443 P(S|O) = -224
INFO: ngram_search.c(890): bestpath 0.00 CPU 0.004 xRT
INFO: ngram_search.c(893): bestpath 0.00 wall 0.006 xRT
2015-01-20 16:13:12.069 OpenEarsSampleApp[5469:2180544] Pocketsphinx heard &quot;FOUR&quot; with a score of (-224) and an utterance ID of 2.
2015-01-20 16:13:12.070 OpenEarsSampleApp[5469:2180532] Flite sending interrupt speech request.
2015-01-20 16:13:12.073 OpenEarsSampleApp[5469:2180532] I&#039;m running flite
2015-01-20 16:13:12.264 OpenEarsSampleApp[5469:2180532] I&#039;m done running flite and it took 0.190912 seconds
2015-01-20 16:13:12.265 OpenEarsSampleApp[5469:2180532] Flite audio player was nil when referenced so attempting to allocate a new audio player.
2015-01-20 16:13:12.266 OpenEarsSampleApp[5469:2180532] Loading speech data for Flite concluded successfully.
2015-01-20 16:13:12.367 OpenEarsSampleApp[5469:2180532] Flite sending suspend recognition notification.
2015-01-20 16:13:13.575 OpenEarsSampleApp[5469:2180532] AVAudioPlayer did finish playing with success flag of 1
2015-01-20 16:13:13.727 OpenEarsSampleApp[5469:2180532] Flite sending resume recognition notification.
2015-01-20 16:13:14.235 OpenEarsSampleApp[5469:2180532] Valid setSecondsOfSilence value of 0.200000 will be used.
INFO: cmn_prior.c(131): cmn_prior_update: from &lt; 66.26  8.59 -4.91 -2.19 -13.70 11.91 -10.44 -1.99 -5.42  0.23  6.81 -2.35 -2.18 &gt;
INFO: cmn_prior.c(149): cmn_prior_update: to   &lt; 66.26  8.59 -4.91 -2.19 -13.70 11.91 -10.44 -1.99 -5.42  0.23  6.81 -2.35 -2.18 &gt;
INFO: ngram_search_fwdflat.c(302): Utterance vocabulary contains 0 words
2015-01-20 16:13:25.601 OpenEarsSampleApp[5469:2180544] Speech detected...
2015-01-20 16:13:26.249 OpenEarsSampleApp[5469:2180544] End of speech detected...
INFO: cmn_prior.c(131): cmn_prior_update: from &lt; 66.26  8.59 -4.91 -2.19 -13.70 11.91 -10.44 -1.99 -5.42  0.23  6.81 -2.35 -2.18 &gt;
INFO: cmn_prior.c(149): cmn_prior_update: to   &lt; 64.91  9.58 -3.64 -3.85 -14.94 12.37 -11.15 -0.83 -3.95  1.28  5.64 -0.12 -2.27 &gt;
INFO: ngram_search_fwdtree.c(1550):      728 words recognized (11/fr)
INFO: ngram_search_fwdtree.c(1552):    16222 senones evaluated (239/fr)
INFO: ngram_search_fwdtree.c(1556):     8497 channels searched (124/fr), 1024 1st, 5287 last
INFO: ngram_search_fwdtree.c(1559):      812 words for which last channels evaluated (11/fr)
INFO: ngram_search_fwdtree.c(1561):      533 candidate words for entering last phone (7/fr)
INFO: ngram_search_fwdtree.c(1564): fwdtree 0.65 CPU 0.954 xRT
INFO: ngram_search_fwdtree.c(1567): fwdtree 11.91 wall 17.514 xRT
INFO: ngram_search_fwdflat.c(302): Utterance vocabulary contains 8 words
INFO: ngram_search_fwdflat.c(938):      544 words recognized (8/fr)
INFO: ngram_search_fwdflat.c(940):    17484 senones evaluated (257/fr)
INFO: ngram_search_fwdflat.c(942):    11455 channels searched (168/fr)
INFO: ngram_search_fwdflat.c(944):      933 words searched (13/fr)
INFO: ngram_search_fwdflat.c(947):      315 word transitions (4/fr)
INFO: ngram_search_fwdflat.c(950): fwdflat 0.03 CPU 0.045 xRT
INFO: ngram_search_fwdflat.c(953): fwdflat 0.03 wall 0.047 xRT
INFO: ngram_search.c(1215): &lt;/s&gt; not found in last frame, using &lt;sil&gt;.66 instead
INFO: ngram_search.c(1268): lattice start node &lt;s&gt;.0 end node &lt;sil&gt;.21
INFO: ngram_search.c(1294): Eliminated 103 nodes before end node
INFO: ngram_search.c(1399): Lattice has 152 nodes, 171 links
INFO: ps_lattice.c(1368): Normalizer P(O) = alpha(&lt;sil&gt;:21:66) = -589713
INFO: ps_lattice.c(1403): Joint P(O,S) = -589713 P(S|O) = 0
INFO: ngram_search.c(890): bestpath 0.01 CPU 0.008 xRT
INFO: ngram_search.c(893): bestpath 0.00 wall 0.001 xRT
2015-01-20 16:13:26.284 OpenEarsSampleApp[5469:2180544] Pocketsphinx heard &quot;ONE&quot; with a score of (0) and an utterance ID of 3.
2015-01-20 16:13:26.285 OpenEarsSampleApp[5469:2180532] Flite sending interrupt speech request.
2015-01-20 16:13:26.287 OpenEarsSampleApp[5469:2180532] I&#039;m running flite
2015-01-20 16:13:26.488 OpenEarsSampleApp[5469:2180532] I&#039;m done running flite and it took 0.199754 seconds
2015-01-20 16:13:26.489 OpenEarsSampleApp[5469:2180532] Flite audio player was nil when referenced so attempting to allocate a new audio player.
2015-01-20 16:13:26.489 OpenEarsSampleApp[5469:2180532] Loading speech data for Flite concluded successfully.
2015-01-20 16:13:26.579 OpenEarsSampleApp[5469:2180532] Flite sending suspend recognition notification.
2015-01-20 16:13:27.786 OpenEarsSampleApp[5469:2180532] AVAudioPlayer did finish playing with success flag of 1
2015-01-20 16:13:27.938 OpenEarsSampleApp[5469:2180532] Flite sending resume recognition notification.
2015-01-20 16:13:28.446 OpenEarsSampleApp[5469:2180532] Valid setSecondsOfSilence value of 0.200000 will be used.
INFO: cmn_prior.c(131): cmn_prior_update: from &lt; 64.91  9.58 -3.64 -3.85 -14.94 12.37 -11.15 -0.83 -3.95  1.28  5.64 -0.12 -2.27 &gt;
INFO: cmn_prior.c(149): cmn_prior_update: to   &lt; 64.91  9.58 -3.64 -3.85 -14.94 12.37 -11.15 -0.83 -3.95  1.28  5.64 -0.12 -2.27 &gt;
INFO: ngram_search_fwdflat.c(302): Utterance vocabulary contains 0 words
</code></pre>
<p>and this is a viewController.m code </p>
<pre><code>#import &quot;ViewController.h&quot;
#import &lt;OpenEars/OEPocketsphinxController.h&gt;
#import &lt;OpenEars/OEFliteController.h&gt;
#import &lt;OpenEars/OELanguageModelGenerator.h&gt;
#import &lt;OpenEars/OELogging.h&gt;
#import &lt;OpenEars/OEAcousticModel.h&gt;
#import &lt;Slt/Slt.h&gt;

@interface ViewController()

// UI actions, not specifically related to OpenEars other than the fact that they invoke OpenEars methods.
- (IBAction) stopButtonAction;
- (IBAction) startButtonAction;
- (IBAction) suspendListeningButtonAction;
- (IBAction) resumeListeningButtonAction;

// Example for reading out the input audio levels without locking the UI using an NSTimer

- (void) startDisplayingLevels;
- (void) stopDisplayingLevels;

// These three are the important OpenEars objects that this class demonstrates the use of.
@property (nonatomic, strong) Slt *slt;

@property (nonatomic, strong) OEEventsObserver *openEarsEventsObserver;
@property (nonatomic, strong) OEPocketsphinxController *pocketsphinxController;
@property (nonatomic, strong) OEFliteController *fliteController;

// Some UI, not specifically related to OpenEars.
@property (nonatomic, strong) IBOutlet UIButton *stopButton;
@property (nonatomic, strong) IBOutlet UIButton *startButton;
@property (nonatomic, strong) IBOutlet UIButton *suspendListeningButton;	
@property (nonatomic, strong) IBOutlet UIButton *resumeListeningButton;	
@property (nonatomic, strong) IBOutlet UITextView *statusTextView;
@property (nonatomic, strong) IBOutlet UITextView *heardTextView;
@property (nonatomic, strong) IBOutlet UILabel *pocketsphinxDbLabel;
@property (nonatomic, strong) IBOutlet UILabel *fliteDbLabel;
@property (nonatomic, assign) BOOL usingStartingLanguageModel;
@property (nonatomic, assign) int restartAttemptsDueToPermissionRequests;
@property (nonatomic, assign) BOOL startupFailedDueToLackOfPermissions;

// Things which help us show off the dynamic language features.
@property (nonatomic, copy) NSString *pathToFirstDynamicallyGeneratedLanguageModel;
@property (nonatomic, copy) NSString *pathToFirstDynamicallyGeneratedDictionary;
@property (nonatomic, copy) NSString *pathToSecondDynamicallyGeneratedLanguageModel;
@property (nonatomic, copy) NSString *pathToSecondDynamicallyGeneratedDictionary;

// Our NSTimer that will help us read and display the input and output levels without locking the UI
@property (nonatomic, strong) 	NSTimer *uiUpdateTimer;

@end

@implementation ViewController

#define kLevelUpdatesPerSecond 18 // We&#039;ll have the ui update 18 times a second to show some fluidity without hitting the CPU too hard.

//#define kGetNbest // Uncomment this if you want to try out nbest
#pragma mark - 
#pragma mark Memory Management

- (void)dealloc {
    [self stopDisplayingLevels];
}

#pragma mark - int to word converter

-(NSString *)GetWordOfInteger:(int)anInt
{
    NSNumber *numberValue = [NSNumber numberWithInt:anInt]; //needs to be NSNumber!
    NSNumberFormatter *numberFormatter = [[NSNumberFormatter alloc] init];
    [numberFormatter setNumberStyle:NSNumberFormatterSpellOutStyle];
    NSString *wordNumber = [numberFormatter stringFromNumber:numberValue];
    wordNumber = [[wordNumber stringByReplacingOccurrencesOfString:@&quot;-&quot; withString:@&quot; &quot;] uppercaseString];
    ////NSLog(@&quot;Answer: %@&quot;, wordNumber);
    return wordNumber;
}

#pragma mark View Lifecycle

- (void)viewDidLoad {
    [super viewDidLoad];
    self.fliteController = [[OEFliteController alloc] init];
    self.openEarsEventsObserver = [[OEEventsObserver alloc] init];
    self.openEarsEventsObserver.delegate = self;
    self.slt = [[Slt alloc] init];
    
    self.restartAttemptsDueToPermissionRequests = 0;
    self.startupFailedDueToLackOfPermissions = FALSE;
    
    [OELogging startOpenEarsLogging]; // Uncomment me for OELogging, which is verbose logging about internal OpenEars operations such as audio settings. If you have issues, show this logging in the forums.
    [OEPocketsphinxController sharedInstance].verbosePocketSphinx = TRUE; // Uncomment this for much more verbose speech recognition engine output. If you have issues, show this logging in the forums.
    [[OEPocketsphinxController sharedInstance] setSecondsOfSilenceToDetect:0.2];
    [[OEPocketsphinxController sharedInstance] setVadThreshold:3.0];
    
    [self.openEarsEventsObserver setDelegate:self]; // Make this class the delegate of OpenEarsObserver so we can get all of the messages about what OpenEars is doing.
    
    [[OEPocketsphinxController sharedInstance] setActive:TRUE error:nil]; // Call this before setting any OEPocketsphinxController characteristics
    
    // This is the language model we&#039;re going to start up with. The only reason I&#039;m making it a class property is that I reuse it a bunch of times in this example, 
    // but you can pass the string contents directly to OEPocketsphinxController:startListeningWithLanguageModelAtPath:dictionaryAtPath:languageModelIsJSGF:
    
    NSMutableArray *numberArray = [[NSMutableArray alloc] init];
    
    
    for(int i=1;i&lt;=26;i++)
    {
        [numberArray addObject:[self GetWordOfInteger:i]];
    }
    
    
    NSArray *firstLanguageArray = [[NSArray alloc] initWithArray:numberArray];
    
   /* NSArray *firstLanguageArray = @[@&quot;BACKWARD&quot;,
                                    @&quot;CHANGE&quot;,
                                    @&quot;FORWARD&quot;,
                                    @&quot;GO&quot;,
                                    @&quot;LEFT&quot;,
                                    @&quot;MODEL&quot;,
                                    @&quot;RIGHT&quot;,
                                    @&quot;TURN&quot;];*/
    
    OELanguageModelGenerator *languageModelGenerator = [[OELanguageModelGenerator alloc] init]; 
    
    // languageModelGenerator.verboseLanguageModelGenerator = TRUE; // Uncomment me for verbose language model generator debug output.

    NSError *error = [languageModelGenerator generateLanguageModelFromArray:firstLanguageArray withFilesNamed:@&quot;FirstOpenEarsDynamicLanguageModel&quot; forAcousticModelAtPath:[OEAcousticModel pathToModel:@&quot;AcousticModelEnglish&quot;]]; // Change &quot;AcousticModelEnglish&quot; to &quot;AcousticModelSpanish&quot; in order to create a language model for Spanish recognition instead of English.
    
    
    if(error) {
        //NSLog(@&quot;Dynamic language generator reported error %@&quot;, [error description]);	
    } else {
        self.pathToFirstDynamicallyGeneratedLanguageModel = [languageModelGenerator pathToSuccessfullyGeneratedLanguageModelWithRequestedName:@&quot;FirstOpenEarsDynamicLanguageModel&quot;];
        self.pathToFirstDynamicallyGeneratedDictionary = [languageModelGenerator pathToSuccessfullyGeneratedDictionaryWithRequestedName:@&quot;FirstOpenEarsDynamicLanguageModel&quot;];
        
        
    }
    
    self.usingStartingLanguageModel = TRUE; // This is not an OpenEars thing, this is just so I can switch back and forth between the two models in this sample app.
    
    // Here is an example of dynamically creating an in-app grammar.
    
    // We want it to be able to response to the speech &quot;CHANGE MODEL&quot; and a few other things.  Items we want to have recognized as a whole phrase (like &quot;CHANGE MODEL&quot;) 
    // we put into the array as one string (e.g. &quot;CHANGE MODEL&quot; instead of &quot;CHANGE&quot; and &quot;MODEL&quot;). This increases the probability that they will be recognized as a phrase. This works even better starting with version 1.0 of OpenEars.
    
    NSArray *secondLanguageArray = @[@&quot;SUNDAY&quot;,
                                     @&quot;MONDAY&quot;,
                                     @&quot;TUESDAY&quot;,
                                     @&quot;WEDNESDAY&quot;,
                                     @&quot;THURSDAY&quot;,
                                     @&quot;FRIDAY&quot;,
                                     @&quot;SATURDAY&quot;,
                                     @&quot;QUIDNUNC&quot;,
                                     @&quot;CHANGE MODEL&quot;];
    
    // The last entry, quidnunc, is an example of a word which will not be found in the lookup dictionary and will be passed to the fallback method. The fallback method is slower,
    // so, for instance, creating a new language model from dictionary words will be pretty fast, but a model that has a lot of unusual names in it or invented/rare/recent-slang
    // words will be slower to generate. You can use this information to give your users good UI feedback about what the expectations for wait times should be.
    
    // I don&#039;t think it&#039;s beneficial to lazily instantiate OELanguageModelGenerator because you only need to give it a single message and then release it.
    // If you need to create a very large model or any size of model that has many unusual words that have to make use of the fallback generation method,
    // you will want to run this on a background thread so you can give the user some UI feedback that the task is in progress.
        
    // generateLanguageModelFromArray:withFilesNamed returns an NSError which will either have a value of noErr if everything went fine or a specific error if it didn&#039;t.
    error = [languageModelGenerator generateLanguageModelFromArray:secondLanguageArray withFilesNamed:@&quot;SecondOpenEarsDynamicLanguageModel&quot; forAcousticModelAtPath:[OEAcousticModel pathToModel:@&quot;AcousticModelEnglish&quot;]]; // Change &quot;AcousticModelEnglish&quot; to &quot;AcousticModelSpanish&quot; in order to create a language model for Spanish recognition instead of English.
    
    //    NSError *error = [languageModelGenerator generateLanguageModelFromTextFile:[NSString stringWithFormat:@&quot;%@/%@&quot;,[[NSBundle mainBundle] resourcePath], @&quot;OpenEarsCorpus.txt&quot;] withFilesNamed:@&quot;SecondOpenEarsDynamicLanguageModel&quot; forAcousticModelAtPath:[OEAcousticModel pathToModel:@&quot;AcousticModelEnglish&quot;]]; // Try this out to see how generating a language model from a corpus works.
    
    
    if(error) {
        //NSLog(@&quot;Dynamic language generator reported error %@&quot;, [error description]);	
    }	else {
        
        self.pathToSecondDynamicallyGeneratedLanguageModel = [languageModelGenerator pathToSuccessfullyGeneratedLanguageModelWithRequestedName:@&quot;SecondOpenEarsDynamicLanguageModel&quot;]; // We&#039;ll set our new .languagemodel file to be the one to get switched to when the words &quot;CHANGE MODEL&quot; are recognized.
        self.pathToSecondDynamicallyGeneratedDictionary = [languageModelGenerator pathToSuccessfullyGeneratedDictionaryWithRequestedName:@&quot;SecondOpenEarsDynamicLanguageModel&quot;];; // We&#039;ll set our new dictionary to be the one to get switched to when the words &quot;CHANGE MODEL&quot; are recognized.
        
        // Next, an informative message.
        
        ////NSLog(@&quot;\n\nWelcome to the OpenEars sample project. This project understands the words:\nBACKWARD,\nCHANGE,\nFORWARD,\nGO,\nLEFT,\nMODEL,\nRIGHT,\nTURN,\nand if you say \&quot;CHANGE MODEL\&quot; it will switch to its dynamically-generated model which understands the words:\nCHANGE,\nMODEL,\nMONDAY,\nTUESDAY,\nWEDNESDAY,\nTHURSDAY,\nFRIDAY,\nSATURDAY,\nSUNDAY,\nQUIDNUNC&quot;);
        
        // This is how to start the continuous listening loop of an available instance of OEPocketsphinxController. We won&#039;t do this if the language generation failed since it will be listening for a command to change over to the generated language.
        
        [[OEPocketsphinxController sharedInstance] setActive:TRUE error:nil]; // Call this once before setting properties of the OEPocketsphinxController instance.

        //   [OEPocketsphinxController sharedInstance].pathToTestFile = [[NSBundle mainBundle] pathForResource:@&quot;change_model_short&quot; ofType:@&quot;wav&quot;];  // This is how you could use a test WAV (mono/16-bit/16k) rather than live recognition
        
        if(![OEPocketsphinxController sharedInstance].isListening) {
            [[OEPocketsphinxController sharedInstance] startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&quot;AcousticModelEnglish&quot;] languageModelIsJSGF:FALSE]; // Start speech recognition if we aren&#039;t already listening.
        }
        // [self startDisplayingLevels] is not an OpenEars method, just a very simple approach for level reading
        // that I&#039;ve included with this sample app. My example implementation does make use of two OpenEars
        // methods:	the pocketsphinxInputLevel method of OEPocketsphinxController and the fliteOutputLevel
        // method of fliteController. 
        //
        // The example is meant to show one way that you can read those levels continuously without locking the UI, 
        // by using an NSTimer, but the OpenEars level-reading methods 
        // themselves do not include multithreading code since I believe that you will want to design your own 
        // code approaches for level display that are tightly-integrated with your interaction design and the  
        // graphics API you choose. 
   
        [self startDisplayingLevels];
        
        // Here is some UI stuff that has nothing specifically to do with OpenEars implementation
        self.startButton.hidden = TRUE;
        self.stopButton.hidden = TRUE;
        self.suspendListeningButton.hidden = TRUE;
        self.resumeListeningButton.hidden = TRUE;
    }
    
    
    
    
}

#pragma mark -
#pragma mark OEEventsObserver delegate methods

// What follows are all of the delegate methods you can optionally use once you&#039;ve instantiated an OEEventsObserver and set its delegate to self. 
// I&#039;ve provided some pretty granular information about the exact phase of the Pocketsphinx listening loop, the Audio Session, and Flite, but I&#039;d expect 
// that the ones that will really be needed by most projects are the following:
//
//- (void) pocketsphinxDidReceiveHypothesis:(NSString *)hypothesis recognitionScore:(NSString *)recognitionScore utteranceID:(NSString *)utteranceID;
//- (void) audioSessionInterruptionDidBegin;
//- (void) audioSessionInterruptionDidEnd;
//- (void) audioRouteDidChangeToRoute:(NSString *)newRoute;
//- (void) pocketsphinxDidStartListening;
//- (void) pocketsphinxDidStopListening;
//
// It isn&#039;t necessary to have a OEPocketsphinxController or a OEFliteController instantiated in order to use these methods.  If there isn&#039;t anything instantiated that will
// send messages to an OEEventsObserver, all that will happen is that these methods will never fire.  You also do not have to create a OEEventsObserver in
// the same class or view controller in which you are doing things with a OEPocketsphinxController or OEFliteController; you can receive updates from those objects in
// any class in which you instantiate an OEEventsObserver and set its delegate to self.

// This is an optional delegate method of OEEventsObserver which delivers the text of speech that Pocketsphinx heard and analyzed, along with its accuracy score and utterance ID.
- (void) pocketsphinxDidReceiveHypothesis:(NSString *)hypothesis recognitionScore:(NSString *)recognitionScore utteranceID:(NSString *)utteranceID {
    
    //NSLog(@&quot;Local callback: The received hypothesis is %@ with a score of %@ and an ID of %@&quot;, hypothesis, recognitionScore, utteranceID); // Log it.
    if([hypothesis isEqualToString:@&quot;CHANGE MODEL&quot;]) { // If the user says &quot;CHANGE MODEL&quot;, we will switch to the alternate model (which happens to be the dynamically generated model).
        
        // Here is an example of language model switching in OpenEars. Deciding on what logical basis to switch models is your responsibility.
        // For instance, when you call a customer service line and get a response tree that takes you through different options depending on what you say to it,
        // the models are being switched as you progress through it so that only relevant choices can be understood. The construction of that logical branching and 
        // how to react to it is your job, OpenEars just lets you send the signal to switch the language model when you&#039;ve decided it&#039;s the right time to do so.
        
        if(self.usingStartingLanguageModel) { // If we&#039;re on the starting model, switch to the dynamically generated one.
            
            // You can only change language models with ARPA grammars in OpenEars (the ones that end in .languagemodel or .DMP). 
            // Trying to switch between JSGF models (the ones that end in .gram) will return no result.
            [[OEPocketsphinxController sharedInstance] changeLanguageModelToFile:self.pathToSecondDynamicallyGeneratedLanguageModel withDictionary:self.pathToSecondDynamicallyGeneratedDictionary]; 
            self.usingStartingLanguageModel = FALSE;
        } else { // If we&#039;re on the dynamically generated model, switch to the start model (this is just an example of a trigger and method for switching models).
            [[OEPocketsphinxController sharedInstance] changeLanguageModelToFile:self.pathToFirstDynamicallyGeneratedLanguageModel withDictionary:self.pathToFirstDynamicallyGeneratedDictionary];
            self.usingStartingLanguageModel = TRUE;
        }
    }
    
    self.heardTextView.text = [NSString stringWithFormat:@&quot;Heard: \&quot;%@\&quot;&quot;, hypothesis]; // Show it in the status box.
    
    // This is how to use an available instance of OEFliteController. We&#039;re going to repeat back the command that we heard with the voice we&#039;ve chosen.
    [self.fliteController say:[NSString stringWithFormat:@&quot;You said %@&quot;,hypothesis] withVoice:self.slt];
}

#ifdef kGetNbest   
- (void) pocketsphinxDidReceiveNBestHypothesisArray:(NSArray *)hypothesisArray { // Pocketsphinx has an n-best hypothesis dictionary.
    //NSLog(@&quot;Local callback:  hypothesisArray is %@&quot;,hypothesisArray);   
}
#endif
// An optional delegate method of OEEventsObserver which informs that there was an interruption to the audio session (e.g. an incoming phone call).
- (void) audioSessionInterruptionDidBegin {
    //NSLog(@&quot;Local callback:  AudioSession interruption began.&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: AudioSession interruption began.&quot;; // Show it in the status box.
    NSError *error = nil;
    if([OEPocketsphinxController sharedInstance].isListening) {
        error = [[OEPocketsphinxController sharedInstance] stopListening]; // React to it by telling Pocketsphinx to stop listening (if it is listening) since it will need to restart its loop after an interruption.
        if(error) {
            //NSLog(@&quot;Error while stopping listening in audioSessionInterruptionDidBegin: %@&quot;, error);
        }
    }
}

// An optional delegate method of OEEventsObserver which informs that the interruption to the audio session ended.
- (void) audioSessionInterruptionDidEnd {
    
    //NSLog(@&quot;Local callback:  AudioSession interruption ended.&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: AudioSession interruption ended.&quot;; // Show it in the status box.
    // We&#039;re restarting the previously-stopped listening loop.
    if(![OEPocketsphinxController sharedInstance].isListening){
        [[OEPocketsphinxController sharedInstance] startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&quot;AcousticModelEnglish&quot;] languageModelIsJSGF:FALSE]; // Start speech recognition if we aren&#039;t currently listening.    
    }
}

// An optional delegate method of OEEventsObserver which informs that the audio input became unavailable.
- (void) audioInputDidBecomeUnavailable {
    //NSLog(@&quot;Local callback:  The audio input has become unavailable&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: The audio input has become unavailable&quot;; // Show it in the status box.
    NSError *error = nil;
    if([OEPocketsphinxController sharedInstance].isListening){
        error = [[OEPocketsphinxController sharedInstance] stopListening]; // React to it by telling Pocketsphinx to stop listening since there is no available input (but only if we are listening).
        if(error) {
            //NSLog(@&quot;Error while stopping listening in audioInputDidBecomeUnavailable: %@&quot;, error);
        }
    }
}

// An optional delegate method of OEEventsObserver which informs that the unavailable audio input became available again.
- (void) audioInputDidBecomeAvailable {
    //NSLog(@&quot;Local callback: The audio input is available&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: The audio input is available&quot;; // Show it in the status box.
    if(![OEPocketsphinxController sharedInstance].isListening) {
        [[OEPocketsphinxController sharedInstance] startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&quot;AcousticModelEnglish&quot;] languageModelIsJSGF:FALSE]; // Start speech recognition, but only if we aren&#039;t already listening.
    }
}
// An optional delegate method of OEEventsObserver which informs that there was a change to the audio route (e.g. headphones were plugged in or unplugged).
- (void) audioRouteDidChangeToRoute:(NSString *)newRoute {
    //NSLog(@&quot;Local callback: Audio route change. The new audio route is %@&quot;, newRoute); // Log it.
    self.statusTextView.text = [NSString stringWithFormat:@&quot;Status: Audio route change. The new audio route is %@&quot;,newRoute]; // Show it in the status box.
    
    NSError *error = [[OEPocketsphinxController sharedInstance] stopListening]; // React to it by telling the Pocketsphinx loop to shut down and then start listening again on the new route
    
    if(error){
        //NSLog(@&quot;Local callback: error while stopping listening in audioRouteDidChangeToRoute: %@&quot;,error);
    }

    if(![OEPocketsphinxController sharedInstance].isListening) {
        [[OEPocketsphinxController sharedInstance] startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&quot;AcousticModelEnglish&quot;] languageModelIsJSGF:FALSE]; // Start speech recognition if we aren&#039;t already listening.
    }
}

// An optional delegate method of OEEventsObserver which informs that the Pocketsphinx recognition loop has entered its actual loop.
// This might be useful in debugging a conflict between another sound class and Pocketsphinx.
- (void) pocketsphinxRecognitionLoopDidStart {
    
    //NSLog(@&quot;Local callback: Pocketsphinx started.&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: Pocketsphinx started.&quot;; // Show it in the status box.
}

// An optional delegate method of OEEventsObserver which informs that Pocketsphinx is now listening for speech.
- (void) pocketsphinxDidStartListening {
    
    //NSLog(@&quot;Local callback: Pocketsphinx is now listening.&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: Pocketsphinx is now listening.&quot;; // Show it in the status box.
    
    self.startButton.hidden = TRUE; // React to it with some UI changes.
    self.stopButton.hidden = FALSE;
    self.suspendListeningButton.hidden = FALSE;
    self.resumeListeningButton.hidden = TRUE;
}

// An optional delegate method of OEEventsObserver which informs that Pocketsphinx detected speech and is starting to process it.
- (void) pocketsphinxDidDetectSpeech {
    //NSLog(@&quot;Local callback: Pocketsphinx has detected speech.&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: Pocketsphinx has detected speech.&quot;; // Show it in the status box.
}

// An optional delegate method of OEEventsObserver which informs that Pocketsphinx detected a second of silence, indicating the end of an utterance. 
// This was added because developers requested being able to time the recognition speed without the speech time. The processing time is the time between 
// this method being called and the hypothesis being returned.
- (void) pocketsphinxDidDetectFinishedSpeech {
    //NSLog(@&quot;Local callback: Pocketsphinx has detected a second of silence, concluding an utterance.&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: Pocketsphinx has detected finished speech.&quot;; // Show it in the status box.
}

// An optional delegate method of OEEventsObserver which informs that Pocketsphinx has exited its recognition loop, most 
// likely in response to the OEPocketsphinxController being told to stop listening via the stopListening method.
- (void) pocketsphinxDidStopListening {
    //NSLog(@&quot;Local callback: Pocketsphinx has stopped listening.&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: Pocketsphinx has stopped listening.&quot;; // Show it in the status box.
}

// An optional delegate method of OEEventsObserver which informs that Pocketsphinx is still in its listening loop but it is not
// Going to react to speech until listening is resumed.  This can happen as a result of Flite speech being
// in progress on an audio route that doesn&#039;t support simultaneous Flite speech and Pocketsphinx recognition,
// or as a result of the OEPocketsphinxController being told to suspend recognition via the suspendRecognition method.
- (void) pocketsphinxDidSuspendRecognition {
    //NSLog(@&quot;Local callback: Pocketsphinx has suspended recognition.&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: Pocketsphinx has suspended recognition.&quot;; // Show it in the status box.
}

// An optional delegate method of OEEventsObserver which informs that Pocketsphinx is still in its listening loop and after recognition
// having been suspended it is now resuming.  This can happen as a result of Flite speech completing
// on an audio route that doesn&#039;t support simultaneous Flite speech and Pocketsphinx recognition,
// or as a result of the OEPocketsphinxController being told to resume recognition via the resumeRecognition method.
- (void) pocketsphinxDidResumeRecognition {
    //NSLog(@&quot;Local callback: Pocketsphinx has resumed recognition.&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: Pocketsphinx has resumed recognition.&quot;; // Show it in the status box.
}

// An optional delegate method which informs that Pocketsphinx switched over to a new language model at the given URL in the course of
// recognition. This does not imply that it is a valid file or that recognition will be successful using the file.
- (void) pocketsphinxDidChangeLanguageModelToFile:(NSString *)newLanguageModelPathAsString andDictionary:(NSString *)newDictionaryPathAsString {
    //NSLog(@&quot;Local callback: Pocketsphinx is now using the following language model: \n%@ and the following dictionary: %@&quot;,newLanguageModelPathAsString,newDictionaryPathAsString);
}

// An optional delegate method of OEEventsObserver which informs that Flite is speaking, most likely to be useful if debugging a
// complex interaction between sound classes. You don&#039;t have to do anything yourself in order to prevent Pocketsphinx from listening to Flite talk and trying to recognize the speech.
- (void) fliteDidStartSpeaking {
    //NSLog(@&quot;Local callback: Flite has started speaking&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: Flite has started speaking.&quot;; // Show it in the status box.
}

// An optional delegate method of OEEventsObserver which informs that Flite is finished speaking, most likely to be useful if debugging a
// complex interaction between sound classes.
- (void) fliteDidFinishSpeaking {
    //NSLog(@&quot;Local callback: Flite has finished speaking&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: Flite has finished speaking.&quot;; // Show it in the status box.
}

- (void) pocketSphinxContinuousSetupDidFailWithReason:(NSString *)reasonForFailure { // This can let you know that something went wrong with the recognition loop startup. Turn on [OELogging startOpenEarsLogging] to learn why.
    //NSLog(@&quot;Local callback: Setting up the continuous recognition loop has failed for the reason %@, please turn on [OELogging startOpenEarsLogging] to learn more.&quot;, reasonForFailure); // Log it.
    self.statusTextView.text = @&quot;Status: Not possible to start recognition loop.&quot;; // Show it in the status box.	
}

- (void) pocketSphinxContinuousTeardownDidFailWithReason:(NSString *)reasonForFailure { // This can let you know that something went wrong with the recognition loop startup. Turn on [OELogging startOpenEarsLogging] to learn why.
    //NSLog(@&quot;Local callback: Tearing down the continuous recognition loop has failed for the reason %@, please turn on [OELogging startOpenEarsLogging] to learn more.&quot;, reasonForFailure); // Log it.
    self.statusTextView.text = @&quot;Status: Not possible to cleanly end recognition loop.&quot;; // Show it in the status box.	
}

- (void) testRecognitionCompleted { // A test file which was submitted for direct recognition via the audio driver is done.
    //NSLog(@&quot;Local callback: A test file which was submitted for direct recognition via the audio driver is done.&quot;); // Log it.
    NSError *error = nil;
    if([OEPocketsphinxController sharedInstance].isListening) { // If we&#039;re listening, stop listening.
        error = [[OEPocketsphinxController sharedInstance] stopListening];
        if(error)
        {
            //NSLog(@&quot;Error while stopping listening in testRecognitionCompleted: %@&quot;, error);
        }
    }
    
}
/** Pocketsphinx couldn&#039;t start because it has no mic permissions (will only be returned on iOS7 or later).*/
- (void) pocketsphinxFailedNoMicPermissions {
    //NSLog(@&quot;Local callback: The user has never set mic permissions or denied permission to this app&#039;s mic, so listening will not start.&quot;);
    self.startupFailedDueToLackOfPermissions = TRUE;
}

/** The user prompt to get mic permissions, or a check of the mic permissions, has completed with a TRUE or a FALSE result  (will only be returned on iOS7 or later).*/
- (void) micPermissionCheckCompleted:(BOOL)result {
    if(result) {
        self.restartAttemptsDueToPermissionRequests++;
        if(self.restartAttemptsDueToPermissionRequests == 1 &amp;&amp; self.startupFailedDueToLackOfPermissions) { // If we get here because there was an attempt to start which failed due to lack of permissions, and now permissions have been requested and they returned true, we restart exactly once with the new permissions.
            NSError *error = nil;
            if([OEPocketsphinxController sharedInstance].isListening){
                error = [[OEPocketsphinxController sharedInstance] stopListening]; // Stop listening if we are listening.
                if(error) {
                    //NSLog(@&quot;Error while stopping listening in micPermissionCheckCompleted: %@&quot;, error);
                }
            }
            if(!error &amp;&amp; ![OEPocketsphinxController sharedInstance].isListening) { // If there was no error and we aren&#039;t listening, start listening.
                [[OEPocketsphinxController sharedInstance] startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&quot;AcousticModelEnglish&quot;] languageModelIsJSGF:FALSE]; // Start speech recognition.
                    self.startupFailedDueToLackOfPermissions = FALSE;
            }
        }
    }
}

#pragma mark -
#pragma mark UI

// This is not OpenEars-specific stuff, just some UI behavior

- (IBAction) suspendListeningButtonAction { // This is the action for the button which suspends listening without ending the recognition loop
    [[OEPocketsphinxController sharedInstance] suspendRecognition];	
    
    self.startButton.hidden = TRUE;
    self.stopButton.hidden = FALSE;
    self.suspendListeningButton.hidden = TRUE;
    self.resumeListeningButton.hidden = FALSE;
}

- (IBAction) resumeListeningButtonAction { // This is the action for the button which resumes listening if it has been suspended
    [[OEPocketsphinxController sharedInstance] resumeRecognition];
    
    self.startButton.hidden = TRUE;
    self.stopButton.hidden = FALSE;
    self.suspendListeningButton.hidden = FALSE;
    self.resumeListeningButton.hidden = TRUE;	
}

- (IBAction) stopButtonAction { // This is the action for the button which shuts down the recognition loop.
    NSError *error = nil;
    if([OEPocketsphinxController sharedInstance].isListening) { // Stop if we are currently listening.
        error = [[OEPocketsphinxController sharedInstance] stopListening];
        if(error){
            //NSLog(@&quot;Error stopping listening in stopButtonAction: %@&quot;, error);
        }
    }
    self.startButton.hidden = FALSE;
    self.stopButton.hidden = TRUE;
    self.suspendListeningButton.hidden = TRUE;
    self.resumeListeningButton.hidden = TRUE;
}

- (IBAction) startButtonAction { // This is the action for the button which starts up the recognition loop again if it has been shut down.
    if(![OEPocketsphinxController sharedInstance].isListening) {
        [[OEPocketsphinxController sharedInstance] startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&quot;AcousticModelEnglish&quot;] languageModelIsJSGF:FALSE]; // Start speech recognition if we aren&#039;t already listening.
    }
    self.startButton.hidden = TRUE;
    self.stopButton.hidden = FALSE;
    self.suspendListeningButton.hidden = FALSE;
    self.resumeListeningButton.hidden = TRUE;
}

#pragma mark -
#pragma mark Example for reading out Pocketsphinx and Flite audio levels without locking the UI by using an NSTimer

// What follows are not OpenEars methods, just an approach for level reading
// that I&#039;ve included with this sample app. My example implementation does make use of two OpenEars
// methods:	the pocketsphinxInputLevel method of OEPocketsphinxController and the fliteOutputLevel
// method of OEFliteController. 
//
// The example is meant to show one way that you can read those levels continuously without locking the UI, 
// by using an NSTimer, but the OpenEars level-reading methods 
// themselves do not include multithreading code since I believe that you will want to design your own 
// code approaches for level display that are tightly-integrated with your interaction design and the  
// graphics API you choose. 
// 
// Please note that if you use my sample approach, you should pay attention to the way that the timer is always stopped in
// dealloc. This should prevent you from having any difficulties with deallocating a class due to a running NSTimer process.

- (void) startDisplayingLevels { // Start displaying the levels using a timer
    [self stopDisplayingLevels]; // We never want more than one timer valid so we&#039;ll stop any running timers first.
    self.uiUpdateTimer = [NSTimer scheduledTimerWithTimeInterval:1.0/kLevelUpdatesPerSecond target:self selector:@selector(updateLevelsUI) userInfo:nil repeats:YES];
}

- (void) stopDisplayingLevels { // Stop displaying the levels by stopping the timer if it&#039;s running.
    if(self.uiUpdateTimer &amp;&amp; [self.uiUpdateTimer isValid]) { // If there is a running timer, we&#039;ll stop it here.
        [self.uiUpdateTimer invalidate];
        self.uiUpdateTimer = nil;
    }
}

- (void) updateLevelsUI { // And here is how we obtain the levels.  This method includes the actual OpenEars methods and uses their results to update the UI of this view controller.
    
    self.pocketsphinxDbLabel.text = [NSString stringWithFormat:@&quot;Pocketsphinx Input level:%f&quot;,[[OEPocketsphinxController sharedInstance] pocketsphinxInputLevel]];  //pocketsphinxInputLevel is an OpenEars method of the class OEPocketsphinxController.
    
    if(self.fliteController.speechInProgress) {
        self.fliteDbLabel.text = [NSString stringWithFormat:@&quot;Flite Output level: %f&quot;,[self.fliteController fliteOutputLevel]]; // fliteOutputLevel is an OpenEars method of the class OEFliteController.
    }
}

@end
</code></pre>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1024452" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">January 20, 2015 at 10:05 am</span>

		
			<span class="bbp-header">
				in reply to: 				<a class="bbp-topic-permalink" href="/forums/topic/openears-detect-the-word-without-speaking-it/">openEars detect the word without speaking it</a>
			</span>

		
		<a href="/forums/topic/openears-detect-the-word-without-speaking-it/#post-1024452" class="bbp-reply-permalink">#1024452</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1024452 -->

<div class="loop-item-2 user-id-1752 bbp-parent-forum-3654 bbp-parent-topic-1024409 bbp-reply-position-14 odd topic-author  post-1024452 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/maheshvaghela/" title="View maheshvaghela&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">maheshvaghela</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>OK, now I understand about which logging information you needed. I have written <code>[OELogging startOpenEarsLogging];</code> and comment out all other <code>NSLog</code> in <code>viewController.m</code> to get only this logging. There are some noise only while this log is generated, it is of 5 to 7 minutes running on ipad. </p>
<p>The result is below please check out. This is the log information.</p>
<pre><code>2015-01-20 14:28:05.402 OpenEarsSampleApp[5398:2164200] Starting OpenEars logging for OpenEars version 2.01 on 32-bit device (or build): iPad running iOS version: 8.000000
2015-01-20 14:28:06.641 OpenEarsSampleApp[5398:2164232] Speech detected...
2015-01-20 14:28:10.106 OpenEarsSampleApp[5398:2164232] End of speech detected...
2015-01-20 14:28:10.372 OpenEarsSampleApp[5398:2164232] Pocketsphinx heard &quot;TEN&quot; with a score of (-13194) and an utterance ID of 0.
2015-01-20 14:28:10.374 OpenEarsSampleApp[5398:2164200] Flite sending interrupt speech request.
2015-01-20 14:28:10.377 OpenEarsSampleApp[5398:2164200] I&#039;m running flite
2015-01-20 14:28:10.617 OpenEarsSampleApp[5398:2164200] I&#039;m done running flite and it took 0.240182 seconds
2015-01-20 14:28:10.618 OpenEarsSampleApp[5398:2164200] Flite audio player was nil when referenced so attempting to allocate a new audio player.
2015-01-20 14:28:10.619 OpenEarsSampleApp[5398:2164200] Loading speech data for Flite concluded successfully.
2015-01-20 14:28:10.727 OpenEarsSampleApp[5398:2164200] Flite sending suspend recognition notification.
2015-01-20 14:28:11.936 OpenEarsSampleApp[5398:2164200] AVAudioPlayer did finish playing with success flag of 1
2015-01-20 14:28:12.089 OpenEarsSampleApp[5398:2164200] Flite sending resume recognition notification.
2015-01-20 14:28:12.597 OpenEarsSampleApp[5398:2164200] Valid setSecondsOfSilence value of 0.200000 will be used.
2015-01-20 14:28:14.971 OpenEarsSampleApp[5398:2164231] Speech detected...
2015-01-20 14:28:18.299 OpenEarsSampleApp[5398:2164232] End of speech detected...
2015-01-20 14:28:18.421 OpenEarsSampleApp[5398:2164232] Pocketsphinx heard &quot;&quot; with a score of (0) and an utterance ID of 1.
2015-01-20 14:28:18.423 OpenEarsSampleApp[5398:2164232] Hypothesis was null so we aren&#039;t returning it. If you want null hypotheses to also be returned, set OEPocketsphinxController&#039;s property returnNullHypotheses to TRUE before starting OEPocketsphinxController.
2015-01-20 14:28:22.526 OpenEarsSampleApp[5398:2164232] Speech detected...
2015-01-20 14:28:23.287 OpenEarsSampleApp[5398:2164232] End of speech detected...
2015-01-20 14:28:23.326 OpenEarsSampleApp[5398:2164232] Pocketsphinx heard &quot;TWO&quot; with a score of (-398) and an utterance ID of 2.
2015-01-20 14:28:23.328 OpenEarsSampleApp[5398:2164200] Flite sending interrupt speech request.
2015-01-20 14:28:23.331 OpenEarsSampleApp[5398:2164200] I&#039;m running flite
2015-01-20 14:28:23.528 OpenEarsSampleApp[5398:2164200] I&#039;m done running flite and it took 0.196905 seconds
2015-01-20 14:28:23.529 OpenEarsSampleApp[5398:2164200] Flite audio player was nil when referenced so attempting to allocate a new audio player.
2015-01-20 14:28:23.530 OpenEarsSampleApp[5398:2164200] Loading speech data for Flite concluded successfully.
2015-01-20 14:28:23.637 OpenEarsSampleApp[5398:2164200] Flite sending suspend recognition notification.
2015-01-20 14:28:24.847 OpenEarsSampleApp[5398:2164200] AVAudioPlayer did finish playing with success flag of 1
2015-01-20 14:28:24.999 OpenEarsSampleApp[5398:2164200] Flite sending resume recognition notification.
2015-01-20 14:28:25.507 OpenEarsSampleApp[5398:2164200] Valid setSecondsOfSilence value of 0.200000 will be used.
2015-01-20 14:28:26.106 OpenEarsSampleApp[5398:2164231] Speech detected...
2015-01-20 14:28:27.482 OpenEarsSampleApp[5398:2164232] End of speech detected...
2015-01-20 14:28:27.532 OpenEarsSampleApp[5398:2164232] Pocketsphinx heard &quot;&quot; with a score of (0) and an utterance ID of 3.
2015-01-20 14:28:27.533 OpenEarsSampleApp[5398:2164232] Hypothesis was null so we aren&#039;t returning it. If you want null hypotheses to also be returned, set OEPocketsphinxController&#039;s property returnNullHypotheses to TRUE before starting OEPocketsphinxController.
2015-01-20 14:28:36.088 OpenEarsSampleApp[5398:2164231] Speech detected...
2015-01-20 14:28:36.857 OpenEarsSampleApp[5398:2164231] End of speech detected...
2015-01-20 14:28:36.888 OpenEarsSampleApp[5398:2164231] Pocketsphinx heard &quot;&quot; with a score of (1) and an utterance ID of 4.

2015-01-20 14:28:36.890 OpenEarsSampleApp[5398:2164231] Hypothesis was null so we aren&#039;t returning it. If you want null hypotheses to also be returned, set OEPocketsphinxController&#039;s property returnNullHypotheses to TRUE before starting OEPocketsphinxController.
2015-01-20 14:28:44.526 OpenEarsSampleApp[5398:2164232] Speech detected...
2015-01-20 14:28:45.684 OpenEarsSampleApp[5398:2164232] End of speech detected...
2015-01-20 14:28:45.727 OpenEarsSampleApp[5398:2164232] Pocketsphinx heard &quot;&quot; with a score of (0) and an utterance ID of 5.
2015-01-20 14:28:45.727 OpenEarsSampleApp[5398:2164232] Hypothesis was null so we aren&#039;t returning it. If you want null hypotheses to also be returned, set OEPocketsphinxController&#039;s property returnNullHypotheses to TRUE before starting OEPocketsphinxController.
2015-01-20 14:28:49.519 OpenEarsSampleApp[5398:2164232] Speech detected...
2015-01-20 14:28:50.796 OpenEarsSampleApp[5398:2164232] End of speech detected...
2015-01-20 14:28:50.845 OpenEarsSampleApp[5398:2164232] Pocketsphinx heard &quot;&quot; with a score of (0) and an utterance ID of 6.
2015-01-20 14:28:50.847 OpenEarsSampleApp[5398:2164232] Hypothesis was null so we aren&#039;t returning it. If you want null hypotheses to also be returned, set OEPocketsphinxController&#039;s property returnNullHypotheses to TRUE before starting OEPocketsphinxController.
2015-01-20 14:29:13.973 OpenEarsSampleApp[5398:2164232] Speech detected...
2015-01-20 14:29:14.609 OpenEarsSampleApp[5398:2164232] End of speech detected...
2015-01-20 14:29:14.638 OpenEarsSampleApp[5398:2164232] Pocketsphinx heard &quot;&quot; with a score of (0) and an utterance ID of 7.
2015-01-20 14:29:14.640 OpenEarsSampleApp[5398:2164232] Hypothesis was null so we aren&#039;t returning it. If you want null hypotheses to also be returned, set OEPocketsphinxController&#039;s property returnNullHypotheses to TRUE before starting OEPocketsphinxController.
2015-01-20 14:29:40.197 OpenEarsSampleApp[5398:2164231] Speech detected...
2015-01-20 14:29:41.738 OpenEarsSampleApp[5398:2164231] End of speech detected...
2015-01-20 14:29:41.785 OpenEarsSampleApp[5398:2164231] Pocketsphinx heard &quot;&quot; with a score of (1) and an utterance ID of 8.
2015-01-20 14:29:41.786 OpenEarsSampleApp[5398:2164231] Hypothesis was null so we aren&#039;t returning it. If you want null hypotheses to also be returned, set OEPocketsphinxController&#039;s property returnNullHypotheses to TRUE before starting OEPocketsphinxController.
2015-01-20 14:29:49.421 OpenEarsSampleApp[5398:2164231] Speech detected...
2015-01-20 14:29:50.327 OpenEarsSampleApp[5398:2164231] End of speech detected...
2015-01-20 14:29:50.356 OpenEarsSampleApp[5398:2164231] Pocketsphinx heard &quot;&quot; with a score of (0) and an utterance ID of 9.
2015-01-20 14:29:50.357 OpenEarsSampleApp[5398:2164231] Hypothesis was null so we aren&#039;t returning it. If you want null hypotheses to also be returned, set OEPocketsphinxController&#039;s property returnNullHypotheses to TRUE before starting OEPocketsphinxController.
2015-01-20 14:30:17.084 OpenEarsSampleApp[5398:2164231] Speech detected...
2015-01-20 14:30:18.111 OpenEarsSampleApp[5398:2164231] End of speech detected...
2015-01-20 14:30:18.152 OpenEarsSampleApp[5398:2164231] Pocketsphinx heard &quot;&quot; with a score of (-1) and an utterance ID of 10.
2015-01-20 14:30:18.153 OpenEarsSampleApp[5398:2164231] Hypothesis was null so we aren&#039;t returning it. If you want null hypotheses to also be returned, set OEPocketsphinxController&#039;s property returnNullHypotheses to TRUE before starting OEPocketsphinxController.
2015-01-20 14:30:29.632 OpenEarsSampleApp[5398:2164232] Speech detected...
2015-01-20 14:30:30.771 OpenEarsSampleApp[5398:2164232] End of speech detected...
2015-01-20 14:30:30.822 OpenEarsSampleApp[5398:2164232] Pocketsphinx heard &quot;&quot; with a score of (-1) and an utterance ID of 11.
2015-01-20 14:30:30.824 OpenEarsSampleApp[5398:2164232] Hypothesis was null so we aren&#039;t returning it. If you want null hypotheses to also be returned, set OEPocketsphinxController&#039;s property returnNullHypotheses to TRUE before starting OEPocketsphinxController.
2015-01-20 14:30:46.777 OpenEarsSampleApp[5398:2164231] Speech detected...
2015-01-20 14:30:50.459 OpenEarsSampleApp[5398:2164231] End of speech detected...
2015-01-20 14:30:50.614 OpenEarsSampleApp[5398:2164231] Pocketsphinx heard &quot;&quot; with a score of (0) and an utterance ID of 12.
2015-01-20 14:30:50.614 OpenEarsSampleApp[5398:2164231] Hypothesis was null so we aren&#039;t returning it. If you want null hypotheses to also be returned, set OEPocketsphinxController&#039;s property returnNullHypotheses to TRUE before starting OEPocketsphinxController.
2015-01-20 14:31:15.819 OpenEarsSampleApp[5398:2164231] Speech detected...
2015-01-20 14:31:16.584 OpenEarsSampleApp[5398:2164231] End of speech detected...
2015-01-20 14:31:16.616 OpenEarsSampleApp[5398:2164231] Pocketsphinx heard &quot;&quot; with a score of (0) and an utterance ID of 13.
2015-01-20 14:31:16.617 OpenEarsSampleApp[5398:2164231] Hypothesis was null so we aren&#039;t returning it. If you want null hypotheses to also be returned, set OEPocketsphinxController&#039;s property returnNullHypotheses to TRUE before starting OEPocketsphinxController.
2015-01-20 14:31:17.493 OpenEarsSampleApp[5398:2164231] Speech detected...
2015-01-20 14:31:18.261 OpenEarsSampleApp[5398:2164231] End of speech detected...
2015-01-20 14:31:18.288 OpenEarsSampleApp[5398:2164231] Pocketsphinx heard &quot;&quot; with a score of (-1) and an utterance ID of 14.
2015-01-20 14:31:18.289 OpenEarsSampleApp[5398:2164231] Hypothesis was null so we aren&#039;t returning it. If you want null hypotheses to also be returned, set OEPocketsphinxController&#039;s property returnNullHypotheses to TRUE before starting OEPocketsphinxController.
2015-01-20 14:31:23.516 OpenEarsSampleApp[5398:2164232] Speech detected...
2015-01-20 14:31:24.127 OpenEarsSampleApp[5398:2164232] End of speech detected...
2015-01-20 14:31:24.155 OpenEarsSampleApp[5398:2164232] Pocketsphinx heard &quot;&quot; with a score of (0) and an utterance ID of 15.
2015-01-20 14:31:24.155 OpenEarsSampleApp[5398:2164232] Hypothesis was null so we aren&#039;t returning it. If you want null hypotheses to also be returned, set OEPocketsphinxController&#039;s property returnNullHypotheses to TRUE before starting OEPocketsphinxController.
2015-01-20 14:33:00.527 OpenEarsSampleApp[5398:2164232] Speech detected...
2015-01-20 14:33:01.295 OpenEarsSampleApp[5398:2164232] End of speech detected...
2015-01-20 14:33:01.325 OpenEarsSampleApp[5398:2164232] Pocketsphinx heard &quot;&quot; with a score of (0) and an utterance ID of 16.
2015-01-20 14:33:01.326 OpenEarsSampleApp[5398:2164232] Hypothesis was null so we aren&#039;t returning it. If you want null hypotheses to also be returned, set OEPocketsphinxController&#039;s property returnNullHypotheses to TRUE before starting OEPocketsphinxController.
2015-01-20 14:33:02.819 OpenEarsSampleApp[5398:2164232] Speech detected...
2015-01-20 14:33:06.421 OpenEarsSampleApp[5398:2164232] End of speech detected...
2015-01-20 14:33:06.575 OpenEarsSampleApp[5398:2164232] Pocketsphinx heard &quot;&quot; with a score of (0) and an utterance ID of 17.
2015-01-20 14:33:06.577 OpenEarsSampleApp[5398:2164232] Hypothesis was null so we aren&#039;t returning it. If you want null hypotheses to also be returned, set OEPocketsphinxController&#039;s property returnNullHypotheses to TRUE before starting OEPocketsphinxController.
2015-01-20 14:33:11.410 OpenEarsSampleApp[5398:2164231] Speech detected...
2015-01-20 14:33:17.050 OpenEarsSampleApp[5398:2164231] End of speech detected...
2015-01-20 14:33:17.327 OpenEarsSampleApp[5398:2164231] Pocketsphinx heard &quot;&quot; with a score of (0) and an utterance ID of 18.
2015-01-20 14:33:17.328 OpenEarsSampleApp[5398:2164231] Hypothesis was null so we aren&#039;t returning it. If you want null hypotheses to also be returned, set OEPocketsphinxController&#039;s property returnNullHypotheses to TRUE before starting OEPocketsphinxController.
2015-01-20 14:33:18.188 OpenEarsSampleApp[5398:2164231] Speech detected...
2015-01-20 14:33:19.218 OpenEarsSampleApp[5398:2164232] End of speech detected...
2015-01-20 14:33:19.258 OpenEarsSampleApp[5398:2164232] Pocketsphinx heard &quot;&quot; with a score of (0) and an utterance ID of 19.
2015-01-20 14:33:19.259 OpenEarsSampleApp[5398:2164232] Hypothesis was null so we aren&#039;t returning it. If you want null hypotheses to also be returned, set OEPocketsphinxController&#039;s property returnNullHypotheses to TRUE before starting OEPocketsphinxController.
2015-01-20 14:33:19.715 OpenEarsSampleApp[5398:2164232] Speech detected...
2015-01-20 14:33:21.752 OpenEarsSampleApp[5398:2164232] End of speech detected...
2015-01-20 14:33:21.833 OpenEarsSampleApp[5398:2164232] Pocketsphinx heard &quot;TWO&quot; with a score of (-3) and an utterance ID of 20.
2015-01-20 14:33:21.834 OpenEarsSampleApp[5398:2164200] Flite sending interrupt speech request.
2015-01-20 14:33:21.835 OpenEarsSampleApp[5398:2164200] I&#039;m running flite
2015-01-20 14:33:22.031 OpenEarsSampleApp[5398:2164200] I&#039;m done running flite and it took 0.195895 seconds
2015-01-20 14:33:22.032 OpenEarsSampleApp[5398:2164200] Flite audio player was nil when referenced so attempting to allocate a new audio player.
2015-01-20 14:33:22.033 OpenEarsSampleApp[5398:2164200] Loading speech data for Flite concluded successfully.
2015-01-20 14:33:22.060 OpenEarsSampleApp[5398:2164200] Flite sending suspend recognition notification.
2015-01-20 14:33:23.269 OpenEarsSampleApp[5398:2164200] AVAudioPlayer did finish playing with success flag of 1
2015-01-20 14:33:23.421 OpenEarsSampleApp[5398:2164200] Flite sending resume recognition notification.
2015-01-20 14:33:23.929 OpenEarsSampleApp[5398:2164200] Valid setSecondsOfSilence value of 0.200000 will be used.
2015-01-20 14:33:24.216 OpenEarsSampleApp[5398:2164231] Speech detected...
2015-01-20 14:33:27.289 OpenEarsSampleApp[5398:2164231] End of speech detected...
2015-01-20 14:33:27.616 OpenEarsSampleApp[5398:2164231] Pocketsphinx heard &quot;EIGHT&quot; with a score of (-5676) and an utterance ID of 21.</code></pre>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1024450" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">January 20, 2015 at 9:36 am</span>

		
			<span class="bbp-header">
				in reply to: 				<a class="bbp-topic-permalink" href="/forums/topic/openears-detect-the-word-without-speaking-it/">openEars detect the word without speaking it</a>
			</span>

		
		<a href="/forums/topic/openears-detect-the-word-without-speaking-it/#post-1024450" class="bbp-reply-permalink">#1024450</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1024450 -->

<div class="loop-item-3 user-id-1752 bbp-parent-forum-3654 bbp-parent-topic-1024409 bbp-reply-position-12 even topic-author  post-1024450 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/maheshvaghela/" title="View maheshvaghela&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">maheshvaghela</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>yes. There are two situation when problem is occurring. your given last two situation.</p>
<p>• There is noise in the environment and when the app hears the noise, it is giving a hypothesis which is a word in the vocabulary.<br>
• There is human speech in the environment and when the app hears speech it is giving a hypothesis which is a word in the vocabulary, but the speech was not any of the words in the vocabulary.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1024448" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">January 20, 2015 at 8:19 am</span>

		
			<span class="bbp-header">
				in reply to: 				<a class="bbp-topic-permalink" href="/forums/topic/openears-detect-the-word-without-speaking-it/">openEars detect the word without speaking it</a>
			</span>

		
		<a href="/forums/topic/openears-detect-the-word-without-speaking-it/#post-1024448" class="bbp-reply-permalink">#1024448</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1024448 -->

<div class="loop-item-4 user-id-1752 bbp-parent-forum-3654 bbp-parent-topic-1024409 bbp-reply-position-10 odd topic-author  post-1024448 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/maheshvaghela/" title="View maheshvaghela&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">maheshvaghela</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>yes. I update it to 4. As i read somewhere that Threshold value 2 to 4 is an average value.</p>
<p>[[OEPocketsphinxController sharedInstance] setVadThreshold:4.0];</p>
<p>then also that problem occurs.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1024442" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">January 20, 2015 at 5:08 am</span>

		
			<span class="bbp-header">
				in reply to: 				<a class="bbp-topic-permalink" href="/forums/topic/openears-detect-the-word-without-speaking-it/">openEars detect the word without speaking it</a>
			</span>

		
		<a href="/forums/topic/openears-detect-the-word-without-speaking-it/#post-1024442" class="bbp-reply-permalink">#1024442</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1024442 -->

<div class="loop-item-5 user-id-1752 bbp-parent-forum-3654 bbp-parent-topic-1024409 bbp-reply-position-8 even topic-author  post-1024442 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/maheshvaghela/" title="View maheshvaghela&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">maheshvaghela</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Ok, I understand something from your last response that it is a noise problem. I will try the rejecto demo also to verify it. I have copied some log information of 4 to 6 minutes. I have run the sample app and just put ipad as it is and don&#8217;t spoke any word. You can see the log information here. In log, you see the words detected and also understand the problem better. And thank you for your Great support. I really appreciate it. Thanks a lot.</p>
<pre><code>2015-01-20 09:26:34.850 OpenEarsSampleApp[5343:2126098] Array : (
    ONE,
    TWO,
    THREE,
    FOUR,
    FIVE,
    SIX,
    SEVEN,
    EIGHT,
    NINE,
    TEN,
    ELEVEN,
    TWELVE,
    THIRTEEN,
    FOURTEEN,
    FIFTEEN,
    SIXTEEN,
    SEVENTEEN,
    EIGHTEEN,
    NINETEEN,
    TWENTY,
    &quot;TWENTY ONE&quot;,
    &quot;TWENTY TWO&quot;,
    &quot;TWENTY THREE&quot;,
    &quot;TWENTY FOUR&quot;,
    &quot;TWENTY FIVE&quot;,
    &quot;TWENTY SIX&quot;
)

2015-01-20 09:26:36.396 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx is now listening.
2015-01-20 09:26:36.400 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx started.
2015-01-20 09:26:36.854 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected speech.
2015-01-20 09:26:40.936 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected a second of silence, concluding an utterance.
2015-01-20 09:26:42.345 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected speech.
2015-01-20 09:26:45.926 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected a second of silence, concluding an utterance.
2015-01-20 09:26:46.265 OpenEarsSampleApp[5343:2126098] Local callback: The received hypothesis is TWO with a score of -37 and an ID of 1
2015-01-20 09:26:46.606 OpenEarsSampleApp[5343:2126098] Local callback: Flite has started speaking
2015-01-20 09:26:46.616 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has suspended recognition.
2015-01-20 09:26:48.464 OpenEarsSampleApp[5343:2126098] Local callback: Flite has finished speaking
2015-01-20 09:26:48.471 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has resumed recognition.
2015-01-20 09:26:48.753 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected speech.
2015-01-20 09:26:50.142 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected a second of silence, concluding an utterance.
2015-01-20 09:26:52.078 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected speech.
2015-01-20 09:26:59.487 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected a second of silence, concluding an utterance.
2015-01-20 09:26:59.900 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected speech.
2015-01-20 09:27:04.745 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected a second of silence, concluding an utterance.
2015-01-20 09:27:07.182 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected speech.
2015-01-20 09:27:15.247 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected a second of silence, concluding an utterance.
2015-01-20 09:27:15.935 OpenEarsSampleApp[5343:2126098] Local callback: The received hypothesis is EIGHT with a score of -1 and an ID of 5
2015-01-20 09:27:16.231 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected speech.
2015-01-20 09:27:16.234 OpenEarsSampleApp[5343:2126098] Local callback: Flite has started speaking
2015-01-20 09:27:16.244 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has suspended recognition.
2015-01-20 09:27:18.093 OpenEarsSampleApp[5343:2126098] Local callback: Flite has finished speaking
2015-01-20 09:27:18.100 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has resumed recognition.
2015-01-20 09:27:18.577 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected speech.
2015-01-20 09:27:20.362 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected a second of silence, concluding an utterance.
2015-01-20 09:27:20.991 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected speech.
2015-01-20 09:27:36.236 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected a second of silence, concluding an utterance.
2015-01-20 09:27:42.689 OpenEarsSampleApp[5343:2126098] Local callback: The received hypothesis is TWO with a score of -5112 and an ID of 7
2015-01-20 09:27:42.984 OpenEarsSampleApp[5343:2126098] Local callback: Flite has started speaking
2015-01-20 09:27:42.994 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has suspended recognition.
2015-01-20 09:27:43.057 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected speech.
2015-01-20 09:27:44.844 OpenEarsSampleApp[5343:2126098] Local callback: Flite has finished speaking
2015-01-20 09:27:44.851 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has resumed recognition.
2015-01-20 09:27:45.145 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected speech.
2015-01-20 09:28:01.832 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected a second of silence, concluding an utterance.
2015-01-20 09:28:04.503 OpenEarsSampleApp[5343:2126098] Local callback: The received hypothesis is EIGHT with a score of -14729 and an ID of 8
2015-01-20 09:28:04.807 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected speech.
2015-01-20 09:28:04.810 OpenEarsSampleApp[5343:2126098] Local callback: Flite has started speaking
2015-01-20 09:28:04.819 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has suspended recognition.
2015-01-20 09:28:06.668 OpenEarsSampleApp[5343:2126098] Local callback: Flite has finished speaking
2015-01-20 09:28:06.675 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has resumed recognition.
2015-01-20 09:28:07.080 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected speech.
2015-01-20 09:28:20.646 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected a second of silence, concluding an utterance.
2015-01-20 09:28:24.881 OpenEarsSampleApp[5343:2126098] Local callback: The received hypothesis is EIGHT with a score of -2260 and an ID of 9
2015-01-20 09:28:25.147 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected speech.
2015-01-20 09:28:25.151 OpenEarsSampleApp[5343:2126098] Local callback: Flite has started speaking
2015-01-20 09:28:25.160 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has suspended recognition.
2015-01-20 09:28:27.011 OpenEarsSampleApp[5343:2126098] Local callback: Flite has finished speaking
2015-01-20 09:28:27.018 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has resumed recognition.
2015-01-20 09:28:28.339 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected speech.
2015-01-20 09:28:53.420 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected a second of silence, concluding an utterance.
2015-01-20 09:28:54.347 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected speech.
2015-01-20 09:29:07.489 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected a second of silence, concluding an utterance.
2015-01-20 09:29:12.507 OpenEarsSampleApp[5343:2126098] Local callback: The received hypothesis is EIGHT with a score of -2826 and an ID of 10
2015-01-20 09:29:12.798 OpenEarsSampleApp[5343:2126098] Local callback: Flite has started speaking
2015-01-20 09:29:12.809 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has suspended recognition.
2015-01-20 09:29:12.812 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected speech.
2015-01-20 09:29:14.656 OpenEarsSampleApp[5343:2126098] Local callback: Flite has finished speaking
2015-01-20 09:29:14.664 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has resumed recognition.
2015-01-20 09:29:15.565 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected speech.
2015-01-20 09:29:23.366 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected a second of silence, concluding an utterance.
2015-01-20 09:29:24.991 OpenEarsSampleApp[5343:2126098] Local callback: The received hypothesis is EIGHT with a score of -2475 and an ID of 11
2015-01-20 09:29:25.240 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected speech.
2015-01-20 09:29:25.243 OpenEarsSampleApp[5343:2126098] Local callback: Flite has started speaking
2015-01-20 09:29:25.253 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has suspended recognition.
2015-01-20 09:29:27.103 OpenEarsSampleApp[5343:2126098] Local callback: Flite has finished speaking
2015-01-20 09:29:27.110 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has resumed recognition.
2015-01-20 09:29:27.469 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected speech.
2015-01-20 09:29:34.742 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected a second of silence, concluding an utterance.
2015-01-20 09:29:35.225 OpenEarsSampleApp[5343:2126098] Local callback: The received hypothesis is EIGHT with a score of -9302 and an ID of 12
2015-01-20 09:29:35.460 OpenEarsSampleApp[5343:2126098] Local callback: Flite has started speaking
2015-01-20 09:29:35.471 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has suspended recognition.
2015-01-20 09:29:37.320 OpenEarsSampleApp[5343:2126098] Local callback: Flite has finished speaking
2015-01-20 09:29:37.327 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has resumed recognition.
2015-01-20 09:29:37.582 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected speech.
2015-01-20 09:29:45.495 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected a second of silence, concluding an utterance.
2015-01-20 09:29:46.977 OpenEarsSampleApp[5343:2126098] Local callback: The received hypothesis is EIGHT with a score of -26102 and an ID of 13
2015-01-20 09:29:47.253 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected speech.
2015-01-20 09:29:47.256 OpenEarsSampleApp[5343:2126098] Local callback: Flite has started speaking
2015-01-20 09:29:47.265 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has suspended recognition.
2015-01-20 09:29:49.115 OpenEarsSampleApp[5343:2126098] Local callback: Flite has finished speaking
2015-01-20 09:29:49.122 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has resumed recognition.
2015-01-20 09:29:49.360 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected speech.
2015-01-20 09:30:14.422 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected a second of silence, concluding an utterance.
2015-01-20 09:30:15.507 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected speech.
2015-01-20 09:30:40.548 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected a second of silence, concluding an utterance.
2015-01-20 09:30:41.608 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected speech.
2015-01-20 09:31:06.665 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected a second of silence, concluding an utterance.
2015-01-20 09:31:07.708 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected speech.
2015-01-20 09:31:32.778 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected a second of silence, concluding an utterance.
2015-01-20 09:31:33.731 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected speech.
2015-01-20 09:31:42.111 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected a second of silence, concluding an utterance.
2015-01-20 09:31:42.784 OpenEarsSampleApp[5343:2126098] Local callback: The received hypothesis is EIGHT with a score of -4186 and an ID of 14
2015-01-20 09:31:43.074 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected speech.
2015-01-20 09:31:43.077 OpenEarsSampleApp[5343:2126098] Local callback: Flite has started speaking
2015-01-20 09:31:43.086 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has suspended recognition.
2015-01-20 09:31:44.936 OpenEarsSampleApp[5343:2126098] Local callback: Flite has finished speaking
2015-01-20 09:31:44.943 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has resumed recognition.
2015-01-20 09:31:45.192 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected speech.
2015-01-20 09:31:52.484 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected a second of silence, concluding an utterance.
2015-01-20 09:31:53.642 OpenEarsSampleApp[5343:2126098] Local callback: The received hypothesis is EIGHT with a score of -465 and an ID of 15
2015-01-20 09:31:53.940 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected speech.
2015-01-20 09:31:53.944 OpenEarsSampleApp[5343:2126098] Local callback: Flite has started speaking
2015-01-20 09:31:53.953 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has suspended recognition.
2015-01-20 09:31:55.803 OpenEarsSampleApp[5343:2126098] Local callback: Flite has finished speaking
2015-01-20 09:31:55.811 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has resumed recognition.
2015-01-20 09:31:56.583 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected speech.
2015-01-20 09:32:18.854 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected a second of silence, concluding an utterance.
2015-01-20 09:32:36.577 OpenEarsSampleApp[5343:2126098] Local callback: The received hypothesis is EIGHT TWO TWO EIGHT with a score of -13997 and an ID of 16
2015-01-20 09:32:36.948 OpenEarsSampleApp[5343:2126098] Local callback: Flite has started speaking
2015-01-20 09:32:36.958 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has suspended recognition.
2015-01-20 09:32:37.673 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected speech.
2015-01-20 09:32:39.270 OpenEarsSampleApp[5343:2126098] Local callback: Flite has finished speaking
2015-01-20 09:32:39.277 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has resumed recognition.
2015-01-20 09:32:40.096 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected speech.
2015-01-20 09:33:05.197 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected a second of silence, concluding an utterance.
2015-01-20 09:33:06.217 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected speech.
2015-01-20 09:33:30.669 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected a second of silence, concluding an utterance.
2015-01-20 09:33:55.198 OpenEarsSampleApp[5343:2126098] Local callback: The received hypothesis is EIGHT EIGHT EIGHT EIGHT EIGHT with a score of -27370 and an ID of 17
2015-01-20 09:33:55.525 OpenEarsSampleApp[5343:2126098] Local callback: Flite has started speaking
2015-01-20 09:33:55.536 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has suspended recognition.
2015-01-20 09:33:56.691 OpenEarsSampleApp[5343:2126098] Local callback: Pocketsphinx has detected speech.
2015-01-20 09:33:57.847 OpenEarsSampleApp[5343:2126098] Local callback: Flite has finished speaking</code></pre>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1024434" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">January 19, 2015 at 2:32 pm</span>

		
			<span class="bbp-header">
				in reply to: 				<a class="bbp-topic-permalink" href="/forums/topic/openears-detect-the-word-without-speaking-it/">openEars detect the word without speaking it</a>
			</span>

		
		<a href="/forums/topic/openears-detect-the-word-without-speaking-it/#post-1024434" class="bbp-reply-permalink">#1024434</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1024434 -->

<div class="loop-item-6 user-id-1752 bbp-parent-forum-3654 bbp-parent-topic-1024409 bbp-reply-position-6 odd topic-author  post-1024434 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/maheshvaghela/" title="View maheshvaghela&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">maheshvaghela</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Actually I don&#8217;t understand which logging information I have to provide because there is no any crashing or any other event. The app works fine with my words when I spoke. The problem is it recognize the other sound and respond from given words. For example if there is any other noise or sound then it respond &#8220;you said TWO&#8221; etc. yesterday some sparrow at window making some noise and our app say &#8220;you said EIGHT&#8221;. I meant to say that it recognize correct all my words but it also give result in some other utterances also.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1024432" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">January 19, 2015 at 2:10 pm</span>

		
			<span class="bbp-header">
				in reply to: 				<a class="bbp-topic-permalink" href="/forums/topic/openears-detect-the-word-without-speaking-it/">openEars detect the word without speaking it</a>
			</span>

		
		<a href="/forums/topic/openears-detect-the-word-without-speaking-it/#post-1024432" class="bbp-reply-permalink">#1024432</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1024432 -->

<div class="loop-item-7 user-id-1752 bbp-parent-forum-3654 bbp-parent-topic-1024409 bbp-reply-position-4 even topic-author  post-1024432 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/maheshvaghela/" title="View maheshvaghela&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">maheshvaghela</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Ok. I have read a given page link information before and also I have latest version downloaded yesterday. I will show you my viewController.m code below</p>
<pre><code>#import &quot;ViewController.h&quot;
#import &lt;OpenEars/OEPocketsphinxController.h&gt;
#import &lt;OpenEars/OEFliteController.h&gt;
#import &lt;OpenEars/OELanguageModelGenerator.h&gt;
#import &lt;OpenEars/OELogging.h&gt;
#import &lt;OpenEars/OEAcousticModel.h&gt;
#import &lt;Slt/Slt.h&gt;

@interface ViewController()

// UI actions, not specifically related to OpenEars other than the fact that they invoke OpenEars methods.
- (IBAction) stopButtonAction;
- (IBAction) startButtonAction;
- (IBAction) suspendListeningButtonAction;
- (IBAction) resumeListeningButtonAction;

// Example for reading out the input audio levels without locking the UI using an NSTimer

- (void) startDisplayingLevels;
- (void) stopDisplayingLevels;

// These three are the important OpenEars objects that this class demonstrates the use of.
@property (nonatomic, strong) Slt *slt;

@property (nonatomic, strong) OEEventsObserver *openEarsEventsObserver;
@property (nonatomic, strong) OEPocketsphinxController *pocketsphinxController;
@property (nonatomic, strong) OEFliteController *fliteController;

// Some UI, not specifically related to OpenEars.
@property (nonatomic, strong) IBOutlet UIButton *stopButton;
@property (nonatomic, strong) IBOutlet UIButton *startButton;
@property (nonatomic, strong) IBOutlet UIButton *suspendListeningButton;	
@property (nonatomic, strong) IBOutlet UIButton *resumeListeningButton;	
@property (nonatomic, strong) IBOutlet UITextView *statusTextView;
@property (nonatomic, strong) IBOutlet UITextView *heardTextView;
@property (nonatomic, strong) IBOutlet UILabel *pocketsphinxDbLabel;
@property (nonatomic, strong) IBOutlet UILabel *fliteDbLabel;
@property (nonatomic, assign) BOOL usingStartingLanguageModel;
@property (nonatomic, assign) int restartAttemptsDueToPermissionRequests;
@property (nonatomic, assign) BOOL startupFailedDueToLackOfPermissions;

// Things which help us show off the dynamic language features.
@property (nonatomic, copy) NSString *pathToFirstDynamicallyGeneratedLanguageModel;
@property (nonatomic, copy) NSString *pathToFirstDynamicallyGeneratedDictionary;
@property (nonatomic, copy) NSString *pathToSecondDynamicallyGeneratedLanguageModel;
@property (nonatomic, copy) NSString *pathToSecondDynamicallyGeneratedDictionary;

// Our NSTimer that will help us read and display the input and output levels without locking the UI
@property (nonatomic, strong) 	NSTimer *uiUpdateTimer;

@end

@implementation ViewController

#define kLevelUpdatesPerSecond 18 // We&#039;ll have the ui update 18 times a second to show some fluidity without hitting the CPU too hard.

//#define kGetNbest // Uncomment this if you want to try out nbest
#pragma mark - 
#pragma mark Memory Management

- (void)dealloc {
    [self stopDisplayingLevels];
}

#pragma mark - int to word converter

-(NSString *)GetWordOfInteger:(int)anInt
{
    NSNumber *numberValue = [NSNumber numberWithInt:anInt]; //needs to be NSNumber!
    NSNumberFormatter *numberFormatter = [[NSNumberFormatter alloc] init];
    [numberFormatter setNumberStyle:NSNumberFormatterSpellOutStyle];
    NSString *wordNumber = [numberFormatter stringFromNumber:numberValue];
    wordNumber = [[wordNumber stringByReplacingOccurrencesOfString:@&quot;-&quot; withString:@&quot; &quot;] uppercaseString];
    //NSLog(@&quot;Answer: %@&quot;, wordNumber);
    return wordNumber;
}

#pragma mark View Lifecycle

- (void)viewDidLoad {
    [super viewDidLoad];
    self.fliteController = [[OEFliteController alloc] init];
    self.openEarsEventsObserver = [[OEEventsObserver alloc] init];
    self.openEarsEventsObserver.delegate = self;
    self.slt = [[Slt alloc] init];
    
    self.restartAttemptsDueToPermissionRequests = 0;
    self.startupFailedDueToLackOfPermissions = FALSE;
    
    // [OELogging startOpenEarsLogging]; // Uncomment me for OELogging, which is verbose logging about internal OpenEars operations such as audio settings. If you have issues, show this logging in the forums.
    //[OEPocketsphinxController sharedInstance].verbosePocketSphinx = TRUE; // Uncomment this for much more verbose speech recognition engine output. If you have issues, show this logging in the forums.
    
    [self.openEarsEventsObserver setDelegate:self]; // Make this class the delegate of OpenEarsObserver so we can get all of the messages about what OpenEars is doing.
    
    [[OEPocketsphinxController sharedInstance] setActive:TRUE error:nil]; // Call this before setting any OEPocketsphinxController characteristics
    
    // This is the language model we&#039;re going to start up with. The only reason I&#039;m making it a class property is that I reuse it a bunch of times in this example, 
    // but you can pass the string contents directly to OEPocketsphinxController:startListeningWithLanguageModelAtPath:dictionaryAtPath:languageModelIsJSGF:
    
    NSMutableArray *numberArray = [[NSMutableArray alloc] init];
    
    
    for(int i=1;i&lt;=26;i++)
    {
        [numberArray addObject:[self GetWordOfInteger:i]];
    }
    
    
    NSLog(@&quot;Array : %@&quot;,numberArray);
    
    NSArray *firstLanguageArray = [[NSArray alloc] initWithArray:numberArray];
    
   /* NSArray *firstLanguageArray = @[@&quot;BACKWARD&quot;,
                                    @&quot;CHANGE&quot;,
                                    @&quot;FORWARD&quot;,
                                    @&quot;GO&quot;,
                                    @&quot;LEFT&quot;,
                                    @&quot;MODEL&quot;,
                                    @&quot;RIGHT&quot;,
                                    @&quot;TURN&quot;];*/
    
    OELanguageModelGenerator *languageModelGenerator = [[OELanguageModelGenerator alloc] init]; 
    
    // languageModelGenerator.verboseLanguageModelGenerator = TRUE; // Uncomment me for verbose language model generator debug output.

    NSError *error = [languageModelGenerator generateLanguageModelFromArray:firstLanguageArray withFilesNamed:@&quot;FirstOpenEarsDynamicLanguageModel&quot; forAcousticModelAtPath:[OEAcousticModel pathToModel:@&quot;AcousticModelEnglish&quot;]]; // Change &quot;AcousticModelEnglish&quot; to &quot;AcousticModelSpanish&quot; in order to create a language model for Spanish recognition instead of English.
    
    
    if(error) {
        NSLog(@&quot;Dynamic language generator reported error %@&quot;, [error description]);	
    } else {
        self.pathToFirstDynamicallyGeneratedLanguageModel = [languageModelGenerator pathToSuccessfullyGeneratedLanguageModelWithRequestedName:@&quot;FirstOpenEarsDynamicLanguageModel&quot;];
        self.pathToFirstDynamicallyGeneratedDictionary = [languageModelGenerator pathToSuccessfullyGeneratedDictionaryWithRequestedName:@&quot;FirstOpenEarsDynamicLanguageModel&quot;];
        
        
    }
    
    self.usingStartingLanguageModel = TRUE; // This is not an OpenEars thing, this is just so I can switch back and forth between the two models in this sample app.
    
    // Here is an example of dynamically creating an in-app grammar.
    
    // We want it to be able to response to the speech &quot;CHANGE MODEL&quot; and a few other things.  Items we want to have recognized as a whole phrase (like &quot;CHANGE MODEL&quot;) 
    // we put into the array as one string (e.g. &quot;CHANGE MODEL&quot; instead of &quot;CHANGE&quot; and &quot;MODEL&quot;). This increases the probability that they will be recognized as a phrase. This works even better starting with version 1.0 of OpenEars.
    
    NSArray *secondLanguageArray = @[@&quot;SUNDAY&quot;,
                                     @&quot;MONDAY&quot;,
                                     @&quot;TUESDAY&quot;,
                                     @&quot;WEDNESDAY&quot;,
                                     @&quot;THURSDAY&quot;,
                                     @&quot;FRIDAY&quot;,
                                     @&quot;SATURDAY&quot;,
                                     @&quot;QUIDNUNC&quot;,
                                     @&quot;CHANGE MODEL&quot;];
    
    // The last entry, quidnunc, is an example of a word which will not be found in the lookup dictionary and will be passed to the fallback method. The fallback method is slower,
    // so, for instance, creating a new language model from dictionary words will be pretty fast, but a model that has a lot of unusual names in it or invented/rare/recent-slang
    // words will be slower to generate. You can use this information to give your users good UI feedback about what the expectations for wait times should be.
    
    // I don&#039;t think it&#039;s beneficial to lazily instantiate OELanguageModelGenerator because you only need to give it a single message and then release it.
    // If you need to create a very large model or any size of model that has many unusual words that have to make use of the fallback generation method,
    // you will want to run this on a background thread so you can give the user some UI feedback that the task is in progress.
        
    // generateLanguageModelFromArray:withFilesNamed returns an NSError which will either have a value of noErr if everything went fine or a specific error if it didn&#039;t.
    error = [languageModelGenerator generateLanguageModelFromArray:secondLanguageArray withFilesNamed:@&quot;SecondOpenEarsDynamicLanguageModel&quot; forAcousticModelAtPath:[OEAcousticModel pathToModel:@&quot;AcousticModelEnglish&quot;]]; // Change &quot;AcousticModelEnglish&quot; to &quot;AcousticModelSpanish&quot; in order to create a language model for Spanish recognition instead of English.
    
    //    NSError *error = [languageModelGenerator generateLanguageModelFromTextFile:[NSString stringWithFormat:@&quot;%@/%@&quot;,[[NSBundle mainBundle] resourcePath], @&quot;OpenEarsCorpus.txt&quot;] withFilesNamed:@&quot;SecondOpenEarsDynamicLanguageModel&quot; forAcousticModelAtPath:[OEAcousticModel pathToModel:@&quot;AcousticModelEnglish&quot;]]; // Try this out to see how generating a language model from a corpus works.
    
    
    if(error) {
        NSLog(@&quot;Dynamic language generator reported error %@&quot;, [error description]);	
    }	else {
        
        self.pathToSecondDynamicallyGeneratedLanguageModel = [languageModelGenerator pathToSuccessfullyGeneratedLanguageModelWithRequestedName:@&quot;SecondOpenEarsDynamicLanguageModel&quot;]; // We&#039;ll set our new .languagemodel file to be the one to get switched to when the words &quot;CHANGE MODEL&quot; are recognized.
        self.pathToSecondDynamicallyGeneratedDictionary = [languageModelGenerator pathToSuccessfullyGeneratedDictionaryWithRequestedName:@&quot;SecondOpenEarsDynamicLanguageModel&quot;];; // We&#039;ll set our new dictionary to be the one to get switched to when the words &quot;CHANGE MODEL&quot; are recognized.
        
        // Next, an informative message.
        
        //NSLog(@&quot;\n\nWelcome to the OpenEars sample project. This project understands the words:\nBACKWARD,\nCHANGE,\nFORWARD,\nGO,\nLEFT,\nMODEL,\nRIGHT,\nTURN,\nand if you say \&quot;CHANGE MODEL\&quot; it will switch to its dynamically-generated model which understands the words:\nCHANGE,\nMODEL,\nMONDAY,\nTUESDAY,\nWEDNESDAY,\nTHURSDAY,\nFRIDAY,\nSATURDAY,\nSUNDAY,\nQUIDNUNC&quot;);
        
        // This is how to start the continuous listening loop of an available instance of OEPocketsphinxController. We won&#039;t do this if the language generation failed since it will be listening for a command to change over to the generated language.
        
        [[OEPocketsphinxController sharedInstance] setActive:TRUE error:nil]; // Call this once before setting properties of the OEPocketsphinxController instance.

        //   [OEPocketsphinxController sharedInstance].pathToTestFile = [[NSBundle mainBundle] pathForResource:@&quot;change_model_short&quot; ofType:@&quot;wav&quot;];  // This is how you could use a test WAV (mono/16-bit/16k) rather than live recognition
        
        if(![OEPocketsphinxController sharedInstance].isListening) {
            [[OEPocketsphinxController sharedInstance] startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&quot;AcousticModelEnglish&quot;] languageModelIsJSGF:FALSE]; // Start speech recognition if we aren&#039;t already listening.
        }
        // [self startDisplayingLevels] is not an OpenEars method, just a very simple approach for level reading
        // that I&#039;ve included with this sample app. My example implementation does make use of two OpenEars
        // methods:	the pocketsphinxInputLevel method of OEPocketsphinxController and the fliteOutputLevel
        // method of fliteController. 
        //
        // The example is meant to show one way that you can read those levels continuously without locking the UI, 
        // by using an NSTimer, but the OpenEars level-reading methods 
        // themselves do not include multithreading code since I believe that you will want to design your own 
        // code approaches for level display that are tightly-integrated with your interaction design and the  
        // graphics API you choose. 
   
        [self startDisplayingLevels];
        
        // Here is some UI stuff that has nothing specifically to do with OpenEars implementation
        self.startButton.hidden = TRUE;
        self.stopButton.hidden = TRUE;
        self.suspendListeningButton.hidden = TRUE;
        self.resumeListeningButton.hidden = TRUE;
    }
    
    [[OEPocketsphinxController sharedInstance] setSecondsOfSilenceToDetect:0.2];
    [[OEPocketsphinxController sharedInstance] setVadThreshold:3.0];
}

#pragma mark -
#pragma mark OEEventsObserver delegate methods

// What follows are all of the delegate methods you can optionally use once you&#039;ve instantiated an OEEventsObserver and set its delegate to self. 
// I&#039;ve provided some pretty granular information about the exact phase of the Pocketsphinx listening loop, the Audio Session, and Flite, but I&#039;d expect 
// that the ones that will really be needed by most projects are the following:
//
//- (void) pocketsphinxDidReceiveHypothesis:(NSString *)hypothesis recognitionScore:(NSString *)recognitionScore utteranceID:(NSString *)utteranceID;
//- (void) audioSessionInterruptionDidBegin;
//- (void) audioSessionInterruptionDidEnd;
//- (void) audioRouteDidChangeToRoute:(NSString *)newRoute;
//- (void) pocketsphinxDidStartListening;
//- (void) pocketsphinxDidStopListening;
//
// It isn&#039;t necessary to have a OEPocketsphinxController or a OEFliteController instantiated in order to use these methods.  If there isn&#039;t anything instantiated that will
// send messages to an OEEventsObserver, all that will happen is that these methods will never fire.  You also do not have to create a OEEventsObserver in
// the same class or view controller in which you are doing things with a OEPocketsphinxController or OEFliteController; you can receive updates from those objects in
// any class in which you instantiate an OEEventsObserver and set its delegate to self.

// This is an optional delegate method of OEEventsObserver which delivers the text of speech that Pocketsphinx heard and analyzed, along with its accuracy score and utterance ID.
- (void) pocketsphinxDidReceiveHypothesis:(NSString *)hypothesis recognitionScore:(NSString *)recognitionScore utteranceID:(NSString *)utteranceID {
    
    NSLog(@&quot;Local callback: The received hypothesis is %@ with a score of %@ and an ID of %@&quot;, hypothesis, recognitionScore, utteranceID); // Log it.
    if([hypothesis isEqualToString:@&quot;CHANGE MODEL&quot;]) { // If the user says &quot;CHANGE MODEL&quot;, we will switch to the alternate model (which happens to be the dynamically generated model).
        
        // Here is an example of language model switching in OpenEars. Deciding on what logical basis to switch models is your responsibility.
        // For instance, when you call a customer service line and get a response tree that takes you through different options depending on what you say to it,
        // the models are being switched as you progress through it so that only relevant choices can be understood. The construction of that logical branching and 
        // how to react to it is your job, OpenEars just lets you send the signal to switch the language model when you&#039;ve decided it&#039;s the right time to do so.
        
        if(self.usingStartingLanguageModel) { // If we&#039;re on the starting model, switch to the dynamically generated one.
            
            // You can only change language models with ARPA grammars in OpenEars (the ones that end in .languagemodel or .DMP). 
            // Trying to switch between JSGF models (the ones that end in .gram) will return no result.
            [[OEPocketsphinxController sharedInstance] changeLanguageModelToFile:self.pathToSecondDynamicallyGeneratedLanguageModel withDictionary:self.pathToSecondDynamicallyGeneratedDictionary]; 
            self.usingStartingLanguageModel = FALSE;
        } else { // If we&#039;re on the dynamically generated model, switch to the start model (this is just an example of a trigger and method for switching models).
            [[OEPocketsphinxController sharedInstance] changeLanguageModelToFile:self.pathToFirstDynamicallyGeneratedLanguageModel withDictionary:self.pathToFirstDynamicallyGeneratedDictionary];
            self.usingStartingLanguageModel = TRUE;
        }
    }
    
    self.heardTextView.text = [NSString stringWithFormat:@&quot;Heard: \&quot;%@\&quot;&quot;, hypothesis]; // Show it in the status box.
    
    // This is how to use an available instance of OEFliteController. We&#039;re going to repeat back the command that we heard with the voice we&#039;ve chosen.
    [self.fliteController say:[NSString stringWithFormat:@&quot;You said %@&quot;,hypothesis] withVoice:self.slt];
}

#ifdef kGetNbest   
- (void) pocketsphinxDidReceiveNBestHypothesisArray:(NSArray *)hypothesisArray { // Pocketsphinx has an n-best hypothesis dictionary.
    NSLog(@&quot;Local callback:  hypothesisArray is %@&quot;,hypothesisArray);   
}
#endif
// An optional delegate method of OEEventsObserver which informs that there was an interruption to the audio session (e.g. an incoming phone call).
- (void) audioSessionInterruptionDidBegin {
    NSLog(@&quot;Local callback:  AudioSession interruption began.&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: AudioSession interruption began.&quot;; // Show it in the status box.
    NSError *error = nil;
    if([OEPocketsphinxController sharedInstance].isListening) {
        error = [[OEPocketsphinxController sharedInstance] stopListening]; // React to it by telling Pocketsphinx to stop listening (if it is listening) since it will need to restart its loop after an interruption.
        if(error) NSLog(@&quot;Error while stopping listening in audioSessionInterruptionDidBegin: %@&quot;, error);
    }
}

// An optional delegate method of OEEventsObserver which informs that the interruption to the audio session ended.
- (void) audioSessionInterruptionDidEnd {
    NSLog(@&quot;Local callback:  AudioSession interruption ended.&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: AudioSession interruption ended.&quot;; // Show it in the status box.
    // We&#039;re restarting the previously-stopped listening loop.
    if(![OEPocketsphinxController sharedInstance].isListening){
        [[OEPocketsphinxController sharedInstance] startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&quot;AcousticModelEnglish&quot;] languageModelIsJSGF:FALSE]; // Start speech recognition if we aren&#039;t currently listening.    
    }
}

// An optional delegate method of OEEventsObserver which informs that the audio input became unavailable.
- (void) audioInputDidBecomeUnavailable {
    NSLog(@&quot;Local callback:  The audio input has become unavailable&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: The audio input has become unavailable&quot;; // Show it in the status box.
    NSError *error = nil;
    if([OEPocketsphinxController sharedInstance].isListening){
        error = [[OEPocketsphinxController sharedInstance] stopListening]; // React to it by telling Pocketsphinx to stop listening since there is no available input (but only if we are listening).
        if(error) NSLog(@&quot;Error while stopping listening in audioInputDidBecomeUnavailable: %@&quot;, error);
    }
}

// An optional delegate method of OEEventsObserver which informs that the unavailable audio input became available again.
- (void) audioInputDidBecomeAvailable {
    NSLog(@&quot;Local callback: The audio input is available&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: The audio input is available&quot;; // Show it in the status box.
    if(![OEPocketsphinxController sharedInstance].isListening) {
        [[OEPocketsphinxController sharedInstance] startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&quot;AcousticModelEnglish&quot;] languageModelIsJSGF:FALSE]; // Start speech recognition, but only if we aren&#039;t already listening.
    }
}
// An optional delegate method of OEEventsObserver which informs that there was a change to the audio route (e.g. headphones were plugged in or unplugged).
- (void) audioRouteDidChangeToRoute:(NSString *)newRoute {
    NSLog(@&quot;Local callback: Audio route change. The new audio route is %@&quot;, newRoute); // Log it.
    self.statusTextView.text = [NSString stringWithFormat:@&quot;Status: Audio route change. The new audio route is %@&quot;,newRoute]; // Show it in the status box.
    
    NSError *error = [[OEPocketsphinxController sharedInstance] stopListening]; // React to it by telling the Pocketsphinx loop to shut down and then start listening again on the new route
    
    if(error)NSLog(@&quot;Local callback: error while stopping listening in audioRouteDidChangeToRoute: %@&quot;,error);

    if(![OEPocketsphinxController sharedInstance].isListening) {
        [[OEPocketsphinxController sharedInstance] startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&quot;AcousticModelEnglish&quot;] languageModelIsJSGF:FALSE]; // Start speech recognition if we aren&#039;t already listening.
    }
}

// An optional delegate method of OEEventsObserver which informs that the Pocketsphinx recognition loop has entered its actual loop.
// This might be useful in debugging a conflict between another sound class and Pocketsphinx.
- (void) pocketsphinxRecognitionLoopDidStart {
    
    NSLog(@&quot;Local callback: Pocketsphinx started.&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: Pocketsphinx started.&quot;; // Show it in the status box.
}

// An optional delegate method of OEEventsObserver which informs that Pocketsphinx is now listening for speech.
- (void) pocketsphinxDidStartListening {
    
    NSLog(@&quot;Local callback: Pocketsphinx is now listening.&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: Pocketsphinx is now listening.&quot;; // Show it in the status box.
    
    self.startButton.hidden = TRUE; // React to it with some UI changes.
    self.stopButton.hidden = FALSE;
    self.suspendListeningButton.hidden = FALSE;
    self.resumeListeningButton.hidden = TRUE;
}

// An optional delegate method of OEEventsObserver which informs that Pocketsphinx detected speech and is starting to process it.
- (void) pocketsphinxDidDetectSpeech {
    NSLog(@&quot;Local callback: Pocketsphinx has detected speech.&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: Pocketsphinx has detected speech.&quot;; // Show it in the status box.
}

// An optional delegate method of OEEventsObserver which informs that Pocketsphinx detected a second of silence, indicating the end of an utterance. 
// This was added because developers requested being able to time the recognition speed without the speech time. The processing time is the time between 
// this method being called and the hypothesis being returned.
- (void) pocketsphinxDidDetectFinishedSpeech {
    NSLog(@&quot;Local callback: Pocketsphinx has detected a second of silence, concluding an utterance.&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: Pocketsphinx has detected finished speech.&quot;; // Show it in the status box.
}

// An optional delegate method of OEEventsObserver which informs that Pocketsphinx has exited its recognition loop, most 
// likely in response to the OEPocketsphinxController being told to stop listening via the stopListening method.
- (void) pocketsphinxDidStopListening {
    NSLog(@&quot;Local callback: Pocketsphinx has stopped listening.&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: Pocketsphinx has stopped listening.&quot;; // Show it in the status box.
}

// An optional delegate method of OEEventsObserver which informs that Pocketsphinx is still in its listening loop but it is not
// Going to react to speech until listening is resumed.  This can happen as a result of Flite speech being
// in progress on an audio route that doesn&#039;t support simultaneous Flite speech and Pocketsphinx recognition,
// or as a result of the OEPocketsphinxController being told to suspend recognition via the suspendRecognition method.
- (void) pocketsphinxDidSuspendRecognition {
    NSLog(@&quot;Local callback: Pocketsphinx has suspended recognition.&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: Pocketsphinx has suspended recognition.&quot;; // Show it in the status box.
}

// An optional delegate method of OEEventsObserver which informs that Pocketsphinx is still in its listening loop and after recognition
// having been suspended it is now resuming.  This can happen as a result of Flite speech completing
// on an audio route that doesn&#039;t support simultaneous Flite speech and Pocketsphinx recognition,
// or as a result of the OEPocketsphinxController being told to resume recognition via the resumeRecognition method.
- (void) pocketsphinxDidResumeRecognition {
    NSLog(@&quot;Local callback: Pocketsphinx has resumed recognition.&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: Pocketsphinx has resumed recognition.&quot;; // Show it in the status box.
}

// An optional delegate method which informs that Pocketsphinx switched over to a new language model at the given URL in the course of
// recognition. This does not imply that it is a valid file or that recognition will be successful using the file.
- (void) pocketsphinxDidChangeLanguageModelToFile:(NSString *)newLanguageModelPathAsString andDictionary:(NSString *)newDictionaryPathAsString {
    NSLog(@&quot;Local callback: Pocketsphinx is now using the following language model: \n%@ and the following dictionary: %@&quot;,newLanguageModelPathAsString,newDictionaryPathAsString);
}

// An optional delegate method of OEEventsObserver which informs that Flite is speaking, most likely to be useful if debugging a
// complex interaction between sound classes. You don&#039;t have to do anything yourself in order to prevent Pocketsphinx from listening to Flite talk and trying to recognize the speech.
- (void) fliteDidStartSpeaking {
    NSLog(@&quot;Local callback: Flite has started speaking&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: Flite has started speaking.&quot;; // Show it in the status box.
}

// An optional delegate method of OEEventsObserver which informs that Flite is finished speaking, most likely to be useful if debugging a
// complex interaction between sound classes.
- (void) fliteDidFinishSpeaking {
    NSLog(@&quot;Local callback: Flite has finished speaking&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: Flite has finished speaking.&quot;; // Show it in the status box.
}

- (void) pocketSphinxContinuousSetupDidFailWithReason:(NSString *)reasonForFailure { // This can let you know that something went wrong with the recognition loop startup. Turn on [OELogging startOpenEarsLogging] to learn why.
    NSLog(@&quot;Local callback: Setting up the continuous recognition loop has failed for the reason %@, please turn on [OELogging startOpenEarsLogging] to learn more.&quot;, reasonForFailure); // Log it.
    self.statusTextView.text = @&quot;Status: Not possible to start recognition loop.&quot;; // Show it in the status box.	
}

- (void) pocketSphinxContinuousTeardownDidFailWithReason:(NSString *)reasonForFailure { // This can let you know that something went wrong with the recognition loop startup. Turn on [OELogging startOpenEarsLogging] to learn why.
    NSLog(@&quot;Local callback: Tearing down the continuous recognition loop has failed for the reason %@, please turn on [OELogging startOpenEarsLogging] to learn more.&quot;, reasonForFailure); // Log it.
    self.statusTextView.text = @&quot;Status: Not possible to cleanly end recognition loop.&quot;; // Show it in the status box.	
}

- (void) testRecognitionCompleted { // A test file which was submitted for direct recognition via the audio driver is done.
    NSLog(@&quot;Local callback: A test file which was submitted for direct recognition via the audio driver is done.&quot;); // Log it.
    NSError *error = nil;
    if([OEPocketsphinxController sharedInstance].isListening) { // If we&#039;re listening, stop listening.
        error = [[OEPocketsphinxController sharedInstance] stopListening];
        if(error) NSLog(@&quot;Error while stopping listening in testRecognitionCompleted: %@&quot;, error);
    }
    
}
/** Pocketsphinx couldn&#039;t start because it has no mic permissions (will only be returned on iOS7 or later).*/
- (void) pocketsphinxFailedNoMicPermissions {
    NSLog(@&quot;Local callback: The user has never set mic permissions or denied permission to this app&#039;s mic, so listening will not start.&quot;);
    self.startupFailedDueToLackOfPermissions = TRUE;
}

/** The user prompt to get mic permissions, or a check of the mic permissions, has completed with a TRUE or a FALSE result  (will only be returned on iOS7 or later).*/
- (void) micPermissionCheckCompleted:(BOOL)result {
    if(result) {
        self.restartAttemptsDueToPermissionRequests++;
        if(self.restartAttemptsDueToPermissionRequests == 1 &amp;&amp; self.startupFailedDueToLackOfPermissions) { // If we get here because there was an attempt to start which failed due to lack of permissions, and now permissions have been requested and they returned true, we restart exactly once with the new permissions.
            NSError *error = nil;
            if([OEPocketsphinxController sharedInstance].isListening){
                error = [[OEPocketsphinxController sharedInstance] stopListening]; // Stop listening if we are listening.
                if(error) NSLog(@&quot;Error while stopping listening in micPermissionCheckCompleted: %@&quot;, error);
            }
            if(!error &amp;&amp; ![OEPocketsphinxController sharedInstance].isListening) { // If there was no error and we aren&#039;t listening, start listening.
                [[OEPocketsphinxController sharedInstance] startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&quot;AcousticModelEnglish&quot;] languageModelIsJSGF:FALSE]; // Start speech recognition.
                    self.startupFailedDueToLackOfPermissions = FALSE;
            }
        }
    }
}

#pragma mark -
#pragma mark UI

// This is not OpenEars-specific stuff, just some UI behavior

- (IBAction) suspendListeningButtonAction { // This is the action for the button which suspends listening without ending the recognition loop
    [[OEPocketsphinxController sharedInstance] suspendRecognition];	
    
    self.startButton.hidden = TRUE;
    self.stopButton.hidden = FALSE;
    self.suspendListeningButton.hidden = TRUE;
    self.resumeListeningButton.hidden = FALSE;
}

- (IBAction) resumeListeningButtonAction { // This is the action for the button which resumes listening if it has been suspended
    [[OEPocketsphinxController sharedInstance] resumeRecognition];
    
    self.startButton.hidden = TRUE;
    self.stopButton.hidden = FALSE;
    self.suspendListeningButton.hidden = FALSE;
    self.resumeListeningButton.hidden = TRUE;	
}

- (IBAction) stopButtonAction { // This is the action for the button which shuts down the recognition loop.
    NSError *error = nil;
    if([OEPocketsphinxController sharedInstance].isListening) { // Stop if we are currently listening.
        error = [[OEPocketsphinxController sharedInstance] stopListening];
        if(error)NSLog(@&quot;Error stopping listening in stopButtonAction: %@&quot;, error);
    }
    self.startButton.hidden = FALSE;
    self.stopButton.hidden = TRUE;
    self.suspendListeningButton.hidden = TRUE;
    self.resumeListeningButton.hidden = TRUE;
}

- (IBAction) startButtonAction { // This is the action for the button which starts up the recognition loop again if it has been shut down.
    if(![OEPocketsphinxController sharedInstance].isListening) {
        [[OEPocketsphinxController sharedInstance] startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&quot;AcousticModelEnglish&quot;] languageModelIsJSGF:FALSE]; // Start speech recognition if we aren&#039;t already listening.
    }
    self.startButton.hidden = TRUE;
    self.stopButton.hidden = FALSE;
    self.suspendListeningButton.hidden = FALSE;
    self.resumeListeningButton.hidden = TRUE;
}

#pragma mark -
#pragma mark Example for reading out Pocketsphinx and Flite audio levels without locking the UI by using an NSTimer

// What follows are not OpenEars methods, just an approach for level reading
// that I&#039;ve included with this sample app. My example implementation does make use of two OpenEars
// methods:	the pocketsphinxInputLevel method of OEPocketsphinxController and the fliteOutputLevel
// method of OEFliteController. 
//
// The example is meant to show one way that you can read those levels continuously without locking the UI, 
// by using an NSTimer, but the OpenEars level-reading methods 
// themselves do not include multithreading code since I believe that you will want to design your own 
// code approaches for level display that are tightly-integrated with your interaction design and the  
// graphics API you choose. 
// 
// Please note that if you use my sample approach, you should pay attention to the way that the timer is always stopped in
// dealloc. This should prevent you from having any difficulties with deallocating a class due to a running NSTimer process.

- (void) startDisplayingLevels { // Start displaying the levels using a timer
    [self stopDisplayingLevels]; // We never want more than one timer valid so we&#039;ll stop any running timers first.
    self.uiUpdateTimer = [NSTimer scheduledTimerWithTimeInterval:1.0/kLevelUpdatesPerSecond target:self selector:@selector(updateLevelsUI) userInfo:nil repeats:YES];
}

- (void) stopDisplayingLevels { // Stop displaying the levels by stopping the timer if it&#039;s running.
    if(self.uiUpdateTimer &amp;&amp; [self.uiUpdateTimer isValid]) { // If there is a running timer, we&#039;ll stop it here.
        [self.uiUpdateTimer invalidate];
        self.uiUpdateTimer = nil;
    }
}

- (void) updateLevelsUI { // And here is how we obtain the levels.  This method includes the actual OpenEars methods and uses their results to update the UI of this view controller.
    
    self.pocketsphinxDbLabel.text = [NSString stringWithFormat:@&quot;Pocketsphinx Input level:%f&quot;,[[OEPocketsphinxController sharedInstance] pocketsphinxInputLevel]];  //pocketsphinxInputLevel is an OpenEars method of the class OEPocketsphinxController.
    
    if(self.fliteController.speechInProgress) {
        self.fliteDbLabel.text = [NSString stringWithFormat:@&quot;Flite Output level: %f&quot;,[self.fliteController fliteOutputLevel]]; // fliteOutputLevel is an OpenEars method of the class OEFliteController.
    }
}

@end
</code></pre>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
		
	</li>
<!-- .bbp-body -->

	<li class="bbp-footer">
		<div class="bbp-reply-author">Author</div>
		<div class="bbp-reply-content">Posts</div>
<!-- .bbp-reply-content -->
	</li>
<!-- .bbp-footer -->
</ul>
<!-- #topic-0-replies -->


			
<div class="bbp-pagination">
	<div class="bbp-pagination-count">Viewing 8 posts - 1 through 8 (of 8 total)</div>
	<div class="bbp-pagination-links"></div>
</div>


		
	</div>
</div>
<!-- #bbp-user-replies-created -->

								</div>
	</div>

	
</div>

					               	</section><!-- /.entry -->

				                
            </article><!-- /.post -->
            
              
        
		</section><!-- /#main -->
		
		
        	
<aside id="sidebar" class="col-right">

	
	   
	
	 
	
</aside><!-- /#sidebar -->

    </div>
<!-- /#content -->
		
		<footer id="footer" class="col-full">
	
			<div id="copyright" class="col-left">
							<p>Politepix &copy; 2024. All Rights Reserved.</p>
						</div>
	
			<div id="credit" class="col-right">
	        <div style="text-align:right;line-height:1.3em">OpenEars® is a registered trademark of Politepix<br>AllHours® is a registered trademark of Politepix<br>The Politepix site uses cookies in order to understand how the website is used by visitors and in order to enable some required functionality. You can learn all about which cookies we use on the <a href="/about/">About</a> page, as well as everything about our privacy policy.<br>
<a href="https://twitter.com/Politepix" rel="me">TWITTER</a> | <a href="/contact" id="impressumlink">CONTACT POLITEPIX</a><a href="/about" id="impressumlink"> | IMPRESSUM | ABOUT | LEGAL | IMPRINT</a>
</div>			</div>
	<a rel="me" href="https://mastodon.social/@Halle">M</a>
		</footer><!-- /#footer  -->

</div>
<!-- /#wrapper -->

<!-- Consent Management powered by Complianz | GDPR/CCPA Cookie Consent https://wordpress.org/plugins/complianz-gdpr -->
<div id="cmplz-cookiebanner-container">
<div class="cmplz-cookiebanner cmplz-hidden banner-1 bottom-right-view-preferences optin cmplz-bottom-right cmplz-categories-type-view-preferences" aria-modal="true" data-nosnippet="true" role="dialog" aria-live="polite" aria-labelledby="cmplz-header-1-optin" aria-describedby="cmplz-message-1-optin">
	<div class="cmplz-header">
		<div class="cmplz-logo"></div>
		<div class="cmplz-title" id="cmplz-header-1-optin">Manage Cookie Consent</div>
		<div class="cmplz-close" tabindex="0" role="button" aria-label="Close dialog">
			<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="times" class="svg-inline--fa fa-times fa-w-11" role="img" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 352 512"><path fill="currentColor" d="M242.72 256l100.07-100.07c12.28-12.28 12.28-32.19 0-44.48l-22.24-22.24c-12.28-12.28-32.19-12.28-44.48 0L176 189.28 75.93 89.21c-12.28-12.28-32.19-12.28-44.48 0L9.21 111.45c-12.28 12.28-12.28 32.19 0 44.48L109.28 256 9.21 356.07c-12.28 12.28-12.28 32.19 0 44.48l22.24 22.24c12.28 12.28 32.2 12.28 44.48 0L176 322.72l100.07 100.07c12.28 12.28 32.2 12.28 44.48 0l22.24-22.24c12.28-12.28 12.28-32.19 0-44.48L242.72 256z"></path></svg>
		</div>
	</div>

	<div class="cmplz-divider cmplz-divider-header"></div>
	<div class="cmplz-body">
		<div class="cmplz-message" id="cmplz-message-1-optin">To provide the best experiences, we use technologies like cookies to store and/or access device information. Consenting to these technologies will allow us to process data such as browsing behavior or unique IDs on this site. Not consenting or withdrawing consent, may adversely affect certain features and functions.</div>
		<!-- categories start -->
		<div class="cmplz-categories">
			<details class="cmplz-category cmplz-functional">
				<summary>
						<span class="cmplz-category-header">
							<span class="cmplz-category-title">Functional</span>
							<span class="cmplz-always-active">
								<span class="cmplz-banner-checkbox">
									<input type="checkbox" id="cmplz-functional-optin" data-category="cmplz_functional" class="cmplz-consent-checkbox cmplz-functional" size="40" value="1">
									<label class="cmplz-label" for="cmplz-functional-optin" tabindex="0"><span class="screen-reader-text">Functional</span></label>
								</span>
								Always active							</span>
							<span class="cmplz-icon cmplz-open">
								<svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 448 512" height="18"><path d="M224 416c-8.188 0-16.38-3.125-22.62-9.375l-192-192c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L224 338.8l169.4-169.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-192 192C240.4 412.9 232.2 416 224 416z"></path></svg>
							</span>
						</span>
				</summary>
				<div class="cmplz-description">
					<span class="cmplz-description-functional">The technical storage or access is strictly necessary for the legitimate purpose of enabling the use of a specific service explicitly requested by the subscriber or user, or for the sole purpose of carrying out the transmission of a communication over an electronic communications network.</span>
				</div>
			</details>

			<details class="cmplz-category cmplz-preferences">
				<summary>
						<span class="cmplz-category-header">
							<span class="cmplz-category-title">Preferences</span>
							<span class="cmplz-banner-checkbox">
								<input type="checkbox" id="cmplz-preferences-optin" data-category="cmplz_preferences" class="cmplz-consent-checkbox cmplz-preferences" size="40" value="1">
								<label class="cmplz-label" for="cmplz-preferences-optin" tabindex="0"><span class="screen-reader-text">Preferences</span></label>
							</span>
							<span class="cmplz-icon cmplz-open">
								<svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 448 512" height="18"><path d="M224 416c-8.188 0-16.38-3.125-22.62-9.375l-192-192c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L224 338.8l169.4-169.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-192 192C240.4 412.9 232.2 416 224 416z"></path></svg>
							</span>
						</span>
				</summary>
				<div class="cmplz-description">
					<span class="cmplz-description-preferences">The technical storage or access is necessary for the legitimate purpose of storing preferences that are not requested by the subscriber or user.</span>
				</div>
			</details>

			<details class="cmplz-category cmplz-statistics">
				<summary>
						<span class="cmplz-category-header">
							<span class="cmplz-category-title">Statistics</span>
							<span class="cmplz-banner-checkbox">
								<input type="checkbox" id="cmplz-statistics-optin" data-category="cmplz_statistics" class="cmplz-consent-checkbox cmplz-statistics" size="40" value="1">
								<label class="cmplz-label" for="cmplz-statistics-optin" tabindex="0"><span class="screen-reader-text">Statistics</span></label>
							</span>
							<span class="cmplz-icon cmplz-open">
								<svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 448 512" height="18"><path d="M224 416c-8.188 0-16.38-3.125-22.62-9.375l-192-192c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L224 338.8l169.4-169.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-192 192C240.4 412.9 232.2 416 224 416z"></path></svg>
							</span>
						</span>
				</summary>
				<div class="cmplz-description">
					<span class="cmplz-description-statistics">The technical storage or access that is used exclusively for statistical purposes.</span>
					<span class="cmplz-description-statistics-anonymous">The technical storage or access that is used exclusively for anonymous statistical purposes. Without a subpoena, voluntary compliance on the part of your Internet Service Provider, or additional records from a third party, information stored or retrieved for this purpose alone cannot usually be used to identify you.</span>
				</div>
			</details>
			<details class="cmplz-category cmplz-marketing">
				<summary>
						<span class="cmplz-category-header">
							<span class="cmplz-category-title">Marketing</span>
							<span class="cmplz-banner-checkbox">
								<input type="checkbox" id="cmplz-marketing-optin" data-category="cmplz_marketing" class="cmplz-consent-checkbox cmplz-marketing" size="40" value="1">
								<label class="cmplz-label" for="cmplz-marketing-optin" tabindex="0"><span class="screen-reader-text">Marketing</span></label>
							</span>
							<span class="cmplz-icon cmplz-open">
								<svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 448 512" height="18"><path d="M224 416c-8.188 0-16.38-3.125-22.62-9.375l-192-192c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L224 338.8l169.4-169.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-192 192C240.4 412.9 232.2 416 224 416z"></path></svg>
							</span>
						</span>
				</summary>
				<div class="cmplz-description">
					<span class="cmplz-description-marketing">The technical storage or access is required to create user profiles to send advertising, or to track the user on a website or across several websites for similar marketing purposes.</span>
				</div>
			</details>
		</div>
<!-- categories end -->
			</div>

	<div class="cmplz-links cmplz-information">
		<a class="cmplz-link cmplz-manage-options cookie-statement" href="#" data-relative_url="#cmplz-manage-consent-container">Manage options</a>
		<a class="cmplz-link cmplz-manage-third-parties cookie-statement" href="#" data-relative_url="#cmplz-cookies-overview">Manage services</a>
		<a class="cmplz-link cmplz-manage-vendors tcf cookie-statement" href="#" data-relative_url="#cmplz-tcf-wrapper">Manage {vendor_count} vendors</a>
		<a class="cmplz-link cmplz-external cmplz-read-more-purposes tcf" target="_blank" rel="noopener noreferrer nofollow" href="https://cookiedatabase.org/tcf/purposes/">Read more about these purposes</a>
			</div>

	<div class="cmplz-divider cmplz-footer"></div>

	<div class="cmplz-buttons">
		<button class="cmplz-btn cmplz-accept">Accept</button>
		<button class="cmplz-btn cmplz-deny">Deny</button>
		<button class="cmplz-btn cmplz-view-preferences">View preferences</button>
		<button class="cmplz-btn cmplz-save-preferences">Save preferences</button>
		<a class="cmplz-btn cmplz-manage-options tcf cookie-statement" href="#" data-relative_url="#cmplz-manage-consent-container">View preferences</a>
			</div>

	<div class="cmplz-links cmplz-documents">
		<a class="cmplz-link cookie-statement" href="#" data-relative_url="">{title}</a>
		<a class="cmplz-link privacy-statement" href="#" data-relative_url="">{title}</a>
		<a class="cmplz-link impressum" href="#" data-relative_url="">{title}</a>
			</div>

</div>
</div>
					<div id="cmplz-manage-consent" data-nosnippet="true">
<button class="cmplz-btn cmplz-hidden cmplz-manage-consent manage-consent-1">Manage consent</button>

</div>
<!--[if lt IE 9]>
<script src="https://www.politepix.com/wp-content/themes/pixelpress/includes/js/respond-IE.js"></script>
<![endif]-->
			<script>jQuery(document).ready(function(){
					jQuery('.images a').attr('rel', 'prettyPhoto[product-gallery]');
				});</script>
			<script type="text/javascript">(function () {
			var c = document.body.className;
			c = c.replace(/woocommerce-no-js/, 'woocommerce-js');
			document.body.className = c;
		})();</script>
	<link rel="stylesheet" id="wc-blocks-style-css" href="https://c0.wp.com/p/woocommerce/8.8.2/assets/client/blocks/wc-blocks.css" type="text/css" media="all">
<script type="text/javascript" src="https://c0.wp.com/p/woocommerce/8.8.2/assets/js/accounting/accounting.min.js" id="accounting-js"></script>
<script type="text/javascript" src="https://c0.wp.com/c/6.5.2/wp-includes/js/jquery/ui/core.min.js" id="jquery-ui-core-js"></script>
<script type="text/javascript" src="https://c0.wp.com/c/6.5.2/wp-includes/js/jquery/ui/datepicker.min.js" id="jquery-ui-datepicker-js"></script>
<script type="text/javascript" id="jquery-ui-datepicker-js-after">
/* <![CDATA[ */
jQuery(function(jQuery){jQuery.datepicker.setDefaults({"closeText":"Close","currentText":"Today","monthNames":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthNamesShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"nextText":"Next","prevText":"Previous","dayNames":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"dayNamesShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"dayNamesMin":["S","M","T","W","T","F","S"],"dateFormat":"MM d, yy","firstDay":1,"isRTL":false});});
/* ]]> */
</script>
<script type="text/javascript" id="woocommerce-addons-js-extra">
/* <![CDATA[ */
var woocommerce_addons_params = {"price_display_suffix":"","tax_enabled":"1","price_include_tax":"","display_include_tax":"","ajax_url":"\/wp-admin\/admin-ajax.php","i18n_validation_required_select":"Please choose an option.","i18n_validation_required_input":"Please enter some text in this field.","i18n_validation_required_number":"Please enter a number in this field.","i18n_validation_required_file":"Please upload a file.","i18n_validation_letters_only":"Please enter letters only.","i18n_validation_numbers_only":"Please enter numbers only.","i18n_validation_letters_and_numbers_only":"Please enter letters and numbers only.","i18n_validation_email_only":"Please enter a valid email address.","i18n_validation_min_characters":"Please enter at least %c characters.","i18n_validation_max_characters":"Please enter up to %c characters.","i18n_validation_min_number":"Please enter %c or more.","i18n_validation_max_number":"Please enter %c or less.","i18n_sub_total":"Subtotal","i18n_remaining":"<span><\/span> characters remaining","currency_format_num_decimals":"2","currency_format_symbol":"€","currency_format_decimal_sep":".","currency_format_thousand_sep":",","trim_trailing_zeros":"","is_bookings":"","trim_user_input_characters":"1000","quantity_symbol":"x ","datepicker_class":"wc_pao_datepicker","datepicker_date_format":"MM d, yy","gmt_offset":"-2","date_input_timezone_reference":"default","currency_format":"%s%v"};
/* ]]> */
</script>
<script type="text/javascript" src="/wp-content/plugins/woocommerce-product-addons/assets/js/frontend/addons.min.js?ver=6.8.2" id="woocommerce-addons-js" defer data-wp-strategy="defer"></script>
<script type="text/javascript" src="/wp-content/plugins/jetpack/jetpack_vendor/automattic/jetpack-image-cdn/dist/image-cdn.js?minify=false&amp;ver=132249e245926ae3e188" id="jetpack-photon-js"></script>
<script type="text/javascript" src="/wp-content/plugins/bbpress/templates/default/js/editor.min.js?ver=2.6.9" id="bbpress-editor-js"></script>
<script type="text/javascript" src="https://c0.wp.com/p/woocommerce/8.8.2/assets/js/sourcebuster/sourcebuster.min.js" id="sourcebuster-js-js"></script>
<script type="text/javascript" id="wc-order-attribution-js-extra">
/* <![CDATA[ */
var wc_order_attribution = {"params":{"lifetime":1.0e-5,"session":30,"ajaxurl":"\/wp-admin\/admin-ajax.php","prefix":"wc_order_attribution_","allowTracking":true},"fields":{"source_type":"current.typ","referrer":"current_add.rf","utm_campaign":"current.cmp","utm_source":"current.src","utm_medium":"current.mdm","utm_content":"current.cnt","utm_id":"current.id","utm_term":"current.trm","session_entry":"current_add.ep","session_start_time":"current_add.fd","session_pages":"session.pgs","session_count":"udata.vst","user_agent":"udata.uag"}};
/* ]]> */
</script>
<script type="text/javascript" src="https://c0.wp.com/p/woocommerce/8.8.2/assets/js/frontend/order-attribution.min.js" id="wc-order-attribution-js"></script>
<script data-service="jetpack-statistics" data-category="statistics" type="text/plain" data-cmplz-src="https://stats.wp.com/e-202417.js" id="jetpack-stats-js" data-wp-strategy="defer"></script>
<script type="text/javascript" id="jetpack-stats-js-after">
/* <![CDATA[ */
_stq = window._stq || [];
_stq.push([ "view", JSON.parse("{\"v\":\"ext\",\"blog\":\"206848719\",\"post\":\"0\",\"tz\":\"2\",\"srv\":\"www.politepix.com\",\"j\":\"1:13.3.1\"}") ]);
_stq.push([ "clickTrackerInit", "206848719", "0" ]);
/* ]]> */
</script>
<script type="text/javascript" id="cmplz-cookiebanner-js-extra">
/* <![CDATA[ */
var complianz = {"prefix":"cmplz_","user_banner_id":"1","set_cookies":[],"block_ajax_content":"","banner_version":"18","version":"7.0.4","store_consent":"","do_not_track_enabled":"1","consenttype":"optin","region":"eu","geoip":"","dismiss_timeout":"","disable_cookiebanner":"","soft_cookiewall":"","dismiss_on_scroll":"","cookie_expiry":"365","url":"\/wp-json\/complianz\/v1\/","locale":"lang=en&locale=en_US","set_cookies_on_root":"","cookie_domain":"","current_policy_id":"16","cookie_path":"\/","categories":{"statistics":"statistics","marketing":"marketing"},"tcf_active":"","placeholdertext":"Click to accept {category} cookies and enable this content","css_file":"\/wp-content\/uploads\/complianz\/css\/banner-{banner_id}-{type}.css?v=18","page_links":{"eu":{"cookie-statement":{"title":"About &#038; Impressum &#038; Privacy Policy","url":"\/about\/"},"privacy-statement":{"title":"About &#038; Impressum &#038; Privacy Policy","url":"\/about\/"},"impressum":{"title":"About &#038; Impressum &#038; Privacy Policy","url":"\/about\/"}},"us":{"impressum":{"title":"About &#038; Impressum &#038; Privacy Policy","url":"\/about\/"}},"uk":{"impressum":{"title":"About &#038; Impressum &#038; Privacy Policy","url":"\/about\/"}},"ca":{"impressum":{"title":"About &#038; Impressum &#038; Privacy Policy","url":"\/about\/"}},"au":{"impressum":{"title":"About &#038; Impressum &#038; Privacy Policy","url":"\/about\/"}},"za":{"impressum":{"title":"About &#038; Impressum &#038; Privacy Policy","url":"\/about\/"}},"br":{"impressum":{"title":"About &#038; Impressum &#038; Privacy Policy","url":"\/about\/"}}},"tm_categories":"","forceEnableStats":"","preview":"","clean_cookies":"","aria_label":"Click to accept {category} cookies and enable this content"};
/* ]]> */
</script>
<script defer type="text/javascript" src="/wp-content/plugins/complianz-gdpr/cookiebanner/js/complianz.min.js?ver=1710283454" id="cmplz-cookiebanner-js"></script>
</body>
</html>