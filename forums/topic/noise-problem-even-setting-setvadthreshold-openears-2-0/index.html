<!DOCTYPE html>
<html lang="en-US">
<head>

<meta charset="UTF-8">

<title>Topic: [Resolved] Noise problem even setting setVadThreshold openEars 2.0 | Politepix</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link href="/wp-content/uploads/omgf/omgf-stylesheet-56/omgf-stylesheet-56.css?ver=1668781124" rel="stylesheet" type="text/css">

<link rel="stylesheet" type="text/css" href="/wp-content/themes/politepix-pixelpress-child-theme/style.css" media="screen">

<link rel="pingback" href="/xmlrpc.php">
<meta name="robots" content="max-image-preview:large">
<script>window._wca = window._wca || [];</script>
<link rel="dns-prefetch" href="//stats.wp.com">
<link rel="dns-prefetch" href="//i0.wp.com">
<link rel="dns-prefetch" href="//c0.wp.com">
<link rel="alternate" type="application/rss+xml" title="Politepix &raquo; Feed" href="http://feeds.feedburner.com/politepixblog">
<link rel="alternate" type="application/rss+xml" title="Politepix &raquo; Comments Feed" href="/comments/feed/">
<link rel="alternate" type="application/rss+xml" title="Politepix &raquo; [Resolved] Noise problem even setting setVadThreshold openEars 2.0 Comments Feed" href="/forums/topic/noise-problem-even-setting-setvadthreshold-openears-2-0/feed/">
<script type="text/javascript">
/* <![CDATA[ */
window._wpemojiSettings = {"baseUrl":"https:\/\/s.w.org\/images\/core\/emoji\/15.0.3\/72x72\/","ext":".png","svgUrl":"https:\/\/s.w.org\/images\/core\/emoji\/15.0.3\/svg\/","svgExt":".svg","source":{"concatemoji":"\/wp-includes\/js\/wp-emoji-release.min.js?ver=6.5.2"}};
/*! This file is auto-generated */
!function(i,n){var o,s,e;function c(e){try{var t={supportTests:e,timestamp:(new Date).valueOf()};sessionStorage.setItem(o,JSON.stringify(t))}catch(e){}}function p(e,t,n){e.clearRect(0,0,e.canvas.width,e.canvas.height),e.fillText(t,0,0);var t=new Uint32Array(e.getImageData(0,0,e.canvas.width,e.canvas.height).data),r=(e.clearRect(0,0,e.canvas.width,e.canvas.height),e.fillText(n,0,0),new Uint32Array(e.getImageData(0,0,e.canvas.width,e.canvas.height).data));return t.every(function(e,t){return e===r[t]})}function u(e,t,n){switch(t){case"flag":return n(e,"🏳️‍⚧️","🏳️​⚧️")?!1:!n(e,"🇺🇳","🇺​🇳")&&!n(e,"🏴󠁧󠁢󠁥󠁮󠁧󠁿","🏴​󠁧​󠁢​󠁥​󠁮​󠁧​󠁿");case"emoji":return!n(e,"🐦‍⬛","🐦​⬛")}return!1}function f(e,t,n){var r="undefined"!=typeof WorkerGlobalScope&&self instanceof WorkerGlobalScope?new OffscreenCanvas(300,150):i.createElement("canvas"),a=r.getContext("2d",{willReadFrequently:!0}),o=(a.textBaseline="top",a.font="600 32px Arial",{});return e.forEach(function(e){o[e]=t(a,e,n)}),o}function t(e){var t=i.createElement("script");t.src=e,t.defer=!0,i.head.appendChild(t)}"undefined"!=typeof Promise&&(o="wpEmojiSettingsSupports",s=["flag","emoji"],n.supports={everything:!0,everythingExceptFlag:!0},e=new Promise(function(e){i.addEventListener("DOMContentLoaded",e,{once:!0})}),new Promise(function(t){var n=function(){try{var e=JSON.parse(sessionStorage.getItem(o));if("object"==typeof e&&"number"==typeof e.timestamp&&(new Date).valueOf()<e.timestamp+604800&&"object"==typeof e.supportTests)return e.supportTests}catch(e){}return null}();if(!n){if("undefined"!=typeof Worker&&"undefined"!=typeof OffscreenCanvas&&"undefined"!=typeof URL&&URL.createObjectURL&&"undefined"!=typeof Blob)try{var e="postMessage("+f.toString()+"("+[JSON.stringify(s),u.toString(),p.toString()].join(",")+"));",r=new Blob([e],{type:"text/javascript"}),a=new Worker(URL.createObjectURL(r),{name:"wpTestEmojiSupports"});return void(a.onmessage=function(e){c(n=e.data),a.terminate(),t(n)})}catch(e){}c(n=f(s,u,p))}t(n)}).then(function(e){for(var t in e)n.supports[t]=e[t],n.supports.everything=n.supports.everything&&n.supports[t],"flag"!==t&&(n.supports.everythingExceptFlag=n.supports.everythingExceptFlag&&n.supports[t]);n.supports.everythingExceptFlag=n.supports.everythingExceptFlag&&!n.supports.flag,n.DOMReady=!1,n.readyCallback=function(){n.DOMReady=!0}}).then(function(){return e}).then(function(){var e;n.supports.everything||(n.readyCallback(),(e=n.source||{}).concatemoji?t(e.concatemoji):e.wpemoji&&e.twemoji&&(t(e.twemoji),t(e.wpemoji)))}))}((window,document),window._wpemojiSettings);
/* ]]> */
</script>
<link rel="stylesheet" id="woo-layout-css" href="/wp-content/themes/pixelpress/css/layout.css?ver=6.5.2" type="text/css" media="all">
<link rel="stylesheet" id="woocommerce-css" href="/wp-content/themes/pixelpress/css/woocommerce.css?ver=6.5.2" type="text/css" media="all">
<style id="wp-emoji-styles-inline-css" type="text/css">img.wp-smiley, img.emoji {
		display: inline !important;
		border: none !important;
		box-shadow: none !important;
		height: 1em !important;
		width: 1em !important;
		margin: 0 0.07em !important;
		vertical-align: -0.1em !important;
		background: none !important;
		padding: 0 !important;
	}</style>
<link rel="stylesheet" id="wp-block-library-css" href="https://c0.wp.com/c/6.5.2/wp-includes/css/dist/block-library/style.min.css" type="text/css" media="all">
<style id="wp-block-library-inline-css" type="text/css">.has-text-align-justify{text-align:justify;}</style>
<link rel="stylesheet" id="mediaelement-css" href="https://c0.wp.com/c/6.5.2/wp-includes/js/mediaelement/mediaelementplayer-legacy.min.css" type="text/css" media="all">
<link rel="stylesheet" id="wp-mediaelement-css" href="https://c0.wp.com/c/6.5.2/wp-includes/js/mediaelement/wp-mediaelement.min.css" type="text/css" media="all">
<style id="jetpack-sharing-buttons-style-inline-css" type="text/css">.jetpack-sharing-buttons__services-list{display:flex;flex-direction:row;flex-wrap:wrap;gap:0;list-style-type:none;margin:5px;padding:0}.jetpack-sharing-buttons__services-list.has-small-icon-size{font-size:12px}.jetpack-sharing-buttons__services-list.has-normal-icon-size{font-size:16px}.jetpack-sharing-buttons__services-list.has-large-icon-size{font-size:24px}.jetpack-sharing-buttons__services-list.has-huge-icon-size{font-size:36px}@media print{.jetpack-sharing-buttons__services-list{display:none!important}}.editor-styles-wrapper .wp-block-jetpack-sharing-buttons{gap:0;padding-inline-start:0}ul.jetpack-sharing-buttons__services-list.has-background{padding:1.25em 2.375em}</style>
<style id="classic-theme-styles-inline-css" type="text/css">/*! This file is auto-generated */
.wp-block-button__link{color:#fff;background-color:#32373c;border-radius:9999px;box-shadow:none;text-decoration:none;padding:calc(.667em + 2px) calc(1.333em + 2px);font-size:1.125em}.wp-block-file__button{background:#32373c;color:#fff;text-decoration:none}</style>
<style id="global-styles-inline-css" type="text/css">body{--wp--preset--color--black: #000000;--wp--preset--color--cyan-bluish-gray: #abb8c3;--wp--preset--color--white: #ffffff;--wp--preset--color--pale-pink: #f78da7;--wp--preset--color--vivid-red: #cf2e2e;--wp--preset--color--luminous-vivid-orange: #ff6900;--wp--preset--color--luminous-vivid-amber: #fcb900;--wp--preset--color--light-green-cyan: #7bdcb5;--wp--preset--color--vivid-green-cyan: #00d084;--wp--preset--color--pale-cyan-blue: #8ed1fc;--wp--preset--color--vivid-cyan-blue: #0693e3;--wp--preset--color--vivid-purple: #9b51e0;--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple: linear-gradient(135deg,rgba(6,147,227,1) 0%,rgb(155,81,224) 100%);--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan: linear-gradient(135deg,rgb(122,220,180) 0%,rgb(0,208,130) 100%);--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange: linear-gradient(135deg,rgba(252,185,0,1) 0%,rgba(255,105,0,1) 100%);--wp--preset--gradient--luminous-vivid-orange-to-vivid-red: linear-gradient(135deg,rgba(255,105,0,1) 0%,rgb(207,46,46) 100%);--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray: linear-gradient(135deg,rgb(238,238,238) 0%,rgb(169,184,195) 100%);--wp--preset--gradient--cool-to-warm-spectrum: linear-gradient(135deg,rgb(74,234,220) 0%,rgb(151,120,209) 20%,rgb(207,42,186) 40%,rgb(238,44,130) 60%,rgb(251,105,98) 80%,rgb(254,248,76) 100%);--wp--preset--gradient--blush-light-purple: linear-gradient(135deg,rgb(255,206,236) 0%,rgb(152,150,240) 100%);--wp--preset--gradient--blush-bordeaux: linear-gradient(135deg,rgb(254,205,165) 0%,rgb(254,45,45) 50%,rgb(107,0,62) 100%);--wp--preset--gradient--luminous-dusk: linear-gradient(135deg,rgb(255,203,112) 0%,rgb(199,81,192) 50%,rgb(65,88,208) 100%);--wp--preset--gradient--pale-ocean: linear-gradient(135deg,rgb(255,245,203) 0%,rgb(182,227,212) 50%,rgb(51,167,181) 100%);--wp--preset--gradient--electric-grass: linear-gradient(135deg,rgb(202,248,128) 0%,rgb(113,206,126) 100%);--wp--preset--gradient--midnight: linear-gradient(135deg,rgb(2,3,129) 0%,rgb(40,116,252) 100%);--wp--preset--font-size--small: 13px;--wp--preset--font-size--medium: 20px;--wp--preset--font-size--large: 36px;--wp--preset--font-size--x-large: 42px;--wp--preset--spacing--20: 0.44rem;--wp--preset--spacing--30: 0.67rem;--wp--preset--spacing--40: 1rem;--wp--preset--spacing--50: 1.5rem;--wp--preset--spacing--60: 2.25rem;--wp--preset--spacing--70: 3.38rem;--wp--preset--spacing--80: 5.06rem;--wp--preset--shadow--natural: 6px 6px 9px rgba(0, 0, 0, 0.2);--wp--preset--shadow--deep: 12px 12px 50px rgba(0, 0, 0, 0.4);--wp--preset--shadow--sharp: 6px 6px 0px rgba(0, 0, 0, 0.2);--wp--preset--shadow--outlined: 6px 6px 0px -3px rgba(255, 255, 255, 1), 6px 6px rgba(0, 0, 0, 1);--wp--preset--shadow--crisp: 6px 6px 0px rgba(0, 0, 0, 1);}:where(.is-layout-flex){gap: 0.5em;}:where(.is-layout-grid){gap: 0.5em;}body .is-layout-flow > .alignleft{float: left;margin-inline-start: 0;margin-inline-end: 2em;}body .is-layout-flow > .alignright{float: right;margin-inline-start: 2em;margin-inline-end: 0;}body .is-layout-flow > .aligncenter{margin-left: auto !important;margin-right: auto !important;}body .is-layout-constrained > .alignleft{float: left;margin-inline-start: 0;margin-inline-end: 2em;}body .is-layout-constrained > .alignright{float: right;margin-inline-start: 2em;margin-inline-end: 0;}body .is-layout-constrained > .aligncenter{margin-left: auto !important;margin-right: auto !important;}body .is-layout-constrained > :where(:not(.alignleft):not(.alignright):not(.alignfull)){max-width: var(--wp--style--global--content-size);margin-left: auto !important;margin-right: auto !important;}body .is-layout-constrained > .alignwide{max-width: var(--wp--style--global--wide-size);}body .is-layout-flex{display: flex;}body .is-layout-flex{flex-wrap: wrap;align-items: center;}body .is-layout-flex > *{margin: 0;}body .is-layout-grid{display: grid;}body .is-layout-grid > *{margin: 0;}:where(.wp-block-columns.is-layout-flex){gap: 2em;}:where(.wp-block-columns.is-layout-grid){gap: 2em;}:where(.wp-block-post-template.is-layout-flex){gap: 1.25em;}:where(.wp-block-post-template.is-layout-grid){gap: 1.25em;}.has-black-color{color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-color{color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-color{color: var(--wp--preset--color--white) !important;}.has-pale-pink-color{color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-color{color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-color{color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-color{color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-color{color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-color{color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-color{color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-color{color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-color{color: var(--wp--preset--color--vivid-purple) !important;}.has-black-background-color{background-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-background-color{background-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-background-color{background-color: var(--wp--preset--color--white) !important;}.has-pale-pink-background-color{background-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-background-color{background-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-background-color{background-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-background-color{background-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-background-color{background-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-background-color{background-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-background-color{background-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-background-color{background-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-background-color{background-color: var(--wp--preset--color--vivid-purple) !important;}.has-black-border-color{border-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-border-color{border-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-border-color{border-color: var(--wp--preset--color--white) !important;}.has-pale-pink-border-color{border-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-border-color{border-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-border-color{border-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-border-color{border-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-border-color{border-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-border-color{border-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-border-color{border-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-border-color{border-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-border-color{border-color: var(--wp--preset--color--vivid-purple) !important;}.has-vivid-cyan-blue-to-vivid-purple-gradient-background{background: var(--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple) !important;}.has-light-green-cyan-to-vivid-green-cyan-gradient-background{background: var(--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan) !important;}.has-luminous-vivid-amber-to-luminous-vivid-orange-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange) !important;}.has-luminous-vivid-orange-to-vivid-red-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-orange-to-vivid-red) !important;}.has-very-light-gray-to-cyan-bluish-gray-gradient-background{background: var(--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray) !important;}.has-cool-to-warm-spectrum-gradient-background{background: var(--wp--preset--gradient--cool-to-warm-spectrum) !important;}.has-blush-light-purple-gradient-background{background: var(--wp--preset--gradient--blush-light-purple) !important;}.has-blush-bordeaux-gradient-background{background: var(--wp--preset--gradient--blush-bordeaux) !important;}.has-luminous-dusk-gradient-background{background: var(--wp--preset--gradient--luminous-dusk) !important;}.has-pale-ocean-gradient-background{background: var(--wp--preset--gradient--pale-ocean) !important;}.has-electric-grass-gradient-background{background: var(--wp--preset--gradient--electric-grass) !important;}.has-midnight-gradient-background{background: var(--wp--preset--gradient--midnight) !important;}.has-small-font-size{font-size: var(--wp--preset--font-size--small) !important;}.has-medium-font-size{font-size: var(--wp--preset--font-size--medium) !important;}.has-large-font-size{font-size: var(--wp--preset--font-size--large) !important;}.has-x-large-font-size{font-size: var(--wp--preset--font-size--x-large) !important;}
.wp-block-navigation a:where(:not(.wp-element-button)){color: inherit;}
:where(.wp-block-post-template.is-layout-flex){gap: 1.25em;}:where(.wp-block-post-template.is-layout-grid){gap: 1.25em;}
:where(.wp-block-columns.is-layout-flex){gap: 2em;}:where(.wp-block-columns.is-layout-grid){gap: 2em;}
.wp-block-pullquote{font-size: 1.5em;line-height: 1.6;}</style>
<link rel="stylesheet" id="bbp-default-css" href="/wp-content/plugins/bbpress/templates/default/css/bbpress.min.css?ver=2.6.9" type="text/css" media="all">
<link rel="stylesheet" id="currency_converter_styles-css" href="/wp-content/plugins/woocommerce-currency-converter-widget/assets/css/converter.css?ver=2.2.2" type="text/css" media="all">
<style id="woocommerce-inline-inline-css" type="text/css">.woocommerce form .form-row .required { visibility: visible; }</style>
<link rel="stylesheet" id="cmplz-general-css" href="/wp-content/plugins/complianz-gdpr/assets/css/cookieblocker.min.css?ver=1710283454" type="text/css" media="all">
<link rel="stylesheet" id="jetpack_css-css" href="https://c0.wp.com/p/jetpack/13.3.1/css/jetpack.css" type="text/css" media="all">
<link rel="stylesheet" id="prettyPhoto-css" href="/wp-content/themes/pixelpress/includes/css/prettyPhoto.css?ver=6.5.2" type="text/css" media="all">
<script type="text/template" id="tmpl-variation-template">
	<div class="woocommerce-variation-description">{{{ data.variation.variation_description }}}<\/div>
	<div class="woocommerce-variation-price">{{{ data.variation.price_html }}}<\/div>
	<div class="woocommerce-variation-availability">{{{ data.variation.availability_html }}}<\/div>
</script>
<script type="text/template" id="tmpl-unavailable-variation-template">
	<p>Sorry, this product is unavailable. Please choose a different combination.<\/p>
</script>
<script type="text/javascript" src="https://c0.wp.com/c/6.5.2/wp-includes/js/dist/vendor/wp-polyfill-inert.min.js" id="wp-polyfill-inert-js"></script>
<script type="text/javascript" src="https://c0.wp.com/c/6.5.2/wp-includes/js/dist/vendor/regenerator-runtime.min.js" id="regenerator-runtime-js"></script>
<script type="text/javascript" src="https://c0.wp.com/c/6.5.2/wp-includes/js/dist/vendor/wp-polyfill.min.js" id="wp-polyfill-js"></script>
<script type="text/javascript" src="https://c0.wp.com/c/6.5.2/wp-includes/js/dist/hooks.min.js" id="wp-hooks-js"></script>
<script data-service="jetpack-statistics" data-category="statistics" type="text/plain" data-cmplz-src="https://stats.wp.com/w.js?ver=202417" id="woo-tracks-js"></script>
<script type="text/javascript" src="https://c0.wp.com/c/6.5.2/wp-includes/js/jquery/jquery.min.js" id="jquery-core-js"></script>
<script type="text/javascript" src="https://c0.wp.com/c/6.5.2/wp-includes/js/jquery/jquery-migrate.min.js" id="jquery-migrate-js"></script>
<script type="text/javascript" src="https://c0.wp.com/c/6.5.2/wp-includes/js/underscore.min.js" id="underscore-js"></script>
<script type="text/javascript" id="wp-util-js-extra">
/* <![CDATA[ */
var _wpUtilSettings = {"ajax":{"url":"\/wp-admin\/admin-ajax.php"}};
/* ]]> */
</script>
<script type="text/javascript" src="https://c0.wp.com/c/6.5.2/wp-includes/js/wp-util.min.js" id="wp-util-js"></script>
<script type="text/javascript" src="https://c0.wp.com/p/woocommerce/8.8.2/assets/js/jquery-blockui/jquery.blockUI.min.js" id="jquery-blockui-js" defer data-wp-strategy="defer"></script>
<script type="text/javascript" id="wc-add-to-cart-variation-js-extra">
/* <![CDATA[ */
var wc_add_to_cart_variation_params = {"wc_ajax_url":"\/?wc-ajax=%%endpoint%%","i18n_no_matching_variations_text":"Sorry, no products matched your selection. Please choose a different combination.","i18n_make_a_selection_text":"Please select some product options before adding this product to your cart.","i18n_unavailable_text":"Sorry, this product is unavailable. Please choose a different combination."};
/* ]]> */
</script>
<script type="text/javascript" src="https://c0.wp.com/p/woocommerce/8.8.2/assets/js/frontend/add-to-cart-variation.min.js" id="wc-add-to-cart-variation-js" defer data-wp-strategy="defer"></script>
<script type="text/javascript" src="https://c0.wp.com/p/woocommerce/8.8.2/assets/js/jquery-cookie/jquery.cookie.min.js" id="jquery-cookie-js" defer data-wp-strategy="defer"></script>
<script data-service="jetpack-statistics" data-category="statistics" type="text/plain" data-cmplz-src="https://stats.wp.com/s-202417.js" id="woocommerce-analytics-js" defer data-wp-strategy="defer"></script>
<script type="text/javascript" src="/wp-content/themes/pixelpress/includes/js/third-party.js?ver=6.5.2" id="third party-js"></script>
<script type="text/javascript" src="https://c0.wp.com/p/woocommerce/8.8.2/assets/js/prettyPhoto/jquery.prettyPhoto.min.js" id="prettyPhoto-js" data-wp-strategy="defer"></script>
<script type="text/javascript" src="/wp-content/themes/pixelpress/includes/js/general.js?ver=6.5.2" id="general-js"></script>
<script type="text/javascript" src="/wp-content/themes/pixelpress/includes/js/uniform.js?ver=6.5.2" id="uniform-js"></script>
<link rel="https://api.w.org/" href="/wp-json/">
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="/xmlrpc.php?rsd">
<meta name="generator" content="WordPress 6.5.2">
<meta name="generator" content="WooCommerce 8.8.2">
<link rel="canonical" href="/forums/topic/noise-problem-even-setting-setvadthreshold-openears-2-0/">
<link rel="shortlink" href="/?p=1024045">
<link rel="alternate" type="application/json+oembed" href="/wp-json/oembed/1.0/embed?url=https%3A%2F%2F%2Fforums%2Ftopic%2Fnoise-problem-even-setting-setvadthreshold-openears-2-0%2F">
<link rel="alternate" type="text/xml+oembed" href="/wp-json/oembed/1.0/embed?url=https%3A%2F%2F%2Fforums%2Ftopic%2Fnoise-problem-even-setting-setvadthreshold-openears-2-0%2F#038;format=xml">
	<style>img#wpstats{display:none}</style>
					<style>.cmplz-hidden {
					display: none !important;
				}</style>		<script>( function() {
				window.onpageshow = function( event ) {
					// Defined window.wpforms means that a form exists on a page.
					// If so and back/forward button has been clicked,
					// force reload a page to prevent the submit button state stuck.
					if ( typeof window.wpforms !== 'undefined' && event.persisted ) {
						window.location.reload();
					}
				};
			}() );</script>
		
<!-- Theme version -->
<meta name="generator" content="PixelPress Politepix 1.0">
<meta name="generator" content="PixelPress 1.5.4">
<meta name="generator" content="WooFramework 6.2.9">

<!-- Always force latest IE rendering engine (even in intranet) & Chrome Frame -->
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

<!--  Mobile viewport scale | Disable user zooming as the layout is optimised -->
<meta content="initial-scale=1.0; maximum-scale=1.0; user-scalable=no" name="viewport">
		<!--[if lt IE 9]>
			<script src="https://html5shim.googlecode.com/svn/trunk/html5.js"></script>
		<![endif]-->
			<noscript><style>.woocommerce-product-gallery{ opacity: 1 !important; }</style></noscript>
	
<!-- Google Webfonts -->
<link href="/wp-content/uploads/omgf/omgf-stylesheet-165/omgf-stylesheet-165.css?ver=1668781124" rel="stylesheet" type="text/css">

<!-- Alt Stylesheet -->
<link href="/wp-content/themes/pixelpress/styles/default.css" rel="stylesheet" type="text/css">

<!-- Custom Favicon -->
<link rel="shortcut icon" href="/wp-content/uploads/favicon.png">

<!-- Woo Shortcodes CSS -->
<link href="/wp-content/themes/pixelpress/functions/css/shortcodes.css" rel="stylesheet" type="text/css">

<!-- Custom Stylesheet -->
<link href="/wp-content/themes/pixelpress/custom.css" rel="stylesheet" type="text/css">

<!-- Custom Stylesheet In Child Theme -->
<link href="/wp-content/themes/politepix-pixelpress-child-theme/custom.css" rel="stylesheet" type="text/css">

</head>

<body data-cmplz="1" class="topic bbpress no-js topic-template-default single single-topic postid-1024045 theme-pixelpress woocommerce-no-js unknown alt-style-default has-lightbox layout-left-content">

<div id="wrapper">

	    
        
    <div id="header-wrap">

		<header id="header" class="col-full">
		
			<div id="logo" class="fl">
												    <a id="logo" href="/" title="iOS Frameworks for speech recognition, text to speech and more">
				    	<img src="/wp-content/uploads/logo1.png" alt="Politepix">
				    </a>
			    			    
			    <hgroup>
			        
					<h1 class="site-title"><a href="/">Politepix</a></h1>
					<h2 class="site-description">iOS Frameworks for speech recognition, text to speech and more</h2>
					<h3 class="nav-toggle"><a href="#navigation">Navigation</a></h3>
				      	
				</hgroup>
			</div>
<!-- /#logo -->		
	        
	        	
	        <div id="header-right" class="fr">

				<nav id="navigation" role="navigation">
					
					<ul id="main-nav" class="nav fl">
<li id="menu-item-1175" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-1175">
<a href="/openears/">OpenEars</a>
<ul class="sub-menu">
	<li id="menu-item-11108" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-11108"><a href="/openears/tutorial/">OpenEars Tutorials</a></li>
	<li id="menu-item-1171" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1171"><a href="/openears/support/">OpenEars FAQ/Support</a></li>
	<li id="menu-item-1189" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-1189"><a href="/forums/forum/openearsforum/">OpenEars Support Forum</a></li>
	<li id="menu-item-14947" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-14947"><a href="/openearsplatform/">About the OpenEars Platform</a></li>
</ul>
</li>
<li id="menu-item-11706" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-11706">
<a href="/neatspeech">NeatSpeech</a>
<ul class="sub-menu">
	<li id="menu-item-11707" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-11707"><a href="/forums/forum/openears-plugins/">NeatSpeech support forum</a></li>
</ul>
</li>
<li id="menu-item-9871" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-9871">
<a href="/rapidears/">RapidEars</a>
<ul class="sub-menu">
	<li id="menu-item-10389" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-10389"><a href="/forums/forum/openears-plugins">RapidEars Support Forum</a></li>
</ul>
</li>
<li id="menu-item-11030" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-11030">
<a href="/rejecto/">Rejecto</a>
<ul class="sub-menu">
	<li id="menu-item-11031" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-11031"><a href="/forums/forum/openears-plugins/">Rejecto Support Forum</a></li>
</ul>
</li>
<li id="menu-item-13071" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-13071"><a href="/savethatwave/">SaveThatWave</a></li>
<li id="menu-item-1021000" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-1021000">
<a href="/ruleorama/">RuleORama</a>
<ul class="sub-menu">
	<li id="menu-item-9002" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-9002"><a href="/forums/forum/openears-plugins/">RuleORama Free Support Forum</a></li>
</ul>
</li>
<li id="menu-item-1178" class="menu-item menu-item-type-post_type menu-item-object-page current_page_parent menu-item-1178"><a title="Blog" href="/blog/">Blog</a></li>
<li id="menu-item-1015870" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-1015870">
<a href="/shop">Shop</a>
<ul class="sub-menu">
	<li id="menu-item-8499" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-8499"><a href="/shop">View all products</a></li>
	<li id="menu-item-8486" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-8486"><a href="/cart/">Cart</a></li>
	<li id="menu-item-8498" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-8498"><a href="/order-tracking/">Track your order</a></li>
	<li id="menu-item-8539" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-8539"><a href="/termsandconditions/">AGB/Terms and Conditions</a></li>
</ul>
</li>
</ul>			        
			        			        <ul class="mini-cart">
						<li>
							<a href="/cart/" title="View your shopping cart" class="cart-parent">
								<span> 
							<span class="woocommerce-Price-amount amount"><bdi><span class="woocommerce-Price-currencySymbol">&euro;</span>0.00</bdi></span><mark>0</mark>							</span>
							</a>
							<ul class="cart_list"><li class="empty">No products in the cart.</li></ul>						</li>
					</ul>
			      				      	
				</nav><!-- /#navigation -->

				
<div id="header-social" class="social fr">
				<a href="http://feeds.feedburner.com/politepixblog" class="subscribe" title="RSS"></a>

				<a href="http://www.twitter.com/politepix" class="twitter" title="Follow me on Twitter"></a>

		</div>
<!-- /.social -->
				

			</div>
<!-- /#header-right -->
			
					
		</header><!-- /#header -->
	
	</div>
<!-- /#header-wrap -->
	
	
	       
    <div id="content" class="page col-full">
    
    	    	
		<section id="main" class="col-left"> 			

                                                                   
            <article class="post-1024045 topic type-topic status-publish hentry topic-tag-noise-problem-vadthreshold">
				
				<header>
			    	<h1>[Resolved] Noise problem even setting setVadThreshold openEars 2.0</h1>
				</header>
				
                <section class="entry">
                	
<div id="bbpress-forums" class="bbpress-wrapper">

	<div class="bbp-breadcrumb"><p><a href="/" class="bbp-breadcrumb-home">Home</a> <span class="bbp-breadcrumb-sep">&rsaquo;</span> <a href="/forums/" class="bbp-breadcrumb-root">Forums</a> <span class="bbp-breadcrumb-sep">&rsaquo;</span> <a href="/forums/forum/openearsforum/" class="bbp-breadcrumb-forum">OpenEars</a> <span class="bbp-breadcrumb-sep">&rsaquo;</span> <span class="bbp-breadcrumb-current">[Resolved] Noise problem even setting setVadThreshold openEars 2.0</span></p></div>
	
	
	
	
		<div class="bbp-topic-tags"><p>Tagged:&nbsp;<a href="/forums/tags/noise-problem-vadthreshold/" rel="tag">noise problem vadThreshold</a></p></div>
		<div class="bbp-template-notice info"><ul><li class="bbp-topic-description">This topic has 21 replies, 2 voices, and was last updated <a href="/forums/topic/noise-problem-even-setting-setvadthreshold-openears-2-0/#post-1024199" title="Reply To: [Resolved] Noise problem even setting setVadThreshold openEars 2.0">9 years, 3 months ago</a> by <a href="/forums/profile/maxgarmar/" title="View maxgarmar&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">maxgarmar</span></a>.</li></ul></div>
		
		
			
<div class="bbp-pagination">
	<div class="bbp-pagination-count">Viewing 22 posts - 1 through 22 (of 22 total)</div>
	<div class="bbp-pagination-links"></div>
</div>


			<div class="burma">Advertisement: <a href="/rapidears">&ldquo;Don't want to wait for pauses before receiving speech recognition results? try RapidEars!&rdquo;</a>
</div>
<p>
</p>
<ul id="topic-1024045-replies" class="forums bbp-replies">

	<li class="bbp-header">
		<div class="bbp-reply-author">Author</div>
<!-- .bbp-reply-author -->
		<div class="bbp-reply-content">Posts</div>
<!-- .bbp-reply-content -->
	</li>
<!-- .bbp-header -->

	<li class="bbp-body">

		
			
				
<div id="post-1024045" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">January 1, 2015 at 11:21 pm</span>

		
		<a href="/forums/topic/noise-problem-even-setting-setvadthreshold-openears-2-0/#post-1024045" class="bbp-reply-permalink">#1024045</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1024045 -->

<div class="loop-item-0 user-id-1735 bbp-parent-forum-3654 bbp-parent-topic-3654 bbp-reply-position-1 odd  post-1024045 topic type-topic status-publish hentry topic-tag-noise-problem-vadthreshold">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/maxgarmar/" title="View maxgarmar&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">maxgarmar</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Hi guys,</p>
<p>I was using until now openears 1.66 in an app which is currently working in Spanish and I have to say that I was really happy with the results and the accuracy.<br>
Then some days ago I realized that this version has problems with &#8220;ch&#8221; middle-phonemes words.<br>
For instance:<br>
&#8220;Leche&#8221; is working perfectly but however &#8220;Lechuga&#8221; is impossible to recognize.<br>
Then I read on the changelog the correction on 1.7.1 version about spanish phonemes and I guess this will solve my problem and I will be the happiest guy in the world :D. But I cannot download this version only the 2.0.<br>
I tried the 2.0 and then the words with &#8220;ch&#8221; in the middle are working now but the accuracy is worse than 1.66 and with a TV making noises in background openears is too sensitive and is crazy how it keeps listening in comparison with 1.66. I read all the post about that and I tried to change the values:</p>
<pre><code>[[OEPocketsphinxController sharedInstance] setSecondsOfSilenceToDetect:0.2];
 [[OEPocketsphinxController sharedInstance] setVadThreshold:3.0];</code></pre>
<p>but does not seems to do anything, I don&#8217;t feel any difference perhaps I am setting the values in the wrong place.</p>
<p>Anyway, could I download or get 1.7.1 version somehow?, would be awesome. I don&#8217;t find it in any place. If it is not possible, please could you help me to use 2.0 correctly?.</p>
<p>Thanks in advance ! and really you are doing a good job</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1024048" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">January 2, 2015 at 9:18 am</span>

		
		<a href="/forums/topic/noise-problem-even-setting-setvadthreshold-openears-2-0/#post-1024048" class="bbp-reply-permalink">#1024048</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1024048 -->

<div class="loop-item-1 user-id-4 bbp-parent-forum-3654 bbp-parent-topic-1024045 bbp-reply-position-2 even  post-1024048 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/halle-2/" title="View Halle Winkler&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Halle Winkler</span></a><div class="bbp-author-role">Politepix</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Welcome,</p>
<p>A couple of things that should help. </p>
<p>The first is that you can&#8217;t use unrealistically low secondsOfSilenceToDetect values like .1 or .2 with OpenEars 2.0 anymore. There isn&#8217;t an advantage to using a pause detection of a length which is less than a speaker&#8217;s actual pause (at least .3 seconds if not more) so this has never provided a benefit as a UI for the user and causes actual speech to be interrupted, as well as speech-like sounds to be submitted constantly for recognition. It doesn&#8217;t result in a real-time-like user experience because speech still has to be processed as a whole, and it is more frequently in the process of being submitted for recognition, meaning it is likely to not be listening at the time that the user is speaking due to processing. </p>
<p>secondsOfSilenceToDetect values that were significantly shorter than a speaker&#8217;s pause have been the cause of almost all of the issues reported for 2.0, so it&#8217;s pretty likely I&#8217;m going to limit that property to realistic values in upcoming versions. It will need to be increased to a value greater than .2 for 2.0.</p>
<p>From the upgrade guide: </p>
<pre>
Another API change is that before you start setting properties of OEPocketsphinxController for the first time, it is necessary to call its setActive method:

[[OEPocketsphinxController sharedInstance] setActive:TRUE error:nil];
</pre>
<p>If you haven&#8217;t called setActive: and your vadThreshold settings are the first properties you&#8217;re setting, this will cause them not to take effect.</p>
<p>Lastly, you can set vadThreshold to a value higher than 3.0 if 3.0 is still causing oversensitive recognition, although this is probably more due to the secondsOfSilenceToDetect setting.</p>
<p>Sorry, it isn&#8217;t possible to offer dual support to 1.x and 2.x simultaneously – if there are issues with 2.0 they need to be reported and then they will be fixed.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1024049" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">January 2, 2015 at 10:45 am</span>

		
		<a href="/forums/topic/noise-problem-even-setting-setvadthreshold-openears-2-0/#post-1024049" class="bbp-reply-permalink">#1024049</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1024049 -->

<div class="loop-item-2 user-id-1735 bbp-parent-forum-3654 bbp-parent-topic-1024045 bbp-reply-position-3 odd topic-author  post-1024049 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/maxgarmar/" title="View maxgarmar&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">maxgarmar</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Thanks Halle for that fast answer. </p>
<p>Ok I will try the couple of things you told me, but then before I would like to know a couple of things to fully understand how this is working. </p>
<p>First, what are the realistic values for secondsOfSilenceToDetect being the minimum .3 as you told me? What would be the best value for you if I tell you that I&#8217;m recognizing maximum 3 words in a phrase but normally it will be just one word?</p>
<p>Another thing is, I would like to know what is the maximum value for the vadThreshold to play with it. </p>
<p>Thanks a lot </p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1024050" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">January 2, 2015 at 11:59 am</span>

		
		<a href="/forums/topic/noise-problem-even-setting-setvadthreshold-openears-2-0/#post-1024050" class="bbp-reply-permalink">#1024050</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1024050 -->

<div class="loop-item-3 user-id-4 bbp-parent-forum-3654 bbp-parent-topic-1024045 bbp-reply-position-4 even  post-1024050 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/halle-2/" title="View Halle Winkler&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Halle Winkler</span></a><div class="bbp-author-role">Politepix</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Hi,</p>
<p>The maximum value for vadThreshold is 4.0. </p>
<p>I would expect 2.0-3.0 to work fine for most applications as long as the secondsOfSilenceToDetect has a realistic relationship with user pauses.</p>
<p>Regarding secondsOfSilenceToDetect, the smallest value I would expect to begin to give a good user experience is .3, since that is getting towards the average durations that speakers pause between utterances. I would probably expect it to be a bit larger at .4 or maybe .5 since the intention isn&#8217;t to only catch the speech of speakers whose pauses are shorter than the average.</p>
<p>If you are ever trying to recognize more than one word at a time this should be a value that is distinctly longer than a normal inter-word pause, since setting it to the same length or shorter means you are insuring that the user will have their speech interrupted and recognized while they are still in the process of active app-directed speech, and that also means their subsequent speech will not be heard by the engine while it is in the process of performing recognition on the partial speech that it interrupted, which is a big downside risk for a speech UX. That&#8217;s what the default value of .7 is meant to help with. Probably a value like .5 is still reasonable here to split the difference between good UX in terms of getting all the speech and good UX in terms of reactivity.</p>
<p>I don&#8217;t think there is a big gain in setting it to a tiny value purely in terms of the sense of reactiveness, because that latency is only going to be a small percentage of the overall interaction time which includes the time the user spoke, and their actual intended period of silence, and the time that the engine took to process the entire finalized speech, which is likely to be a minimum of 3 seconds overall (assuming that everything else that happens in your UI as a result of the speech is instantaneous), making a question of tenths of a second one way or another not the biggest part of the puzzle in terms of UX. </p>
<p>In some cases I think the idea behind setting it very low is to give a RapidEars-like lack of latency, but RapidEars returns the user speech continuously while it is still in progress, which is just a very different UX to the UX of complete utterances being analyzed after they have been finished, regardless of the secondsOfSilenceToDetect time.</p>
<p>In the absence of a real-time approach like that in RapidEars, I think it&#8217;s likely to make speech UI users happier when the secondsOfSilenceToDetect corresponds to their intention to denote that their app-directed speech is finished, versus other events such as a inter-word pause or hesitation.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1024053" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">January 2, 2015 at 3:29 pm</span>

		
		<a href="/forums/topic/noise-problem-even-setting-setvadthreshold-openears-2-0/#post-1024053" class="bbp-reply-permalink">#1024053</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1024053 -->

<div class="loop-item-4 user-id-1735 bbp-parent-forum-3654 bbp-parent-topic-1024045 bbp-reply-position-5 odd topic-author  post-1024053 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/maxgarmar/" title="View maxgarmar&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">maxgarmar</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Hi, </p>
<p>regarding what you you told me about where to set the vadThreshold and secondsOfSilenceToDetect, I was setting those values before setActive line, so I would expect that when I move it after this line I would feel a real difference. But I am sorry, nothing changed.</p>
<p>Tests:</p>
<p>vadThreshold = 4.0 (maximun)<br>
secondsOfSilenceToDetect= 0.3</p>
<p>and with the TV in background with a normal level of sound, the recognition does not stop &#8230; and it recognize 6 or 7 words.. when I just said one.</p>
<p>after this test I changed secondsOfSilenceToDetect to 0.4 and 0.5 but not with better results&#8230; I really don&#8217;t get what&#8217;s going on. What am I doing wrong, Halle? </p>
<p>Could I get somehow 1.7.1 version ? I would like to make tests with it to see the results. </p>
<p>Thanks, I hope we can get this working</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1024056" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">January 2, 2015 at 4:03 pm</span>

		
		<a href="/forums/topic/noise-problem-even-setting-setvadthreshold-openears-2-0/#post-1024056" class="bbp-reply-permalink">#1024056</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1024056 -->

<div class="loop-item-5 user-id-4 bbp-parent-forum-3654 bbp-parent-topic-1024045 bbp-reply-position-6 even  post-1024056 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/halle-2/" title="View Halle Winkler&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Halle Winkler</span></a><div class="bbp-author-role">Politepix</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>OK, can you make a replication case for me so I can see what you&#8217;re seeing exactly? </p>
<p>What I would need is a recording of your speech (in the environment you&#8217;re describing) added to pathToTestFile (there is a commented-out usage of pathToTestFile in the sample app and you can read more about it in OEPocketsphinxController.h) and the changes to the sample app which will demonstrate this issue (I believe the only changes should be vadThreshold, secondsOfSilenceToDetect, the generation of your language model, and the selection of the Spanish acoustic model). Once you can demonstrate the issue to yourself using the sample app, contact me through the contact form with a place to download your recording and a description of your changes to the sample app and I will test it out and get back to you. Sorry you&#8217;re having this issue and I promise I&#8217;ll check it out.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1024060" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">January 2, 2015 at 9:08 pm</span>

		
		<a href="/forums/topic/noise-problem-even-setting-setvadthreshold-openears-2-0/#post-1024060" class="bbp-reply-permalink">#1024060</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1024060 -->

<div class="loop-item-6 user-id-1735 bbp-parent-forum-3654 bbp-parent-topic-1024045 bbp-reply-position-7 odd topic-author  post-1024060 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/maxgarmar/" title="View maxgarmar&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">maxgarmar</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Ok here we go. </p>
<p>This is the complete code I changed on ViewController.m of the test application. You could see how I set the values and everything.</p>
<pre><code>//  ViewController.m
//  OpenEarsSampleApp
//
//  ViewController.m demonstrates the use of the OpenEars framework. 
//
//  Copyright Politepix UG (haftungsbeschränkt) 2014. All rights reserved.
//  https://www.politepix.com
//  Contact at https://www.politepix.com/contact
//
//  This file is licensed under the Politepix Shared Source license found in the root of the source distribution.

// **************************************************************************************************************************************************************
// **************************************************************************************************************************************************************
// **************************************************************************************************************************************************************
// IMPORTANT NOTE: Audio driver and hardware behavior is completely different between the Simulator and a real device. It is not informative to test OpenEars&#039; accuracy on the Simulator, and please do not report Simulator-only bugs since I only actively support 
// the device driver. Please only do testing/bug reporting based on results on a real device such as an iPhone or iPod Touch. Thanks!
// **************************************************************************************************************************************************************
// **************************************************************************************************************************************************************
// **************************************************************************************************************************************************************

#import &quot;ViewController.h&quot;
#import &lt;OpenEars/OEPocketsphinxController.h&gt;
#import &lt;OpenEars/OEFliteController.h&gt;
#import &lt;OpenEars/OELanguageModelGenerator.h&gt;
#import &lt;OpenEars/OELogging.h&gt;
#import &lt;OpenEars/OEAcousticModel.h&gt;
#import &lt;Slt/Slt.h&gt;

@interface ViewController()

// UI actions, not specifically related to OpenEars other than the fact that they invoke OpenEars methods.
- (IBAction) stopButtonAction;
- (IBAction) startButtonAction;
- (IBAction) suspendListeningButtonAction;
- (IBAction) resumeListeningButtonAction;

// Example for reading out the input audio levels without locking the UI using an NSTimer

- (void) startDisplayingLevels;
- (void) stopDisplayingLevels;

// These three are the important OpenEars objects that this class demonstrates the use of.
@property (nonatomic, strong) Slt *slt;

@property (nonatomic, strong) OEEventsObserver *openEarsEventsObserver;
@property (nonatomic, strong) OEPocketsphinxController *pocketsphinxController;
@property (nonatomic, strong) OEFliteController *fliteController;

// Some UI, not specifically related to OpenEars.
@property (nonatomic, strong) IBOutlet UIButton *stopButton;
@property (nonatomic, strong) IBOutlet UIButton *startButton;
@property (nonatomic, strong) IBOutlet UIButton *suspendListeningButton;	
@property (nonatomic, strong) IBOutlet UIButton *resumeListeningButton;	
@property (nonatomic, strong) IBOutlet UITextView *statusTextView;
@property (nonatomic, strong) IBOutlet UITextView *heardTextView;
@property (nonatomic, strong) IBOutlet UILabel *pocketsphinxDbLabel;
@property (nonatomic, strong) IBOutlet UILabel *fliteDbLabel;
@property (nonatomic, assign) BOOL usingStartingLanguageModel;
@property (nonatomic, assign) int restartAttemptsDueToPermissionRequests;
@property (nonatomic, assign) BOOL startupFailedDueToLackOfPermissions;

// Things which help us show off the dynamic language features.
@property (nonatomic, copy) NSString *pathToFirstDynamicallyGeneratedLanguageModel;
@property (nonatomic, copy) NSString *pathToFirstDynamicallyGeneratedDictionary;
@property (nonatomic, copy) NSString *pathToSecondDynamicallyGeneratedLanguageModel;
@property (nonatomic, copy) NSString *pathToSecondDynamicallyGeneratedDictionary;

// Our NSTimer that will help us read and display the input and output levels without locking the UI
@property (nonatomic, strong) 	NSTimer *uiUpdateTimer;

@end

@implementation ViewController

#define kLevelUpdatesPerSecond 18 // We&#039;ll have the ui update 18 times a second to show some fluidity without hitting the CPU too hard.

//#define kGetNbest // Uncomment this if you want to try out nbest
#pragma mark - 
#pragma mark Memory Management

- (void)dealloc {
    [self stopDisplayingLevels];
}

#pragma mark -
#pragma mark View Lifecycle

- (void)viewDidLoad {
    [super viewDidLoad];
    self.fliteController = [[OEFliteController alloc] init];
    self.openEarsEventsObserver = [[OEEventsObserver alloc] init];
    self.openEarsEventsObserver.delegate = self;
    self.slt = [[Slt alloc] init];
    
    self.restartAttemptsDueToPermissionRequests = 0;
    self.startupFailedDueToLackOfPermissions = FALSE;
    
    // [OELogging startOpenEarsLogging]; // Uncomment me for OELogging, which is verbose logging about internal OpenEars operations such as audio settings. If you have issues, show this logging in the forums.
    //[OEPocketsphinxController sharedInstance].verbosePocketSphinx = TRUE; // Uncomment this for much more verbose speech recognition engine output. If you have issues, show this logging in the forums.
    
    [self.openEarsEventsObserver setDelegate:self]; // Make this class the delegate of OpenEarsObserver so we can get all of the messages about what OpenEars is doing.
    
    [[OEPocketsphinxController sharedInstance] setActive:TRUE error:nil]; // Call this before setting any OEPocketsphinxController characteristics
    
    [[OEPocketsphinxController sharedInstance] setSecondsOfSilenceToDetect:0.3];
    [[OEPocketsphinxController sharedInstance] setVadThreshold:4.0];
    
    // This is the language model we&#039;re going to start up with. The only reason I&#039;m making it a class property is that I reuse it a bunch of times in this example, 
    // but you can pass the string contents directly to OEPocketsphinxController:startListeningWithLanguageModelAtPath:dictionaryAtPath:languageModelIsJSGF:
    
    NSArray *firstLanguageArray = @[@&quot;ADIOS&quot;,
                                    @&quot;LECHUGA&quot;,
                                    @&quot;MADRID&quot;,
                                    @&quot;BARCELONA&quot;,
                                    @&quot;PARIS&quot;,
                                    @&quot;ROMA&quot;,
                                    @&quot;MAINZ&quot;,
                                    @&quot;HOLA&quot;];
    
    OELanguageModelGenerator *languageModelGenerator = [[OELanguageModelGenerator alloc] init];
    
    // languageModelGenerator.verboseLanguageModelGenerator = TRUE; // Uncomment me for verbose language model generator debug output.

    NSError *error = [languageModelGenerator generateLanguageModelFromArray:firstLanguageArray withFilesNamed:@&quot;FirstOpenEarsDynamicLanguageModel&quot; forAcousticModelAtPath:[OEAcousticModel pathToModel:@&quot;AcousticModelSpanish&quot;]]; // Change &quot;AcousticModelSpanish&quot; to &quot;AcousticModelSpanish&quot; in order to create a language model for Spanish recognition instead of English.
    
    
    if(error) {
        NSLog(@&quot;Dynamic language generator reported error %@&quot;, [error description]);	
    } else {
        self.pathToFirstDynamicallyGeneratedLanguageModel = [languageModelGenerator pathToSuccessfullyGeneratedLanguageModelWithRequestedName:@&quot;FirstOpenEarsDynamicLanguageModel&quot;];
        self.pathToFirstDynamicallyGeneratedDictionary = [languageModelGenerator pathToSuccessfullyGeneratedDictionaryWithRequestedName:@&quot;FirstOpenEarsDynamicLanguageModel&quot;];
    }
    
    self.usingStartingLanguageModel = TRUE; // This is not an OpenEars thing, this is just so I can switch back and forth between the two models in this sample app.
    
    // Here is an example of dynamically creating an in-app grammar.
    
    // We want it to be able to response to the speech &quot;CHANGE MODEL&quot; and a few other things.  Items we want to have recognized as a whole phrase (like &quot;CHANGE MODEL&quot;) 
    // we put into the array as one string (e.g. &quot;CHANGE MODEL&quot; instead of &quot;CHANGE&quot; and &quot;MODEL&quot;). This increases the probability that they will be recognized as a phrase. This works even better starting with version 1.0 of OpenEars.
    
    
    NSArray *secondLanguageArray = @[@&quot;ADIOS&quot;,
                                    @&quot;LECHUGA&quot;,
                                    @&quot;MADRID&quot;,
                                    @&quot;BARCELONA&quot;,
                                    @&quot;PARIS&quot;,
                                    @&quot;ROMA&quot;,
                                    @&quot;MAINZ&quot;,
                                    @&quot;HOLA&quot;];
    
    // The last entry, quidnunc, is an example of a word which will not be found in the lookup dictionary and will be passed to the fallback method. The fallback method is slower,
    // so, for instance, creating a new language model from dictionary words will be pretty fast, but a model that has a lot of unusual names in it or invented/rare/recent-slang
    // words will be slower to generate. You can use this information to give your users good UI feedback about what the expectations for wait times should be.
    
    // I don&#039;t think it&#039;s beneficial to lazily instantiate OELanguageModelGenerator because you only need to give it a single message and then release it.
    // If you need to create a very large model or any size of model that has many unusual words that have to make use of the fallback generation method,
    // you will want to run this on a background thread so you can give the user some UI feedback that the task is in progress.
        
    // generateLanguageModelFromArray:withFilesNamed returns an NSError which will either have a value of noErr if everything went fine or a specific error if it didn&#039;t.
    error = [languageModelGenerator generateLanguageModelFromArray:secondLanguageArray withFilesNamed:@&quot;SecondOpenEarsDynamicLanguageModel&quot; forAcousticModelAtPath:[OEAcousticModel pathToModel:@&quot;AcousticModelSpanish&quot;]]; // Change &quot;AcousticModelSpanish&quot; to &quot;AcousticModelSpanish&quot; in order to create a language model for Spanish recognition instead of English.
    
    //    NSError *error = [languageModelGenerator generateLanguageModelFromTextFile:[NSString stringWithFormat:@&quot;%@/%@&quot;,[[NSBundle mainBundle] resourcePath], @&quot;OpenEarsCorpus.txt&quot;] withFilesNamed:@&quot;SecondOpenEarsDynamicLanguageModel&quot; forAcousticModelAtPath:[OEAcousticModel pathToModel:@&quot;AcousticModelSpanish&quot;]]; // Try this out to see how generating a language model from a corpus works.
    
    
    if(error) {
        NSLog(@&quot;Dynamic language generator reported error %@&quot;, [error description]);	
    }	else {
        
        self.pathToSecondDynamicallyGeneratedLanguageModel = [languageModelGenerator pathToSuccessfullyGeneratedLanguageModelWithRequestedName:@&quot;SecondOpenEarsDynamicLanguageModel&quot;]; // We&#039;ll set our new .languagemodel file to be the one to get switched to when the words &quot;CHANGE MODEL&quot; are recognized.
        self.pathToSecondDynamicallyGeneratedDictionary = [languageModelGenerator pathToSuccessfullyGeneratedDictionaryWithRequestedName:@&quot;SecondOpenEarsDynamicLanguageModel&quot;];; // We&#039;ll set our new dictionary to be the one to get switched to when the words &quot;CHANGE MODEL&quot; are recognized.
        
        // Next, an informative message.
        
        NSLog(@&quot;\n\nWelcome to the OpenEars sample project. This project understands the words:\nBACKWARD,\nCHANGE,\nFORWARD,\nGO,\nLEFT,\nMODEL,\nRIGHT,\nTURN,\nand if you say \&quot;CHANGE MODEL\&quot; it will switch to its dynamically-generated model which understands the words:\nCHANGE,\nMODEL,\nMONDAY,\nTUESDAY,\nWEDNESDAY,\nTHURSDAY,\nFRIDAY,\nSATURDAY,\nSUNDAY,\nQUIDNUNC&quot;);
        
        // This is how to start the continuous listening loop of an available instance of OEPocketsphinxController. We won&#039;t do this if the language generation failed since it will be listening for a command to change over to the generated language.
        
        [[OEPocketsphinxController sharedInstance] setActive:TRUE error:nil]; // Call this once before setting properties of the OEPocketsphinxController instance.
        
        
        [[OEPocketsphinxController sharedInstance] setSecondsOfSilenceToDetect:0.3];
        [[OEPocketsphinxController sharedInstance] setVadThreshold:4.0];

        [OEPocketsphinxController sharedInstance].pathToTestFile = [[NSBundle mainBundle] pathForResource:@&quot;openears&quot; ofType:@&quot;wav&quot;];  // This is how you could use a test WAV (mono/16-bit/16k) rather than live recognition
       
        
        if(![OEPocketsphinxController sharedInstance].isListening) {
            [[OEPocketsphinxController sharedInstance] startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&quot;AcousticModelSpanish&quot;] languageModelIsJSGF:FALSE]; // Start speech recognition if we aren&#039;t already listening.
        }
        // [self startDisplayingLevels] is not an OpenEars method, just a very simple approach for level reading
        // that I&#039;ve included with this sample app. My example implementation does make use of two OpenEars
        // methods:	the pocketsphinxInputLevel method of OEPocketsphinxController and the fliteOutputLevel
        // method of fliteController. 
        //
        // The example is meant to show one way that you can read those levels continuously without locking the UI, 
        // by using an NSTimer, but the OpenEars level-reading methods 
        // themselves do not include multithreading code since I believe that you will want to design your own 
        // code approaches for level display that are tightly-integrated with your interaction design and the  
        // graphics API you choose. 
   
        [self startDisplayingLevels];
        
        // Here is some UI stuff that has nothing specifically to do with OpenEars implementation
        self.startButton.hidden = TRUE;
        self.stopButton.hidden = TRUE;
        self.suspendListeningButton.hidden = TRUE;
        self.resumeListeningButton.hidden = TRUE;
    }
}

#pragma mark -
#pragma mark OEEventsObserver delegate methods

// What follows are all of the delegate methods you can optionally use once you&#039;ve instantiated an OEEventsObserver and set its delegate to self. 
// I&#039;ve provided some pretty granular information about the exact phase of the Pocketsphinx listening loop, the Audio Session, and Flite, but I&#039;d expect 
// that the ones that will really be needed by most projects are the following:
//
//- (void) pocketsphinxDidReceiveHypothesis:(NSString *)hypothesis recognitionScore:(NSString *)recognitionScore utteranceID:(NSString *)utteranceID;
//- (void) audioSessionInterruptionDidBegin;
//- (void) audioSessionInterruptionDidEnd;
//- (void) audioRouteDidChangeToRoute:(NSString *)newRoute;
//- (void) pocketsphinxDidStartListening;
//- (void) pocketsphinxDidStopListening;
//
// It isn&#039;t necessary to have a OEPocketsphinxController or a OEFliteController instantiated in order to use these methods.  If there isn&#039;t anything instantiated that will
// send messages to an OEEventsObserver, all that will happen is that these methods will never fire.  You also do not have to create a OEEventsObserver in
// the same class or view controller in which you are doing things with a OEPocketsphinxController or OEFliteController; you can receive updates from those objects in
// any class in which you instantiate an OEEventsObserver and set its delegate to self.

// This is an optional delegate method of OEEventsObserver which delivers the text of speech that Pocketsphinx heard and analyzed, along with its accuracy score and utterance ID.
- (void) pocketsphinxDidReceiveHypothesis:(NSString *)hypothesis recognitionScore:(NSString *)recognitionScore utteranceID:(NSString *)utteranceID {
    
    NSLog(@&quot;Local callback: The received hypothesis is %@ with a score of %@ and an ID of %@&quot;, hypothesis, recognitionScore, utteranceID); // Log it.
    if([hypothesis isEqualToString:@&quot;CHANGE MODEL&quot;]) { // If the user says &quot;CHANGE MODEL&quot;, we will switch to the alternate model (which happens to be the dynamically generated model).
        
        // Here is an example of language model switching in OpenEars. Deciding on what logical basis to switch models is your responsibility.
        // For instance, when you call a customer service line and get a response tree that takes you through different options depending on what you say to it,
        // the models are being switched as you progress through it so that only relevant choices can be understood. The construction of that logical branching and 
        // how to react to it is your job, OpenEars just lets you send the signal to switch the language model when you&#039;ve decided it&#039;s the right time to do so.
        
        if(self.usingStartingLanguageModel) { // If we&#039;re on the starting model, switch to the dynamically generated one.
            
            // You can only change language models with ARPA grammars in OpenEars (the ones that end in .languagemodel or .DMP). 
            // Trying to switch between JSGF models (the ones that end in .gram) will return no result.
            [[OEPocketsphinxController sharedInstance] changeLanguageModelToFile:self.pathToSecondDynamicallyGeneratedLanguageModel withDictionary:self.pathToSecondDynamicallyGeneratedDictionary]; 
            self.usingStartingLanguageModel = FALSE;
        } else { // If we&#039;re on the dynamically generated model, switch to the start model (this is just an example of a trigger and method for switching models).
            [[OEPocketsphinxController sharedInstance] changeLanguageModelToFile:self.pathToFirstDynamicallyGeneratedLanguageModel withDictionary:self.pathToFirstDynamicallyGeneratedDictionary];
            self.usingStartingLanguageModel = TRUE;
        }
    }
    
    self.heardTextView.text = [NSString stringWithFormat:@&quot;Heard: \&quot;%@\&quot;&quot;, hypothesis]; // Show it in the status box.
    
    // This is how to use an available instance of OEFliteController. We&#039;re going to repeat back the command that we heard with the voice we&#039;ve chosen.
    [self.fliteController say:[NSString stringWithFormat:@&quot;You said %@&quot;,hypothesis] withVoice:self.slt];
}

#ifdef kGetNbest   
- (void) pocketsphinxDidReceiveNBestHypothesisArray:(NSArray *)hypothesisArray { // Pocketsphinx has an n-best hypothesis dictionary.
    NSLog(@&quot;Local callback:  hypothesisArray is %@&quot;,hypothesisArray);   
}
#endif
// An optional delegate method of OEEventsObserver which informs that there was an interruption to the audio session (e.g. an incoming phone call).
- (void) audioSessionInterruptionDidBegin {
    NSLog(@&quot;Local callback:  AudioSession interruption began.&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: AudioSession interruption began.&quot;; // Show it in the status box.
    NSError *error = nil;
    if([OEPocketsphinxController sharedInstance].isListening) {
        error = [[OEPocketsphinxController sharedInstance] stopListening]; // React to it by telling Pocketsphinx to stop listening (if it is listening) since it will need to restart its loop after an interruption.
        if(error) NSLog(@&quot;Error while stopping listening in audioSessionInterruptionDidBegin: %@&quot;, error);
    }
}

// An optional delegate method of OEEventsObserver which informs that the interruption to the audio session ended.
- (void) audioSessionInterruptionDidEnd {
    NSLog(@&quot;Local callback:  AudioSession interruption ended.&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: AudioSession interruption ended.&quot;; // Show it in the status box.
    // We&#039;re restarting the previously-stopped listening loop.
    if(![OEPocketsphinxController sharedInstance].isListening){
        [[OEPocketsphinxController sharedInstance] startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&quot;AcousticModelSpanish&quot;] languageModelIsJSGF:FALSE]; // Start speech recognition if we aren&#039;t currently listening.    
    }
}

// An optional delegate method of OEEventsObserver which informs that the audio input became unavailable.
- (void) audioInputDidBecomeUnavailable {
    NSLog(@&quot;Local callback:  The audio input has become unavailable&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: The audio input has become unavailable&quot;; // Show it in the status box.
    NSError *error = nil;
    if([OEPocketsphinxController sharedInstance].isListening){
        error = [[OEPocketsphinxController sharedInstance] stopListening]; // React to it by telling Pocketsphinx to stop listening since there is no available input (but only if we are listening).
        if(error) NSLog(@&quot;Error while stopping listening in audioInputDidBecomeUnavailable: %@&quot;, error);
    }
}

// An optional delegate method of OEEventsObserver which informs that the unavailable audio input became available again.
- (void) audioInputDidBecomeAvailable {
    NSLog(@&quot;Local callback: The audio input is available&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: The audio input is available&quot;; // Show it in the status box.
    if(![OEPocketsphinxController sharedInstance].isListening) {
        [[OEPocketsphinxController sharedInstance] startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&quot;AcousticModelSpanish&quot;] languageModelIsJSGF:FALSE]; // Start speech recognition, but only if we aren&#039;t already listening.
    }
}
// An optional delegate method of OEEventsObserver which informs that there was a change to the audio route (e.g. headphones were plugged in or unplugged).
- (void) audioRouteDidChangeToRoute:(NSString *)newRoute {
    NSLog(@&quot;Local callback: Audio route change. The new audio route is %@&quot;, newRoute); // Log it.
    self.statusTextView.text = [NSString stringWithFormat:@&quot;Status: Audio route change. The new audio route is %@&quot;,newRoute]; // Show it in the status box.
    
    NSError *error = [[OEPocketsphinxController sharedInstance] stopListening]; // React to it by telling the Pocketsphinx loop to shut down and then start listening again on the new route
    
    if(error)NSLog(@&quot;Local callback: error while stopping listening in audioRouteDidChangeToRoute: %@&quot;,error);

    if(![OEPocketsphinxController sharedInstance].isListening) {
        [[OEPocketsphinxController sharedInstance] startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&quot;AcousticModelSpanish&quot;] languageModelIsJSGF:FALSE]; // Start speech recognition if we aren&#039;t already listening.
    }
}

// An optional delegate method of OEEventsObserver which informs that the Pocketsphinx recognition loop has entered its actual loop.
// This might be useful in debugging a conflict between another sound class and Pocketsphinx.
- (void) pocketsphinxRecognitionLoopDidStart {
    
    NSLog(@&quot;Local callback: Pocketsphinx started.&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: Pocketsphinx started.&quot;; // Show it in the status box.
}

// An optional delegate method of OEEventsObserver which informs that Pocketsphinx is now listening for speech.
- (void) pocketsphinxDidStartListening {
    
    NSLog(@&quot;Local callback: Pocketsphinx is now listening.&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: Pocketsphinx is now listening.&quot;; // Show it in the status box.
    
    self.startButton.hidden = TRUE; // React to it with some UI changes.
    self.stopButton.hidden = FALSE;
    self.suspendListeningButton.hidden = FALSE;
    self.resumeListeningButton.hidden = TRUE;
}

// An optional delegate method of OEEventsObserver which informs that Pocketsphinx detected speech and is starting to process it.
- (void) pocketsphinxDidDetectSpeech {
    NSLog(@&quot;Local callback: Pocketsphinx has detected speech.&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: Pocketsphinx has detected speech.&quot;; // Show it in the status box.
}

// An optional delegate method of OEEventsObserver which informs that Pocketsphinx detected a second of silence, indicating the end of an utterance. 
// This was added because developers requested being able to time the recognition speed without the speech time. The processing time is the time between 
// this method being called and the hypothesis being returned.
- (void) pocketsphinxDidDetectFinishedSpeech {
    NSLog(@&quot;Local callback: Pocketsphinx has detected a second of silence, concluding an utterance.&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: Pocketsphinx has detected finished speech.&quot;; // Show it in the status box.
}

// An optional delegate method of OEEventsObserver which informs that Pocketsphinx has exited its recognition loop, most 
// likely in response to the OEPocketsphinxController being told to stop listening via the stopListening method.
- (void) pocketsphinxDidStopListening {
    NSLog(@&quot;Local callback: Pocketsphinx has stopped listening.&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: Pocketsphinx has stopped listening.&quot;; // Show it in the status box.
}

// An optional delegate method of OEEventsObserver which informs that Pocketsphinx is still in its listening loop but it is not
// Going to react to speech until listening is resumed.  This can happen as a result of Flite speech being
// in progress on an audio route that doesn&#039;t support simultaneous Flite speech and Pocketsphinx recognition,
// or as a result of the OEPocketsphinxController being told to suspend recognition via the suspendRecognition method.
- (void) pocketsphinxDidSuspendRecognition {
    NSLog(@&quot;Local callback: Pocketsphinx has suspended recognition.&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: Pocketsphinx has suspended recognition.&quot;; // Show it in the status box.
}

// An optional delegate method of OEEventsObserver which informs that Pocketsphinx is still in its listening loop and after recognition
// having been suspended it is now resuming.  This can happen as a result of Flite speech completing
// on an audio route that doesn&#039;t support simultaneous Flite speech and Pocketsphinx recognition,
// or as a result of the OEPocketsphinxController being told to resume recognition via the resumeRecognition method.
- (void) pocketsphinxDidResumeRecognition {
    NSLog(@&quot;Local callback: Pocketsphinx has resumed recognition.&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: Pocketsphinx has resumed recognition.&quot;; // Show it in the status box.
}

// An optional delegate method which informs that Pocketsphinx switched over to a new language model at the given URL in the course of
// recognition. This does not imply that it is a valid file or that recognition will be successful using the file.
- (void) pocketsphinxDidChangeLanguageModelToFile:(NSString *)newLanguageModelPathAsString andDictionary:(NSString *)newDictionaryPathAsString {
    NSLog(@&quot;Local callback: Pocketsphinx is now using the following language model: \n%@ and the following dictionary: %@&quot;,newLanguageModelPathAsString,newDictionaryPathAsString);
}

// An optional delegate method of OEEventsObserver which informs that Flite is speaking, most likely to be useful if debugging a
// complex interaction between sound classes. You don&#039;t have to do anything yourself in order to prevent Pocketsphinx from listening to Flite talk and trying to recognize the speech.
- (void) fliteDidStartSpeaking {
    NSLog(@&quot;Local callback: Flite has started speaking&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: Flite has started speaking.&quot;; // Show it in the status box.
}

// An optional delegate method of OEEventsObserver which informs that Flite is finished speaking, most likely to be useful if debugging a
// complex interaction between sound classes.
- (void) fliteDidFinishSpeaking {
    NSLog(@&quot;Local callback: Flite has finished speaking&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: Flite has finished speaking.&quot;; // Show it in the status box.
}

- (void) pocketSphinxContinuousSetupDidFailWithReason:(NSString *)reasonForFailure { // This can let you know that something went wrong with the recognition loop startup. Turn on [OELogging startOpenEarsLogging] to learn why.
    NSLog(@&quot;Local callback: Setting up the continuous recognition loop has failed for the reason %@, please turn on [OELogging startOpenEarsLogging] to learn more.&quot;, reasonForFailure); // Log it.
    self.statusTextView.text = @&quot;Status: Not possible to start recognition loop.&quot;; // Show it in the status box.	
}

- (void) pocketSphinxContinuousTeardownDidFailWithReason:(NSString *)reasonForFailure { // This can let you know that something went wrong with the recognition loop startup. Turn on [OELogging startOpenEarsLogging] to learn why.
    NSLog(@&quot;Local callback: Tearing down the continuous recognition loop has failed for the reason %@, please turn on [OELogging startOpenEarsLogging] to learn more.&quot;, reasonForFailure); // Log it.
    self.statusTextView.text = @&quot;Status: Not possible to cleanly end recognition loop.&quot;; // Show it in the status box.	
}

- (void) testRecognitionCompleted { // A test file which was submitted for direct recognition via the audio driver is done.
    NSLog(@&quot;Local callback: A test file which was submitted for direct recognition via the audio driver is done.&quot;); // Log it.
    NSError *error = nil;
    if([OEPocketsphinxController sharedInstance].isListening) { // If we&#039;re listening, stop listening.
        error = [[OEPocketsphinxController sharedInstance] stopListening];
        if(error) NSLog(@&quot;Error while stopping listening in testRecognitionCompleted: %@&quot;, error);
    }
    
}
/** Pocketsphinx couldn&#039;t start because it has no mic permissions (will only be returned on iOS7 or later).*/
- (void) pocketsphinxFailedNoMicPermissions {
    NSLog(@&quot;Local callback: The user has never set mic permissions or denied permission to this app&#039;s mic, so listening will not start.&quot;);
    self.startupFailedDueToLackOfPermissions = TRUE;
}

/** The user prompt to get mic permissions, or a check of the mic permissions, has completed with a TRUE or a FALSE result  (will only be returned on iOS7 or later).*/
- (void) micPermissionCheckCompleted:(BOOL)result {
    if(result) {
        self.restartAttemptsDueToPermissionRequests++;
        if(self.restartAttemptsDueToPermissionRequests == 1 &amp;&amp; self.startupFailedDueToLackOfPermissions) { // If we get here because there was an attempt to start which failed due to lack of permissions, and now permissions have been requested and they returned true, we restart exactly once with the new permissions.
            NSError *error = nil;
            if([OEPocketsphinxController sharedInstance].isListening){
                error = [[OEPocketsphinxController sharedInstance] stopListening]; // Stop listening if we are listening.
                if(error) NSLog(@&quot;Error while stopping listening in micPermissionCheckCompleted: %@&quot;, error);
            }
            if(!error &amp;&amp; ![OEPocketsphinxController sharedInstance].isListening) { // If there was no error and we aren&#039;t listening, start listening.
                [[OEPocketsphinxController sharedInstance] startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&quot;AcousticModelSpanish&quot;] languageModelIsJSGF:FALSE]; // Start speech recognition.
                    self.startupFailedDueToLackOfPermissions = FALSE;
            }
        }
    }
}

#pragma mark -
#pragma mark UI

// This is not OpenEars-specific stuff, just some UI behavior

- (IBAction) suspendListeningButtonAction { // This is the action for the button which suspends listening without ending the recognition loop
    [[OEPocketsphinxController sharedInstance] suspendRecognition];	
    
    self.startButton.hidden = TRUE;
    self.stopButton.hidden = FALSE;
    self.suspendListeningButton.hidden = TRUE;
    self.resumeListeningButton.hidden = FALSE;
}

- (IBAction) resumeListeningButtonAction { // This is the action for the button which resumes listening if it has been suspended
    [[OEPocketsphinxController sharedInstance] resumeRecognition];
    
    self.startButton.hidden = TRUE;
    self.stopButton.hidden = FALSE;
    self.suspendListeningButton.hidden = FALSE;
    self.resumeListeningButton.hidden = TRUE;	
}

- (IBAction) stopButtonAction { // This is the action for the button which shuts down the recognition loop.
    NSError *error = nil;
    if([OEPocketsphinxController sharedInstance].isListening) { // Stop if we are currently listening.
        error = [[OEPocketsphinxController sharedInstance] stopListening];
        if(error)NSLog(@&quot;Error stopping listening in stopButtonAction: %@&quot;, error);
    }
    self.startButton.hidden = FALSE;
    self.stopButton.hidden = TRUE;
    self.suspendListeningButton.hidden = TRUE;
    self.resumeListeningButton.hidden = TRUE;
}

- (IBAction) startButtonAction { // This is the action for the button which starts up the recognition loop again if it has been shut down.
    if(![OEPocketsphinxController sharedInstance].isListening) {
        [[OEPocketsphinxController sharedInstance] startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&quot;AcousticModelSpanish&quot;] languageModelIsJSGF:FALSE]; // Start speech recognition if we aren&#039;t already listening.
    }
    self.startButton.hidden = TRUE;
    self.stopButton.hidden = FALSE;
    self.suspendListeningButton.hidden = FALSE;
    self.resumeListeningButton.hidden = TRUE;
}

#pragma mark -
#pragma mark Example for reading out Pocketsphinx and Flite audio levels without locking the UI by using an NSTimer

// What follows are not OpenEars methods, just an approach for level reading
// that I&#039;ve included with this sample app. My example implementation does make use of two OpenEars
// methods:	the pocketsphinxInputLevel method of OEPocketsphinxController and the fliteOutputLevel
// method of OEFliteController. 
//
// The example is meant to show one way that you can read those levels continuously without locking the UI, 
// by using an NSTimer, but the OpenEars level-reading methods 
// themselves do not include multithreading code since I believe that you will want to design your own 
// code approaches for level display that are tightly-integrated with your interaction design and the  
// graphics API you choose. 
// 
// Please note that if you use my sample approach, you should pay attention to the way that the timer is always stopped in
// dealloc. This should prevent you from having any difficulties with deallocating a class due to a running NSTimer process.

- (void) startDisplayingLevels { // Start displaying the levels using a timer
    [self stopDisplayingLevels]; // We never want more than one timer valid so we&#039;ll stop any running timers first.
    self.uiUpdateTimer = [NSTimer scheduledTimerWithTimeInterval:1.0/kLevelUpdatesPerSecond target:self selector:@selector(updateLevelsUI) userInfo:nil repeats:YES];
}

- (void) stopDisplayingLevels { // Stop displaying the levels by stopping the timer if it&#039;s running.
    if(self.uiUpdateTimer &amp;&amp; [self.uiUpdateTimer isValid]) { // If there is a running timer, we&#039;ll stop it here.
        [self.uiUpdateTimer invalidate];
        self.uiUpdateTimer = nil;
    }
}

- (void) updateLevelsUI { // And here is how we obtain the levels.  This method includes the actual OpenEars methods and uses their results to update the UI of this view controller.
    
    self.pocketsphinxDbLabel.text = [NSString stringWithFormat:@&quot;Pocketsphinx Input level:%f&quot;,[[OEPocketsphinxController sharedInstance] pocketsphinxInputLevel]];  //pocketsphinxInputLevel is an OpenEars method of the class OEPocketsphinxController.
    
    if(self.fliteController.speechInProgress) {
        self.fliteDbLabel.text = [NSString stringWithFormat:@&quot;Flite Output level: %f&quot;,[self.fliteController fliteOutputLevel]]; // fliteOutputLevel is an OpenEars method of the class OEFliteController.
    }
}

@end
</code></pre>
<p>This was the result in the console:</p>
<p><strong>2015-01-02 20:48:16.057 OpenEarsSampleApp[1154:60b] Local callback: Pocketsphinx is now listening.<br>
2015-01-02 20:48:16.062 OpenEarsSampleApp[1154:60b] Local callback: Pocketsphinx started.<br>
2015-01-02 20:48:16.115 OpenEarsSampleApp[1154:60b] Local callback: Pocketsphinx has detected speech.<br>
2015-01-02 20:48:32.753 OpenEarsSampleApp[1154:60b] Local callback: Pocketsphinx has detected a second of silence, concluding an utterance.<br>
2015-01-02 20:48:33.136 OpenEarsSampleApp[1154:60b] Local callback: The received hypothesis is ROMA with a score of 0 and an ID of 0<br>
2015-01-02 20:48:33.364 OpenEarsSampleApp[1154:60b] Local callback: Flite has started speaking<br>
2015-01-02 20:48:33.372 OpenEarsSampleApp[1154:60b] Local callback: Pocketsphinx has suspended recognition.<br>
2015-01-02 20:48:35.130 OpenEarsSampleApp[1154:60b] Local callback: Flite has finished speaking<br>
2015-01-02 20:48:35.137 OpenEarsSampleApp[1154:60b] Local callback: Pocketsphinx has resumed recognition.</strong></p>
<p><a href="https://dl.dropboxusercontent.com/u/6380067/openears.wav" title="openears.wav" target="_blank" rel="nofollow"></a></p>
<p>Here is the link to download the voice recording regarding that I did it with the internal microphone of iPhone 5 perfectly working with the version 1.66 of openEars. I hope this can help us to find what&#8217;s going on.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1024063" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">January 2, 2015 at 9:13 pm</span>

		
		<a href="/forums/topic/noise-problem-even-setting-setvadthreshold-openears-2-0/#post-1024063" class="bbp-reply-permalink">#1024063</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1024063 -->

<div class="loop-item-7 user-id-1735 bbp-parent-forum-3654 bbp-parent-topic-1024045 bbp-reply-position-8 even topic-author  post-1024063 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/maxgarmar/" title="View maxgarmar&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">maxgarmar</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Sorry I was trying to add the link via the tag &#8220;link&#8221; from the editor but I don&#8217;t see the result finally. I edited two times but did not work. Or perhaps it is hidden ?</p>
<p>anyway here it is without the tag:</p>
<p><a href="https://dl.dropboxusercontent.com/u/6380067/openears.wav.zip" rel="nofollow">https://dl.dropboxusercontent.com/u/6380067/openears.wav.zip</a></p>
<p>and compressed.</p>
<p>Sorry for writing again.</p>
<p>Thanks</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1024077" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">January 5, 2015 at 3:17 pm</span>

		
		<a href="/forums/topic/noise-problem-even-setting-setvadthreshold-openears-2-0/#post-1024077" class="bbp-reply-permalink">#1024077</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1024077 -->

<div class="loop-item-8 user-id-1735 bbp-parent-forum-3654 bbp-parent-topic-1024045 bbp-reply-position-9 odd topic-author  post-1024077 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/maxgarmar/" title="View maxgarmar&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">maxgarmar</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Hi Halle, </p>
<p>How far did you get with the problem I was facing ? as you see the recognition thread never ends while my recording is working (because the TV in background) with 1.x it&#8217;s stop in every word I say. That&#8217;s the problem I think. 2.0 never stops listening because the noise.</p>
<p>By the way, playing around your nice framework I just realized if you add any word that before was not recognizing to the <strong>LanguageModelGeneratorLookupList.text</strong> file then the recognition works perfectly. So my problem is solved for one specific word which is crazy if I have to add all the words which the framework is not recognizing.<br>
Then I looked at the same file in 2.0 version and I see that that dictionary is exactly the same like 1.x but still it is working for words that 1.x does not recognize except that you add them to the dictionary.<br>
I hope it helps you to find what&#8217;s happening.</p>
<p>Thanks</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1024078" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">January 5, 2015 at 4:57 pm</span>

		
		<a href="/forums/topic/noise-problem-even-setting-setvadthreshold-openears-2-0/#post-1024078" class="bbp-reply-permalink">#1024078</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1024078 -->

<div class="loop-item-9 user-id-4 bbp-parent-forum-3654 bbp-parent-topic-1024045 bbp-reply-position-10 even  post-1024078 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/halle-2/" title="View Halle Winkler&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Halle Winkler</span></a><div class="bbp-author-role">Politepix</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Yep, I&#8217;ve taken a look at your example and I think I have an idea what the issue is due to. I will be running tests for a few days and then I may have either a fix to push or a beta for you to check out. I appreciate the good example, it was very helpful.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1024090" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">January 6, 2015 at 7:08 pm</span>

		
		<a href="/forums/topic/noise-problem-even-setting-setvadthreshold-openears-2-0/#post-1024090" class="bbp-reply-permalink">#1024090</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1024090 -->

<div class="loop-item-10 user-id-4 bbp-parent-forum-3654 bbp-parent-topic-1024045 bbp-reply-position-11 odd  post-1024090 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/halle-2/" title="View Halle Winkler&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Halle Winkler</span></a><div class="bbp-author-role">Politepix</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>OK, take a look at OpenEars 2.01 out today and see if it improves this issue. In my testing of your example, it detected your statements correctly. Make sure you bring in both the 2.01 framework and the 2.01 spanish acoustic model into any app you are testing. Let me know how it works for you.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1024108" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">January 7, 2015 at 12:21 pm</span>

		
		<a href="/forums/topic/noise-problem-even-setting-setvadthreshold-openears-2-0/#post-1024108" class="bbp-reply-permalink">#1024108</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1024108 -->

<div class="loop-item-11 user-id-1735 bbp-parent-forum-3654 bbp-parent-topic-1024045 bbp-reply-position-12 even topic-author  post-1024108 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/maxgarmar/" title="View maxgarmar&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">maxgarmar</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Ok Halle, after an exhaustive test comparing openEars 2.0 noise treatment and 1.x. OpenEars 2.0.1 has improved a lot, congratulations and thanks by the way, but still the sensibility is bigger than 1.x. If you like I can send you again another .wav with cases that 2.0.1 is still recognizing. But I think is not necessary.<br>
Thinking about apps using this library, almost all the cases will have noises around it so I would increase the vadThreshold to a bigger value like 6.0 or whatever to avoid more noises for people like me that think about how the library is used. If I record with Apple&#8217;s Voice Memos app from apple you can see that the noises around me are not moving even the bars of the app until I am speaking directly to the mic. But still openEars 2 is listening.</p>
<p>People that want to recognize more noises then they could decrease this value to 1.0 for instance. </p>
<p>What do you think? Please don&#8217;t hesitate to ask or collect any information from me. I am just trying to help you.</p>
<p>Thanks </p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1024109" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">January 7, 2015 at 12:34 pm</span>

		
		<a href="/forums/topic/noise-problem-even-setting-setvadthreshold-openears-2-0/#post-1024109" class="bbp-reply-permalink">#1024109</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1024109 -->

<div class="loop-item-12 user-id-4 bbp-parent-forum-3654 bbp-parent-topic-1024045 bbp-reply-position-13 odd  post-1024109 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/halle-2/" title="View Halle Winkler&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Halle Winkler</span></a><div class="bbp-author-role">Politepix</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Yes, please send another example demonstrating the issue you&#8217;ve described in your most recent post, with a WAV and your minimal changes to the sample app, letting me know the following:</p>
<p>1. which device and iOS version you tested both the old version and the new version,<br>
2. your own log results for the old version and the new version with both verbosePocketsphinx and OpenEarsLogging/OELogging turned on. To avoid complications with the &#8220;CH&#8221; issue in your OpenEars 1.x version please test with a language model that doesn&#8217;t have this phoneme.</p>
<p>a vadThreshold as high as 3.5 will suppress actual user speech in testing, so this is probably going to be due to something different if there is still a significant difference in perceived speech onset sensitivity. However, when I ran your test case against 2.01, I had nothing recognized other than your two utterances, they were recognized correctly and immediately on completion, and that was with the default vadThreshold (2.0), so I would need a new case from you if you are seeing something different.</p>
<p>Thanks!</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1024113" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">January 7, 2015 at 1:15 pm</span>

		
		<a href="/forums/topic/noise-problem-even-setting-setvadthreshold-openears-2-0/#post-1024113" class="bbp-reply-permalink">#1024113</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1024113 -->

<div class="loop-item-13 user-id-1735 bbp-parent-forum-3654 bbp-parent-topic-1024045 bbp-reply-position-14 even topic-author  post-1024113 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/maxgarmar/" title="View maxgarmar&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">maxgarmar</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Well ok, let&#8217;s do it again.</p>
<p>1. The devices and versions are the same. Just I have my app with 1.7 installed and also your example app directly taken from the 2.0.1 download on my iPhone 5 with 7.0.1 version.<br>
Sorry but in my app I could not take any recording values because in production testing and I can&#8217;t modify the code. But I can tell you that is not sensible like 2.0.1.<br>
Anyway, regarding your phrase &#8220;a vadThreshold as high as 3.5 will suppress actual user speech in testing&#8221; is enough to know that something is really wrong with 2.0.1 because here my test and you will see the results:</p>
<p>2.</p>
<pre><code>//  ViewController.m
//  OpenEarsSampleApp
//
//  ViewController.m demonstrates the use of the OpenEars framework.
//
//  Copyright Politepix UG (haftungsbeschränkt) 2014. All rights reserved.
//  https://www.politepix.com
//  Contact at https://www.politepix.com/contact
//
//  This file is licensed under the Politepix Shared Source license found in the root of the source distribution.

// **************************************************************************************************************************************************************
// **************************************************************************************************************************************************************
// **************************************************************************************************************************************************************
// IMPORTANT NOTE: Audio driver and hardware behavior is completely different between the Simulator and a real device. It is not informative to test OpenEars&#039; accuracy on the Simulator, and please do not report Simulator-only bugs since I only actively support
// the device driver. Please only do testing/bug reporting based on results on a real device such as an iPhone or iPod Touch. Thanks!
// **************************************************************************************************************************************************************
// **************************************************************************************************************************************************************
// **************************************************************************************************************************************************************

#import &quot;ViewController.h&quot;
#import &lt;OpenEars/OEPocketsphinxController.h&gt;
#import &lt;OpenEars/OEFliteController.h&gt;
#import &lt;OpenEars/OELanguageModelGenerator.h&gt;
#import &lt;OpenEars/OELogging.h&gt;
#import &lt;OpenEars/OEAcousticModel.h&gt;
#import &lt;Slt/Slt.h&gt;

@interface ViewController()

// UI actions, not specifically related to OpenEars other than the fact that they invoke OpenEars methods.
- (IBAction) stopButtonAction;
- (IBAction) startButtonAction;
- (IBAction) suspendListeningButtonAction;
- (IBAction) resumeListeningButtonAction;

// Example for reading out the input audio levels without locking the UI using an NSTimer

- (void) startDisplayingLevels;
- (void) stopDisplayingLevels;

// These three are the important OpenEars objects that this class demonstrates the use of.
@property (nonatomic, strong) Slt *slt;

@property (nonatomic, strong) OEEventsObserver *openEarsEventsObserver;
@property (nonatomic, strong) OEPocketsphinxController *pocketsphinxController;
@property (nonatomic, strong) OEFliteController *fliteController;

// Some UI, not specifically related to OpenEars.
@property (nonatomic, strong) IBOutlet UIButton *stopButton;
@property (nonatomic, strong) IBOutlet UIButton *startButton;
@property (nonatomic, strong) IBOutlet UIButton *suspendListeningButton;
@property (nonatomic, strong) IBOutlet UIButton *resumeListeningButton;
@property (nonatomic, strong) IBOutlet UITextView *statusTextView;
@property (nonatomic, strong) IBOutlet UITextView *heardTextView;
@property (nonatomic, strong) IBOutlet UILabel *pocketsphinxDbLabel;
@property (nonatomic, strong) IBOutlet UILabel *fliteDbLabel;
@property (nonatomic, assign) BOOL usingStartingLanguageModel;
@property (nonatomic, assign) int restartAttemptsDueToPermissionRequests;
@property (nonatomic, assign) BOOL startupFailedDueToLackOfPermissions;

// Things which help us show off the dynamic language features.
@property (nonatomic, copy) NSString *pathToFirstDynamicallyGeneratedLanguageModel;
@property (nonatomic, copy) NSString *pathToFirstDynamicallyGeneratedDictionary;
@property (nonatomic, copy) NSString *pathToSecondDynamicallyGeneratedLanguageModel;
@property (nonatomic, copy) NSString *pathToSecondDynamicallyGeneratedDictionary;

// Our NSTimer that will help us read and display the input and output levels without locking the UI
@property (nonatomic, strong) 	NSTimer *uiUpdateTimer;

@end

@implementation ViewController

#define kLevelUpdatesPerSecond 18 // We&#039;ll have the ui update 18 times a second to show some fluidity without hitting the CPU too hard.

#define kGetNbest // Uncomment this if you want to try out nbest
#pragma mark -
#pragma mark Memory Management

- (void)dealloc {
    [self stopDisplayingLevels];
}

#pragma mark -
#pragma mark View Lifecycle

- (void)viewDidLoad {
    [super viewDidLoad];
    self.fliteController = [[OEFliteController alloc] init];
    self.openEarsEventsObserver = [[OEEventsObserver alloc] init];
    self.openEarsEventsObserver.delegate = self;
    self.slt = [[Slt alloc] init];
    
    self.restartAttemptsDueToPermissionRequests = 0;
    self.startupFailedDueToLackOfPermissions = FALSE;
    
     [OELogging startOpenEarsLogging]; // Uncomment me for OELogging, which is verbose logging about internal OpenEars operations such as audio settings. If you have issues, show this logging in the forums.
    [OEPocketsphinxController sharedInstance].verbosePocketSphinx = TRUE; // Uncomment this for much more verbose speech recognition engine output. If you have issues, show this logging in the forums.
    
    [self.openEarsEventsObserver setDelegate:self]; // Make this class the delegate of OpenEarsObserver so we can get all of the messages about what OpenEars is doing.
    
    [[OEPocketsphinxController sharedInstance] setActive:TRUE error:nil]; // Call this before setting any OEPocketsphinxController characteristics
    
    [OEPocketsphinxController sharedInstance].returnNbest = TRUE;
    [OEPocketsphinxController sharedInstance].nBestNumber = 5;
    
    [[OEPocketsphinxController sharedInstance] setSecondsOfSilenceToDetect:0.5];
    [[OEPocketsphinxController sharedInstance] setVadThreshold:3.5];
    
    // This is the language model we&#039;re going to start up with. The only reason I&#039;m making it a class property is that I reuse it a bunch of times in this example,
    // but you can pass the string contents directly to OEPocketsphinxController:startListeningWithLanguageModelAtPath:dictionaryAtPath:languageModelIsJSGF:
    
    NSArray *firstLanguageArray = @[@&quot;ADIOS&quot;,
                                     @&quot;LECHUGA&quot;,
                                     @&quot;MADRID&quot;,
                                     @&quot;BARCELONA&quot;,
                                     @&quot;PARIS&quot;,
                                     @&quot;ROMA&quot;,
                                     @&quot;MAINZ&quot;,
                                     @&quot;HOLA&quot;,
                                     @&quot;CHORIZO&quot;,
                                     @&quot;HORCHATA&quot;];
    
    OELanguageModelGenerator *languageModelGenerator = [[OELanguageModelGenerator alloc] init];
    
    languageModelGenerator.verboseLanguageModelGenerator = TRUE; // Uncomment me for verbose language model generator debug output.
    
    NSError *error = [languageModelGenerator generateLanguageModelFromArray:firstLanguageArray withFilesNamed:@&quot;FirstOpenEarsDynamicLanguageModel&quot; forAcousticModelAtPath:[OEAcousticModel pathToModel:@&quot;AcousticModelSpanish&quot;]]; // Change &quot;AcousticModelSpanish&quot; to &quot;AcousticModelSpanish&quot; in order to create a language model for Spanish recognition instead of English.
    
    
    if(error) {
        NSLog(@&quot;Dynamic language generator reported error %@&quot;, [error description]);
    } else {
        self.pathToFirstDynamicallyGeneratedLanguageModel = [languageModelGenerator pathToSuccessfullyGeneratedLanguageModelWithRequestedName:@&quot;FirstOpenEarsDynamicLanguageModel&quot;];
        self.pathToFirstDynamicallyGeneratedDictionary = [languageModelGenerator pathToSuccessfullyGeneratedDictionaryWithRequestedName:@&quot;FirstOpenEarsDynamicLanguageModel&quot;];
    }
    
    self.usingStartingLanguageModel = TRUE; // This is not an OpenEars thing, this is just so I can switch back and forth between the two models in this sample app.
    
    // Here is an example of dynamically creating an in-app grammar.
    
    // We want it to be able to response to the speech &quot;CHANGE MODEL&quot; and a few other things.  Items we want to have recognized as a whole phrase (like &quot;CHANGE MODEL&quot;)
    // we put into the array as one string (e.g. &quot;CHANGE MODEL&quot; instead of &quot;CHANGE&quot; and &quot;MODEL&quot;). This increases the probability that they will be recognized as a phrase. This works even better starting with version 1.0 of OpenEars.
    
    
    NSArray *secondLanguageArray = @[@&quot;ADIOS&quot;,
                                     @&quot;LECHUGA&quot;,
                                     @&quot;MADRID&quot;,
                                     @&quot;BARCELONA&quot;,
                                     @&quot;PARIS&quot;,
                                     @&quot;ROMA&quot;,
                                     @&quot;MAINZ&quot;,
                                     @&quot;HOLA&quot;,
                                     @&quot;CHORIZO&quot;,
                                     @&quot;HORCHATA&quot;];
    
    // The last entry, quidnunc, is an example of a word which will not be found in the lookup dictionary and will be passed to the fallback method. The fallback method is slower,
    // so, for instance, creating a new language model from dictionary words will be pretty fast, but a model that has a lot of unusual names in it or invented/rare/recent-slang
    // words will be slower to generate. You can use this information to give your users good UI feedback about what the expectations for wait times should be.
    
    // I don&#039;t think it&#039;s beneficial to lazily instantiate OELanguageModelGenerator because you only need to give it a single message and then release it.
    // If you need to create a very large model or any size of model that has many unusual words that have to make use of the fallback generation method,
    // you will want to run this on a background thread so you can give the user some UI feedback that the task is in progress.
    
    // generateLanguageModelFromArray:withFilesNamed returns an NSError which will either have a value of noErr if everything went fine or a specific error if it didn&#039;t.
    error = [languageModelGenerator generateLanguageModelFromArray:secondLanguageArray withFilesNamed:@&quot;SecondOpenEarsDynamicLanguageModel&quot; forAcousticModelAtPath:[OEAcousticModel pathToModel:@&quot;AcousticModelSpanish&quot;]]; // Change &quot;AcousticModelSpanish&quot; to &quot;AcousticModelSpanish&quot; in order to create a language model for Spanish recognition instead of English.
    
    //    NSError *error = [languageModelGenerator generateLanguageModelFromTextFile:[NSString stringWithFormat:@&quot;%@/%@&quot;,[[NSBundle mainBundle] resourcePath], @&quot;OpenEarsCorpus.txt&quot;] withFilesNamed:@&quot;SecondOpenEarsDynamicLanguageModel&quot; forAcousticModelAtPath:[OEAcousticModel pathToModel:@&quot;AcousticModelSpanish&quot;]]; // Try this out to see how generating a language model from a corpus works.
    
    
    if(error) {
        NSLog(@&quot;Dynamic language generator reported error %@&quot;, [error description]);
    }	else {
        
        self.pathToSecondDynamicallyGeneratedLanguageModel = [languageModelGenerator pathToSuccessfullyGeneratedLanguageModelWithRequestedName:@&quot;SecondOpenEarsDynamicLanguageModel&quot;]; // We&#039;ll set our new .languagemodel file to be the one to get switched to when the words &quot;CHANGE MODEL&quot; are recognized.
        self.pathToSecondDynamicallyGeneratedDictionary = [languageModelGenerator pathToSuccessfullyGeneratedDictionaryWithRequestedName:@&quot;SecondOpenEarsDynamicLanguageModel&quot;];; // We&#039;ll set our new dictionary to be the one to get switched to when the words &quot;CHANGE MODEL&quot; are recognized.
        
        // Next, an informative message.
        
        NSLog(@&quot;\n\nWelcome to the OpenEars sample project. This project understands the words:\nBACKWARD,\nCHANGE,\nFORWARD,\nGO,\nLEFT,\nMODEL,\nRIGHT,\nTURN,\nand if you say \&quot;CHANGE MODEL\&quot; it will switch to its dynamically-generated model which understands the words:\nCHANGE,\nMODEL,\nMONDAY,\nTUESDAY,\nWEDNESDAY,\nTHURSDAY,\nFRIDAY,\nSATURDAY,\nSUNDAY,\nQUIDNUNC&quot;);
        
        // This is how to start the continuous listening loop of an available instance of OEPocketsphinxController. We won&#039;t do this if the language generation failed since it will be listening for a command to change over to the generated language.
        
        [[OEPocketsphinxController sharedInstance] setActive:TRUE error:nil]; // Call this once before setting properties of the OEPocketsphinxController instance.
        
        
        [[OEPocketsphinxController sharedInstance] setSecondsOfSilenceToDetect:0.5];
        [[OEPocketsphinxController sharedInstance] setVadThreshold:3.5];
        
        
        [OEPocketsphinxController sharedInstance].pathToTestFile = [[NSBundle mainBundle] pathForResource:@&quot;openears4&quot; ofType:@&quot;wav&quot;];  // This is how you could use a test WAV (mono/16-bit/16k) rather than live recognition
        
        
        if(![OEPocketsphinxController sharedInstance].isListening) {
            [[OEPocketsphinxController sharedInstance] startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&quot;AcousticModelSpanish&quot;] languageModelIsJSGF:FALSE]; // Start speech recognition if we aren&#039;t already listening.
        }
        // [self startDisplayingLevels] is not an OpenEars method, just a very simple approach for level reading
        // that I&#039;ve included with this sample app. My example implementation does make use of two OpenEars
        // methods:	the pocketsphinxInputLevel method of OEPocketsphinxController and the fliteOutputLevel
        // method of fliteController.
        //
        // The example is meant to show one way that you can read those levels continuously without locking the UI,
        // by using an NSTimer, but the OpenEars level-reading methods
        // themselves do not include multithreading code since I believe that you will want to design your own
        // code approaches for level display that are tightly-integrated with your interaction design and the
        // graphics API you choose.
        
        [self startDisplayingLevels];
        
        // Here is some UI stuff that has nothing specifically to do with OpenEars implementation
        self.startButton.hidden = TRUE;
        self.stopButton.hidden = TRUE;
        self.suspendListeningButton.hidden = TRUE;
        self.resumeListeningButton.hidden = TRUE;
    }
}

#pragma mark -
#pragma mark OEEventsObserver delegate methods

// What follows are all of the delegate methods you can optionally use once you&#039;ve instantiated an OEEventsObserver and set its delegate to self.
// I&#039;ve provided some pretty granular information about the exact phase of the Pocketsphinx listening loop, the Audio Session, and Flite, but I&#039;d expect
// that the ones that will really be needed by most projects are the following:
//
//- (void) pocketsphinxDidReceiveHypothesis:(NSString *)hypothesis recognitionScore:(NSString *)recognitionScore utteranceID:(NSString *)utteranceID;
//- (void) audioSessionInterruptionDidBegin;
//- (void) audioSessionInterruptionDidEnd;
//- (void) audioRouteDidChangeToRoute:(NSString *)newRoute;
//- (void) pocketsphinxDidStartListening;
//- (void) pocketsphinxDidStopListening;
//
// It isn&#039;t necessary to have a OEPocketsphinxController or a OEFliteController instantiated in order to use these methods.  If there isn&#039;t anything instantiated that will
// send messages to an OEEventsObserver, all that will happen is that these methods will never fire.  You also do not have to create a OEEventsObserver in
// the same class or view controller in which you are doing things with a OEPocketsphinxController or OEFliteController; you can receive updates from those objects in
// any class in which you instantiate an OEEventsObserver and set its delegate to self.

// This is an optional delegate method of OEEventsObserver which delivers the text of speech that Pocketsphinx heard and analyzed, along with its accuracy score and utterance ID.
- (void) pocketsphinxDidReceiveHypothesis:(NSString *)hypothesis recognitionScore:(NSString *)recognitionScore utteranceID:(NSString *)utteranceID {
    
    NSLog(@&quot;Local callback: The received hypothesis is %@ with a score of %@ and an ID of %@&quot;, hypothesis, recognitionScore, utteranceID); // Log it.
    if([hypothesis isEqualToString:@&quot;CHANGE MODEL&quot;]) { // If the user says &quot;CHANGE MODEL&quot;, we will switch to the alternate model (which happens to be the dynamically generated model).
        
        // Here is an example of language model switching in OpenEars. Deciding on what logical basis to switch models is your responsibility.
        // For instance, when you call a customer service line and get a response tree that takes you through different options depending on what you say to it,
        // the models are being switched as you progress through it so that only relevant choices can be understood. The construction of that logical branching and
        // how to react to it is your job, OpenEars just lets you send the signal to switch the language model when you&#039;ve decided it&#039;s the right time to do so.
        
        if(self.usingStartingLanguageModel) { // If we&#039;re on the starting model, switch to the dynamically generated one.
            
            // You can only change language models with ARPA grammars in OpenEars (the ones that end in .languagemodel or .DMP).
            // Trying to switch between JSGF models (the ones that end in .gram) will return no result.
            [[OEPocketsphinxController sharedInstance] changeLanguageModelToFile:self.pathToSecondDynamicallyGeneratedLanguageModel withDictionary:self.pathToSecondDynamicallyGeneratedDictionary];
            self.usingStartingLanguageModel = FALSE;
        } else { // If we&#039;re on the dynamically generated model, switch to the start model (this is just an example of a trigger and method for switching models).
            [[OEPocketsphinxController sharedInstance] changeLanguageModelToFile:self.pathToFirstDynamicallyGeneratedLanguageModel withDictionary:self.pathToFirstDynamicallyGeneratedDictionary];
            self.usingStartingLanguageModel = TRUE;
        }
    }
    
    self.heardTextView.text = [NSString stringWithFormat:@&quot;Heard: \&quot;%@\&quot;&quot;, hypothesis]; // Show it in the status box.
    
    // This is how to use an available instance of OEFliteController. We&#039;re going to repeat back the command that we heard with the voice we&#039;ve chosen.
    [self.fliteController say:[NSString stringWithFormat:@&quot;You said %@&quot;,hypothesis] withVoice:self.slt];
}

#ifdef kGetNbest
- (void) pocketsphinxDidReceiveNBestHypothesisArray:(NSArray *)hypothesisArray { // Pocketsphinx has an n-best hypothesis dictionary.
    NSLog(@&quot;Local callback:  hypothesisArray is %@&quot;,hypothesisArray);
}
#endif
// An optional delegate method of OEEventsObserver which informs that there was an interruption to the audio session (e.g. an incoming phone call).
- (void) audioSessionInterruptionDidBegin {
    NSLog(@&quot;Local callback:  AudioSession interruption began.&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: AudioSession interruption began.&quot;; // Show it in the status box.
    NSError *error = nil;
    if([OEPocketsphinxController sharedInstance].isListening) {
        error = [[OEPocketsphinxController sharedInstance] stopListening]; // React to it by telling Pocketsphinx to stop listening (if it is listening) since it will need to restart its loop after an interruption.
        if(error) NSLog(@&quot;Error while stopping listening in audioSessionInterruptionDidBegin: %@&quot;, error);
    }
}

// An optional delegate method of OEEventsObserver which informs that the interruption to the audio session ended.
- (void) audioSessionInterruptionDidEnd {
    NSLog(@&quot;Local callback:  AudioSession interruption ended.&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: AudioSession interruption ended.&quot;; // Show it in the status box.
    // We&#039;re restarting the previously-stopped listening loop.
    if(![OEPocketsphinxController sharedInstance].isListening){
        [[OEPocketsphinxController sharedInstance] startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&quot;AcousticModelSpanish&quot;] languageModelIsJSGF:FALSE]; // Start speech recognition if we aren&#039;t currently listening.
    }
}

// An optional delegate method of OEEventsObserver which informs that the audio input became unavailable.
- (void) audioInputDidBecomeUnavailable {
    NSLog(@&quot;Local callback:  The audio input has become unavailable&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: The audio input has become unavailable&quot;; // Show it in the status box.
    NSError *error = nil;
    if([OEPocketsphinxController sharedInstance].isListening){
        error = [[OEPocketsphinxController sharedInstance] stopListening]; // React to it by telling Pocketsphinx to stop listening since there is no available input (but only if we are listening).
        if(error) NSLog(@&quot;Error while stopping listening in audioInputDidBecomeUnavailable: %@&quot;, error);
    }
}

// An optional delegate method of OEEventsObserver which informs that the unavailable audio input became available again.
- (void) audioInputDidBecomeAvailable {
    NSLog(@&quot;Local callback: The audio input is available&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: The audio input is available&quot;; // Show it in the status box.
    if(![OEPocketsphinxController sharedInstance].isListening) {
        [[OEPocketsphinxController sharedInstance] startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&quot;AcousticModelSpanish&quot;] languageModelIsJSGF:FALSE]; // Start speech recognition, but only if we aren&#039;t already listening.
    }
}
// An optional delegate method of OEEventsObserver which informs that there was a change to the audio route (e.g. headphones were plugged in or unplugged).
- (void) audioRouteDidChangeToRoute:(NSString *)newRoute {
    NSLog(@&quot;Local callback: Audio route change. The new audio route is %@&quot;, newRoute); // Log it.
    self.statusTextView.text = [NSString stringWithFormat:@&quot;Status: Audio route change. The new audio route is %@&quot;,newRoute]; // Show it in the status box.
    
    NSError *error = [[OEPocketsphinxController sharedInstance] stopListening]; // React to it by telling the Pocketsphinx loop to shut down and then start listening again on the new route
    
    if(error)NSLog(@&quot;Local callback: error while stopping listening in audioRouteDidChangeToRoute: %@&quot;,error);
    
    if(![OEPocketsphinxController sharedInstance].isListening) {
        [[OEPocketsphinxController sharedInstance] startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&quot;AcousticModelSpanish&quot;] languageModelIsJSGF:FALSE]; // Start speech recognition if we aren&#039;t already listening.
    }
}

// An optional delegate method of OEEventsObserver which informs that the Pocketsphinx recognition loop has entered its actual loop.
// This might be useful in debugging a conflict between another sound class and Pocketsphinx.
- (void) pocketsphinxRecognitionLoopDidStart {
    
    NSLog(@&quot;Local callback: Pocketsphinx started.&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: Pocketsphinx started.&quot;; // Show it in the status box.
}

// An optional delegate method of OEEventsObserver which informs that Pocketsphinx is now listening for speech.
- (void) pocketsphinxDidStartListening {
    
    NSLog(@&quot;Local callback: Pocketsphinx is now listening.&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: Pocketsphinx is now listening.&quot;; // Show it in the status box.
    
    self.startButton.hidden = TRUE; // React to it with some UI changes.
    self.stopButton.hidden = FALSE;
    self.suspendListeningButton.hidden = FALSE;
    self.resumeListeningButton.hidden = TRUE;
}

// An optional delegate method of OEEventsObserver which informs that Pocketsphinx detected speech and is starting to process it.
- (void) pocketsphinxDidDetectSpeech {
    NSLog(@&quot;Local callback: Pocketsphinx has detected speech.&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: Pocketsphinx has detected speech.&quot;; // Show it in the status box.
}

// An optional delegate method of OEEventsObserver which informs that Pocketsphinx detected a second of silence, indicating the end of an utterance.
// This was added because developers requested being able to time the recognition speed without the speech time. The processing time is the time between
// this method being called and the hypothesis being returned.
- (void) pocketsphinxDidDetectFinishedSpeech {
    NSLog(@&quot;Local callback: Pocketsphinx has detected a second of silence, concluding an utterance.&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: Pocketsphinx has detected finished speech.&quot;; // Show it in the status box.
}

// An optional delegate method of OEEventsObserver which informs that Pocketsphinx has exited its recognition loop, most
// likely in response to the OEPocketsphinxController being told to stop listening via the stopListening method.
- (void) pocketsphinxDidStopListening {
    NSLog(@&quot;Local callback: Pocketsphinx has stopped listening.&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: Pocketsphinx has stopped listening.&quot;; // Show it in the status box.
}

// An optional delegate method of OEEventsObserver which informs that Pocketsphinx is still in its listening loop but it is not
// Going to react to speech until listening is resumed.  This can happen as a result of Flite speech being
// in progress on an audio route that doesn&#039;t support simultaneous Flite speech and Pocketsphinx recognition,
// or as a result of the OEPocketsphinxController being told to suspend recognition via the suspendRecognition method.
- (void) pocketsphinxDidSuspendRecognition {
    NSLog(@&quot;Local callback: Pocketsphinx has suspended recognition.&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: Pocketsphinx has suspended recognition.&quot;; // Show it in the status box.
}

// An optional delegate method of OEEventsObserver which informs that Pocketsphinx is still in its listening loop and after recognition
// having been suspended it is now resuming.  This can happen as a result of Flite speech completing
// on an audio route that doesn&#039;t support simultaneous Flite speech and Pocketsphinx recognition,
// or as a result of the OEPocketsphinxController being told to resume recognition via the resumeRecognition method.
- (void) pocketsphinxDidResumeRecognition {
    NSLog(@&quot;Local callback: Pocketsphinx has resumed recognition.&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: Pocketsphinx has resumed recognition.&quot;; // Show it in the status box.
}

// An optional delegate method which informs that Pocketsphinx switched over to a new language model at the given URL in the course of
// recognition. This does not imply that it is a valid file or that recognition will be successful using the file.
- (void) pocketsphinxDidChangeLanguageModelToFile:(NSString *)newLanguageModelPathAsString andDictionary:(NSString *)newDictionaryPathAsString {
    NSLog(@&quot;Local callback: Pocketsphinx is now using the following language model: \n%@ and the following dictionary: %@&quot;,newLanguageModelPathAsString,newDictionaryPathAsString);
}

// An optional delegate method of OEEventsObserver which informs that Flite is speaking, most likely to be useful if debugging a
// complex interaction between sound classes. You don&#039;t have to do anything yourself in order to prevent Pocketsphinx from listening to Flite talk and trying to recognize the speech.
- (void) fliteDidStartSpeaking {
    NSLog(@&quot;Local callback: Flite has started speaking&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: Flite has started speaking.&quot;; // Show it in the status box.
}

// An optional delegate method of OEEventsObserver which informs that Flite is finished speaking, most likely to be useful if debugging a
// complex interaction between sound classes.
- (void) fliteDidFinishSpeaking {
    NSLog(@&quot;Local callback: Flite has finished speaking&quot;); // Log it.
    self.statusTextView.text = @&quot;Status: Flite has finished speaking.&quot;; // Show it in the status box.
}

- (void) pocketSphinxContinuousSetupDidFailWithReason:(NSString *)reasonForFailure { // This can let you know that something went wrong with the recognition loop startup. Turn on [OELogging startOpenEarsLogging] to learn why.
    NSLog(@&quot;Local callback: Setting up the continuous recognition loop has failed for the reason %@, please turn on [OELogging startOpenEarsLogging] to learn more.&quot;, reasonForFailure); // Log it.
    self.statusTextView.text = @&quot;Status: Not possible to start recognition loop.&quot;; // Show it in the status box.
}

- (void) pocketSphinxContinuousTeardownDidFailWithReason:(NSString *)reasonForFailure { // This can let you know that something went wrong with the recognition loop startup. Turn on [OELogging startOpenEarsLogging] to learn why.
    NSLog(@&quot;Local callback: Tearing down the continuous recognition loop has failed for the reason %@, please turn on [OELogging startOpenEarsLogging] to learn more.&quot;, reasonForFailure); // Log it.
    self.statusTextView.text = @&quot;Status: Not possible to cleanly end recognition loop.&quot;; // Show it in the status box.
}

- (void) testRecognitionCompleted { // A test file which was submitted for direct recognition via the audio driver is done.
    NSLog(@&quot;Local callback: A test file which was submitted for direct recognition via the audio driver is done.&quot;); // Log it.
    NSError *error = nil;
    if([OEPocketsphinxController sharedInstance].isListening) { // If we&#039;re listening, stop listening.
        error = [[OEPocketsphinxController sharedInstance] stopListening];
        if(error) NSLog(@&quot;Error while stopping listening in testRecognitionCompleted: %@&quot;, error);
    }
    
}
/** Pocketsphinx couldn&#039;t start because it has no mic permissions (will only be returned on iOS7 or later).*/
- (void) pocketsphinxFailedNoMicPermissions {
    NSLog(@&quot;Local callback: The user has never set mic permissions or denied permission to this app&#039;s mic, so listening will not start.&quot;);
    self.startupFailedDueToLackOfPermissions = TRUE;
}

/** The user prompt to get mic permissions, or a check of the mic permissions, has completed with a TRUE or a FALSE result  (will only be returned on iOS7 or later).*/
- (void) micPermissionCheckCompleted:(BOOL)result {
    if(result) {
        self.restartAttemptsDueToPermissionRequests++;
        if(self.restartAttemptsDueToPermissionRequests == 1 &amp;&amp; self.startupFailedDueToLackOfPermissions) { // If we get here because there was an attempt to start which failed due to lack of permissions, and now permissions have been requested and they returned true, we restart exactly once with the new permissions.
            NSError *error = nil;
            if([OEPocketsphinxController sharedInstance].isListening){
                error = [[OEPocketsphinxController sharedInstance] stopListening]; // Stop listening if we are listening.
                if(error) NSLog(@&quot;Error while stopping listening in micPermissionCheckCompleted: %@&quot;, error);
            }
            if(!error &amp;&amp; ![OEPocketsphinxController sharedInstance].isListening) { // If there was no error and we aren&#039;t listening, start listening.
                [[OEPocketsphinxController sharedInstance] startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&quot;AcousticModelSpanish&quot;] languageModelIsJSGF:FALSE]; // Start speech recognition.
                self.startupFailedDueToLackOfPermissions = FALSE;
            }
        }
    }
}

#pragma mark -
#pragma mark UI

// This is not OpenEars-specific stuff, just some UI behavior

- (IBAction) suspendListeningButtonAction { // This is the action for the button which suspends listening without ending the recognition loop
    [[OEPocketsphinxController sharedInstance] suspendRecognition];
    
    self.startButton.hidden = TRUE;
    self.stopButton.hidden = FALSE;
    self.suspendListeningButton.hidden = TRUE;
    self.resumeListeningButton.hidden = FALSE;
}

- (IBAction) resumeListeningButtonAction { // This is the action for the button which resumes listening if it has been suspended
    [[OEPocketsphinxController sharedInstance] resumeRecognition];
    
    self.startButton.hidden = TRUE;
    self.stopButton.hidden = FALSE;
    self.suspendListeningButton.hidden = FALSE;
    self.resumeListeningButton.hidden = TRUE;
}

- (IBAction) stopButtonAction { // This is the action for the button which shuts down the recognition loop.
    NSError *error = nil;
    if([OEPocketsphinxController sharedInstance].isListening) { // Stop if we are currently listening.
        error = [[OEPocketsphinxController sharedInstance] stopListening];
        if(error)NSLog(@&quot;Error stopping listening in stopButtonAction: %@&quot;, error);
    }
    self.startButton.hidden = FALSE;
    self.stopButton.hidden = TRUE;
    self.suspendListeningButton.hidden = TRUE;
    self.resumeListeningButton.hidden = TRUE;
}

- (IBAction) startButtonAction { // This is the action for the button which starts up the recognition loop again if it has been shut down.
    if(![OEPocketsphinxController sharedInstance].isListening) {
        [[OEPocketsphinxController sharedInstance] startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&quot;AcousticModelSpanish&quot;] languageModelIsJSGF:FALSE]; // Start speech recognition if we aren&#039;t already listening.
    }
    self.startButton.hidden = TRUE;
    self.stopButton.hidden = FALSE;
    self.suspendListeningButton.hidden = FALSE;
    self.resumeListeningButton.hidden = TRUE;
}

#pragma mark -
#pragma mark Example for reading out Pocketsphinx and Flite audio levels without locking the UI by using an NSTimer

// What follows are not OpenEars methods, just an approach for level reading
// that I&#039;ve included with this sample app. My example implementation does make use of two OpenEars
// methods:	the pocketsphinxInputLevel method of OEPocketsphinxController and the fliteOutputLevel
// method of OEFliteController.
//
// The example is meant to show one way that you can read those levels continuously without locking the UI,
// by using an NSTimer, but the OpenEars level-reading methods
// themselves do not include multithreading code since I believe that you will want to design your own
// code approaches for level display that are tightly-integrated with your interaction design and the
// graphics API you choose.
//
// Please note that if you use my sample approach, you should pay attention to the way that the timer is always stopped in
// dealloc. This should prevent you from having any difficulties with deallocating a class due to a running NSTimer process.

- (void) startDisplayingLevels { // Start displaying the levels using a timer
    [self stopDisplayingLevels]; // We never want more than one timer valid so we&#039;ll stop any running timers first.
    self.uiUpdateTimer = [NSTimer scheduledTimerWithTimeInterval:1.0/kLevelUpdatesPerSecond target:self selector:@selector(updateLevelsUI) userInfo:nil repeats:YES];
}

- (void) stopDisplayingLevels { // Stop displaying the levels by stopping the timer if it&#039;s running.
    if(self.uiUpdateTimer &amp;&amp; [self.uiUpdateTimer isValid]) { // If there is a running timer, we&#039;ll stop it here.
        [self.uiUpdateTimer invalidate];
        self.uiUpdateTimer = nil;
    }
}

- (void) updateLevelsUI { // And here is how we obtain the levels.  This method includes the actual OpenEars methods and uses their results to update the UI of this view controller.
    
    self.pocketsphinxDbLabel.text = [NSString stringWithFormat:@&quot;Pocketsphinx Input level:%f&quot;,[[OEPocketsphinxController sharedInstance] pocketsphinxInputLevel]];  //pocketsphinxInputLevel is an OpenEars method of the class OEPocketsphinxController.
    
    if(self.fliteController.speechInProgress) {
        self.fliteDbLabel.text = [NSString stringWithFormat:@&quot;Flite Output level: %f&quot;,[self.fliteController fliteOutputLevel]]; // fliteOutputLevel is an OpenEars method of the class OEFliteController.
    }
}

@end
</code></pre>
<p>Definitely, is not suppressing at all. Here the result in the log:</p>
<p><strong>2015-01-07 13:01:28.314 OpenEarsSampleApp[842:60b] Starting OpenEars logging for OpenEars version 2.01 on 32-bit device (or build): iPhone running iOS version: 7.000000<br>
2015-01-07 13:01:28.318 OpenEarsSampleApp[842:60b] Creating shared instance of OEPocketsphinxController<br>
2015-01-07 13:01:28.380 OpenEarsSampleApp[842:60b] Starting dynamic language model generation<br>
## Vocab generated by v2 of the CMU-Cambridge Statistcal<br>
## Language Modeling toolkit.<br>
##<br>
## Includes 12 words ##<br>
wfreq2vocab : Done.<br>
text2idngram<br>
Vocab                  : /var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/Library/Caches/FirstOpenEarsDynamicLanguageModel.vocab<br>
Output idngram         : /var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/Library/Caches/FirstOpenEarsDynamicLanguageModel.idngram<br>
N-gram buffer size     : 10<br>
Hash table size        : 5000<br>
Temp directory         : /var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/Library/Caches/cmuclmtk-jsQ2W3<br>
Max open files         : 20<br>
FOF size               : 10<br>
n                      : 3<br>
Initialising hash table&#8230;<br>
Reading vocabulary&#8230;<br>
Allocating memory for the n-gram buffer&#8230;<br>
Reading text into the n-gram buffer&#8230;<br>
20,000 n-grams processed for each &#8220;.&#8221;, 1,000,000 for each line.</strong></p>
<p>Sorting n-grams&#8230;<br>
Writing sorted n-grams to temporary file /var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/Library/Caches/cmuclmtk-jsQ2W3/1<br>
Merging 1 temporary files&#8230;</p>
<p>2-grams occurring:	N times		&gt; N times	Sug. -spec_num value<br>
      0						     21		     31<br>
      1				     20		      1		     11<br>
      2				      0		      1		     11<br>
      3				      0		      1		     11<br>
      4				      0		      1		     11<br>
      5				      0		      1		     11<br>
      6				      0		      1		     11<br>
      7				      0		      1		     11<br>
      8				      0		      1		     11<br>
      9				      0		      1		     11<br>
     10				      1		      0		     10</p>
<p>3-grams occurring:	N times		&gt; N times	Sug. -spec_num value<br>
      0						     30		     40<br>
      1				     30		      0		     10<br>
      2				      0		      0		     10<br>
      3				      0		      0		     10<br>
      4				      0		      0		     10<br>
      5				      0		      0		     10<br>
      6				      0		      0		     10<br>
      7				      0		      0		     10<br>
      8				      0		      0		     10<br>
      9				      0		      0		     10<br>
     10				      0		      0		     10<br>
text2idngram : Done.</p>
<p>read_wlist_into_siht: a list of 12 words was read from &#8220;/var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/Library/Caches/FirstOpenEarsDynamicLanguageModel.vocab&#8221;.<br>
read_wlist_into_array: a list of 12 words was read from &#8220;/var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/Library/Caches/FirstOpenEarsDynamicLanguageModel.vocab&#8221;.<br>
Unigram was renormalized to absorb a mass of 0.5<br>
prob[UNK] = 1e-99<br>
ARPA-style 3-gram will be written to /var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/Library/Caches/FirstOpenEarsDynamicLanguageModel.arpa<br>
idngram2lm : Done.<br>
INFO: cmd_ln.c(702): Parsing command line:<br>
sphinx_lm_convert \<br>
	-i /var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/Library/Caches/FirstOpenEarsDynamicLanguageModel.arpa \<br>
	-o /var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/Library/Caches/FirstOpenEarsDynamicLanguageModel.DMP \<br>
	-debug 10 </p>
<p>Current configuration:<br>
[NAME]		[DEFLT]	[VALUE]<br>
-case<br>
-debug			10<br>
-help		no	no<br>
-i			/var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/Library/Caches/FirstOpenEarsDynamicLanguageModel.arpa<br>
-ienc<br>
-ifmt<br>
-logbase	1.0001	1.000100e+00<br>
-mmap		no	no<br>
-o			/var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/Library/Caches/FirstOpenEarsDynamicLanguageModel.DMP<br>
-oenc		utf8	utf8<br>
-ofmt			</p>
<p>INFO: ngram_model_arpa.c(504): ngrams 1=12, 2=20, 3=10<br>
INFO: ngram_model_arpa.c(137): Reading unigrams<br>
INFO: ngram_model_arpa.c(543):       12 = #unigrams created<br>
INFO: ngram_model_arpa.c(197): Reading bigrams<br>
INFO: ngram_model_arpa.c(561):       20 = #bigrams created<br>
INFO: ngram_model_arpa.c(562):        3 = #prob2 entries<br>
INFO: ngram_model_arpa.c(570):        3 = #bo_wt2 entries<br>
INFO: ngram_model_arpa.c(294): Reading trigrams<br>
INFO: ngram_model_arpa.c(583):       10 = #trigrams created<br>
INFO: ngram_model_arpa.c(584):        2 = #prob3 entries<br>
INFO: ngram_model_dmp.c(518): Building DMP model&#8230;<br>
INFO: ngram_model_dmp.c(548):       12 = #unigrams created<br>
INFO: ngram_model_dmp.c(649):       20 = #bigrams created<br>
INFO: ngram_model_dmp.c(650):        3 = #prob2 entries<br>
INFO: ngram_model_dmp.c(657):        3 = #bo_wt2 entries<br>
INFO: ngram_model_dmp.c(661):       10 = #trigrams created<br>
INFO: ngram_model_dmp.c(662):        2 = #prob3 entries<br>
2015-01-07 13:01:28.439 OpenEarsSampleApp[842:60b] Done creating language model with CMUCLMTK in 0.058217 seconds.<br>
2015-01-07 13:01:28.477 OpenEarsSampleApp[842:60b] The word ADIOS was not found in the dictionary /var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/OpenEarsSampleApp.app/AcousticModelSpanish.bundle/LanguageModelGeneratorLookupList.text/LanguageModelGeneratorLookupList.text.<br>
2015-01-07 13:01:28.479 OpenEarsSampleApp[842:60b] Now using the fallback method to look up the word ADIOS<br>
2015-01-07 13:01:28.480 OpenEarsSampleApp[842:60b] If this is happening more frequently than you would expect, the most likely cause for it is since you are using the Spanish phonetic lookup dictionary is that your words are not in Spanish or aren&#8217;t dictionary words, or that you are submitting the words in lowercase when they need to be entirely written in uppercase.<br>
2015-01-07 13:01:28.493 OpenEarsSampleApp[842:60b] The word HORCHATA was not found in the dictionary /var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/OpenEarsSampleApp.app/AcousticModelSpanish.bundle/LanguageModelGeneratorLookupList.text/LanguageModelGeneratorLookupList.text.<br>
2015-01-07 13:01:28.495 OpenEarsSampleApp[842:60b] Now using the fallback method to look up the word HORCHATA<br>
2015-01-07 13:01:28.496 OpenEarsSampleApp[842:60b] If this is happening more frequently than you would expect, the most likely cause for it is since you are using the Spanish phonetic lookup dictionary is that your words are not in Spanish or aren&#8217;t dictionary words, or that you are submitting the words in lowercase when they need to be entirely written in uppercase.<br>
2015-01-07 13:01:28.503 OpenEarsSampleApp[842:60b] The word LECHUGA was not found in the dictionary /var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/OpenEarsSampleApp.app/AcousticModelSpanish.bundle/LanguageModelGeneratorLookupList.text/LanguageModelGeneratorLookupList.text.<br>
2015-01-07 13:01:28.505 OpenEarsSampleApp[842:60b] Now using the fallback method to look up the word LECHUGA<br>
2015-01-07 13:01:28.506 OpenEarsSampleApp[842:60b] If this is happening more frequently than you would expect, the most likely cause for it is since you are using the Spanish phonetic lookup dictionary is that your words are not in Spanish or aren&#8217;t dictionary words, or that you are submitting the words in lowercase when they need to be entirely written in uppercase.<br>
2015-01-07 13:01:28.513 OpenEarsSampleApp[842:60b] The word MAINZ was not found in the dictionary /var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/OpenEarsSampleApp.app/AcousticModelSpanish.bundle/LanguageModelGeneratorLookupList.text/LanguageModelGeneratorLookupList.text.<br>
2015-01-07 13:01:28.514 OpenEarsSampleApp[842:60b] Now using the fallback method to look up the word MAINZ<br>
2015-01-07 13:01:28.516 OpenEarsSampleApp[842:60b] If this is happening more frequently than you would expect, the most likely cause for it is since you are using the Spanish phonetic lookup dictionary is that your words are not in Spanish or aren&#8217;t dictionary words, or that you are submitting the words in lowercase when they need to be entirely written in uppercase.<br>
2015-01-07 13:01:28.520 OpenEarsSampleApp[842:60b] I&#8217;m done running performDictionaryLookup and it took 0.053911 seconds<br>
2015-01-07 13:01:28.526 OpenEarsSampleApp[842:60b] I&#8217;m done running dynamic language model generation and it took 0.200190 seconds<br>
2015-01-07 13:01:28.532 OpenEarsSampleApp[842:60b] Starting dynamic language model generation<br>
## Vocab generated by v2 of the CMU-Cambridge Statistcal<br>
## Language Modeling toolkit.<br>
##<br>
## Includes 12 words ##<br>
wfreq2vocab : Done.<br>
text2idngram<br>
Vocab                  : /var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/Library/Caches/SecondOpenEarsDynamicLanguageModel.vocab<br>
Output idngram         : /var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/Library/Caches/SecondOpenEarsDynamicLanguageModel.idngram<br>
N-gram buffer size     : 10<br>
Hash table size        : 5000<br>
Temp directory         : /var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/Library/Caches/cmuclmtk-no6FQp<br>
Max open files         : 20<br>
FOF size               : 10<br>
n                      : 3<br>
Initialising hash table&#8230;<br>
Reading vocabulary&#8230;<br>
Allocating memory for the n-gram buffer&#8230;<br>
Reading text into the n-gram buffer&#8230;<br>
20,000 n-grams processed for each &#8220;.&#8221;, 1,000,000 for each line.</p>
<p>Sorting n-grams&#8230;<br>
Writing sorted n-grams to temporary file /var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/Library/Caches/cmuclmtk-no6FQp/1<br>
Merging 1 temporary files&#8230;</p>
<p>2-grams occurring:	N times		&gt; N times	Sug. -spec_num value<br>
      0						     21		     31<br>
      1				     20		      1		     11<br>
      2				      0		      1		     11<br>
      3				      0		      1		     11<br>
      4				      0		      1		     11<br>
      5				      0		      1		     11<br>
      6				      0		      1		     11<br>
      7				      0		      1		     11<br>
      8				      0		      1		     11<br>
      9				      0		      1		     11<br>
     10				      1		      0		     10</p>
<p>3-grams occurring:	N times		&gt; N times	Sug. -spec_num value<br>
      0						     30		     40<br>
      1				     30		      0		     10<br>
      2				      0		      0		     10<br>
      3				      0		      0		     10<br>
      4				      0		      0		     10<br>
      5				      0		      0		     10<br>
      6				      0		      0		     10<br>
      7				      0		      0		     10<br>
      8				      0		      0		     10<br>
      9				      0		      0		     10<br>
     10				      0		      0		     10<br>
text2idngram : Done.</p>
<p>read_wlist_into_siht: a list of 12 words was read from &#8220;/var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/Library/Caches/SecondOpenEarsDynamicLanguageModel.vocab&#8221;.<br>
read_wlist_into_array: a list of 12 words was read from &#8220;/var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/Library/Caches/SecondOpenEarsDynamicLanguageModel.vocab&#8221;.<br>
Unigram was renormalized to absorb a mass of 0.5<br>
prob[UNK] = 1e-99<br>
ARPA-style 3-gram will be written to /var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/Library/Caches/SecondOpenEarsDynamicLanguageModel.arpa<br>
idngram2lm : Done.<br>
INFO: cmd_ln.c(702): Parsing command line:<br>
sphinx_lm_convert \<br>
	-i /var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/Library/Caches/SecondOpenEarsDynamicLanguageModel.arpa \<br>
	-o /var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/Library/Caches/SecondOpenEarsDynamicLanguageModel.DMP \<br>
	-debug 10 </p>
<p>Current configuration:<br>
[NAME]		[DEFLT]	[VALUE]<br>
-case<br>
-debug			10<br>
-help		no	no<br>
-i			/var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/Library/Caches/SecondOpenEarsDynamicLanguageModel.arpa<br>
-ienc<br>
-ifmt<br>
-logbase	1.0001	1.000100e+00<br>
-mmap		no	no<br>
-o			/var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/Library/Caches/SecondOpenEarsDynamicLanguageModel.DMP<br>
-oenc		utf8	utf8<br>
-ofmt			</p>
<p>INFO: ngram_model_arpa.c(504): ngrams 1=12, 2=20, 3=10<br>
INFO: ngram_model_arpa.c(137): Reading unigrams<br>
INFO: ngram_model_arpa.c(543):       12 = #unigrams created<br>
INFO: ngram_model_arpa.c(197): Reading bigrams<br>
INFO: ngram_model_arpa.c(561):       20 = #bigrams created<br>
INFO: ngram_model_arpa.c(562):        3 = #prob2 entries<br>
INFO: ngram_model_arpa.c(570):        3 = #bo_wt2 entries<br>
INFO: ngram_model_arpa.c(294): Reading trigrams<br>
INFO: ngram_model_arpa.c(583):       10 = #trigrams created<br>
INFO: ngram_model_arpa.c(584):        2 = #prob3 entries<br>
INFO: ngram_model_dmp.c(518): Building DMP model&#8230;<br>
INFO: ngram_model_dmp.c(548):       12 = #unigrams created<br>
INFO: ngram_model_dmp.c(649):       20 = #bigrams created<br>
INFO: ngram_model_dmp.c(650):        3 = #prob2 entries<br>
INFO: ngram_model_dmp.c(657):        3 = #bo_wt2 entries<br>
INFO: ngram_model_dmp.c(661):       10 = #trigrams created<br>
INFO: ngram_model_dmp.c(662):        2 = #prob3 entries<br>
2015-01-07 13:01:28.583 OpenEarsSampleApp[842:60b] Done creating language model with CMUCLMTK in 0.049580 seconds.<br>
2015-01-07 13:01:28.621 OpenEarsSampleApp[842:60b] The word ADIOS was not found in the dictionary /var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/OpenEarsSampleApp.app/AcousticModelSpanish.bundle/LanguageModelGeneratorLookupList.text/LanguageModelGeneratorLookupList.text.<br>
2015-01-07 13:01:28.622 OpenEarsSampleApp[842:60b] Now using the fallback method to look up the word ADIOS<br>
2015-01-07 13:01:28.624 OpenEarsSampleApp[842:60b] If this is happening more frequently than you would expect, the most likely cause for it is since you are using the Spanish phonetic lookup dictionary is that your words are not in Spanish or aren&#8217;t dictionary words, or that you are submitting the words in lowercase when they need to be entirely written in uppercase.<br>
2015-01-07 13:01:28.636 OpenEarsSampleApp[842:60b] The word HORCHATA was not found in the dictionary /var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/OpenEarsSampleApp.app/AcousticModelSpanish.bundle/LanguageModelGeneratorLookupList.text/LanguageModelGeneratorLookupList.text.<br>
2015-01-07 13:01:28.638 OpenEarsSampleApp[842:60b] Now using the fallback method to look up the word HORCHATA<br>
2015-01-07 13:01:28.639 OpenEarsSampleApp[842:60b] If this is happening more frequently than you would expect, the most likely cause for it is since you are using the Spanish phonetic lookup dictionary is that your words are not in Spanish or aren&#8217;t dictionary words, or that you are submitting the words in lowercase when they need to be entirely written in uppercase.<br>
2015-01-07 13:01:28.646 OpenEarsSampleApp[842:60b] The word LECHUGA was not found in the dictionary /var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/OpenEarsSampleApp.app/AcousticModelSpanish.bundle/LanguageModelGeneratorLookupList.text/LanguageModelGeneratorLookupList.text.<br>
2015-01-07 13:01:28.648 OpenEarsSampleApp[842:60b] Now using the fallback method to look up the word LECHUGA<br>
2015-01-07 13:01:28.649 OpenEarsSampleApp[842:60b] If this is happening more frequently than you would expect, the most likely cause for it is since you are using the Spanish phonetic lookup dictionary is that your words are not in Spanish or aren&#8217;t dictionary words, or that you are submitting the words in lowercase when they need to be entirely written in uppercase.<br>
2015-01-07 13:01:28.657 OpenEarsSampleApp[842:60b] The word MAINZ was not found in the dictionary /var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/OpenEarsSampleApp.app/AcousticModelSpanish.bundle/LanguageModelGeneratorLookupList.text/LanguageModelGeneratorLookupList.text.<br>
2015-01-07 13:01:28.658 OpenEarsSampleApp[842:60b] Now using the fallback method to look up the word MAINZ<br>
2015-01-07 13:01:28.659 OpenEarsSampleApp[842:60b] If this is happening more frequently than you would expect, the most likely cause for it is since you are using the Spanish phonetic lookup dictionary is that your words are not in Spanish or aren&#8217;t dictionary words, or that you are submitting the words in lowercase when they need to be entirely written in uppercase.<br>
2015-01-07 13:01:28.664 OpenEarsSampleApp[842:60b] I&#8217;m done running performDictionaryLookup and it took 0.053968 seconds<br>
2015-01-07 13:01:28.670 OpenEarsSampleApp[842:60b] I&#8217;m done running dynamic language model generation and it took 0.141725 seconds<br>
2015-01-07 13:01:28.672 OpenEarsSampleApp[842:60b] </p>
<p>Welcome to the OpenEars sample project. This project understands the words:<br>
BACKWARD,<br>
CHANGE,<br>
FORWARD,<br>
GO,<br>
LEFT,<br>
MODEL,<br>
RIGHT,<br>
TURN,<br>
and if you say &#8220;CHANGE MODEL&#8221; it will switch to its dynamically-generated model which understands the words:<br>
CHANGE,<br>
MODEL,<br>
MONDAY,<br>
TUESDAY,<br>
WEDNESDAY,<br>
THURSDAY,<br>
FRIDAY,<br>
SATURDAY,<br>
SUNDAY,<br>
QUIDNUNC<br>
2015-01-07 13:01:28.674 OpenEarsSampleApp[842:60b] Attempting to start listening session from startListeningWithLanguageModelAtPath:<br>
2015-01-07 13:01:28.678 OpenEarsSampleApp[842:60b] User gave mic permission for this app.<br>
2015-01-07 13:01:28.680 OpenEarsSampleApp[842:60b] Valid setSecondsOfSilence value of 0.500000 will be used.<br>
2015-01-07 13:01:28.681 OpenEarsSampleApp[842:60b] Successfully started listening session from startListeningWithLanguageModelAtPath:<br>
2015-01-07 13:01:28.682 OpenEarsSampleApp[842:1803] Starting listening.<br>
2015-01-07 13:01:28.683 OpenEarsSampleApp[842:1803] about to set up audio session<br>
2015-01-07 13:01:28.718 OpenEarsSampleApp[842:3b03] Audio route has changed for the following reason:<br>
2015-01-07 13:01:28.723 OpenEarsSampleApp[842:3b03] There was a category change. The new category is AVAudioSessionCategoryPlayAndRecord<br>
2015-01-07 13:01:28.732 OpenEarsSampleApp[842:3b03] This is not a case in which OpenEars notifies of a route change. At the close of this function, the new audio route is &#8212;SpeakerMicrophoneBuiltIn&#8212;. The previous route before changing to this route was &lt;AVAudioSessionRouteDescription: 0x1667dc20,<br>
inputs = (null);<br>
outputs = (<br>
    &#8220;&lt;AVAudioSessionPortDescription: 0x1667d800, type = Speaker; name = Altavoz; UID = Speaker; selectedDataSource = (null)&gt;&#8221;<br>
)&gt;.<br>
2015-01-07 13:01:29.142 OpenEarsSampleApp[842:1803] done starting audio unit<br>
INFO: cmd_ln.c(702): Parsing command line:<br>
\<br>
	-lm /var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/Library/Caches/FirstOpenEarsDynamicLanguageModel.DMP \<br>
	-vad_prespeech 10 \<br>
	-vad_postspeech 50 \<br>
	-vad_threshold 3.500000 \<br>
	-remove_noise yes \<br>
	-remove_silence yes \<br>
	-bestpath yes \<br>
	-lw 6.500000 \<br>
	-dict /var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/Library/Caches/FirstOpenEarsDynamicLanguageModel.dic \<br>
	-hmm /var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/OpenEarsSampleApp.app/AcousticModelSpanish.bundle </p>
<p>Current configuration:<br>
[NAME]		[DEFLT]		[VALUE]<br>
-agc		none		none<br>
-agcthresh	2.0		2.000000e+00<br>
-allphone<br>
-allphone_ci	no		no<br>
-alpha		0.97		9.700000e-01<br>
-argfile<br>
-ascale		20.0		2.000000e+01<br>
-aw		1		1<br>
-backtrace	no		no<br>
-beam		1e-48		1.000000e-48<br>
-bestpath	yes		yes<br>
-bestpathlw	9.5		9.500000e+00<br>
-bghist		no		no<br>
-ceplen		13		13<br>
-cmn		current		current<br>
-cmninit	8.0		8.0<br>
-compallsen	no		no<br>
-debug				0<br>
-dict				/var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/Library/Caches/FirstOpenEarsDynamicLanguageModel.dic<br>
-dictcase	no		no<br>
-dither		no		no<br>
-doublebw	no		no<br>
-ds		1		1<br>
-fdict<br>
-feat		1s_c_d_dd	1s_c_d_dd<br>
-featparams<br>
-fillprob	1e-8		1.000000e-08<br>
-frate		100		100<br>
-fsg<br>
-fsgusealtpron	yes		yes<br>
-fsgusefiller	yes		yes<br>
-fwdflat	yes		yes<br>
-fwdflatbeam	1e-64		1.000000e-64<br>
-fwdflatefwid	4		4<br>
-fwdflatlw	8.5		8.500000e+00<br>
-fwdflatsfwin	25		25<br>
-fwdflatwbeam	7e-29		7.000000e-29<br>
-fwdtree	yes		yes<br>
-hmm				/var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/OpenEarsSampleApp.app/AcousticModelSpanish.bundle<br>
-input_endian	little		little<br>
-jsgf<br>
-kdmaxbbi	-1		-1<br>
-kdmaxdepth	0		0<br>
-kdtree<br>
-keyphrase<br>
-kws<br>
-kws_plp	1e-1		1.000000e-01<br>
-kws_threshold	1		1.000000e+00<br>
-latsize	5000		5000<br>
-lda<br>
-ldadim		0		0<br>
-lextreedump	0		0<br>
-lifter		0		0<br>
-lm				/var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/Library/Caches/FirstOpenEarsDynamicLanguageModel.DMP<br>
-lmctl<br>
-lmname<br>
-logbase	1.0001		1.000100e+00<br>
-logfn<br>
-logspec	no		no<br>
-lowerf		133.33334	1.333333e+02<br>
-lpbeam		1e-40		1.000000e-40<br>
-lponlybeam	7e-29		7.000000e-29<br>
-lw		6.5		6.500000e+00<br>
-maxhmmpf	10000		10000<br>
-maxnewoov	20		20<br>
-maxwpf		-1		-1<br>
-mdef<br>
-mean<br>
-mfclogdir<br>
-min_endfr	0		0<br>
-mixw<br>
-mixwfloor	0.0000001	1.000000e-07<br>
-mllr<br>
-mmap		yes		yes<br>
-ncep		13		13<br>
-nfft		512		512<br>
-nfilt		40		40<br>
-nwpen		1.0		1.000000e+00<br>
-pbeam		1e-48		1.000000e-48<br>
-pip		1.0		1.000000e+00<br>
-pl_beam	1e-10		1.000000e-10<br>
-pl_pbeam	1e-5		1.000000e-05<br>
-pl_window	0		0<br>
-rawlogdir<br>
-remove_dc	no		no<br>
-remove_noise	yes		yes<br>
-remove_silence	yes		yes<br>
-round_filters	yes		yes<br>
-samprate	16000		1.600000e+04<br>
-seed		-1		-1<br>
-sendump<br>
-senlogdir<br>
-senmgau<br>
-silprob	0.005		5.000000e-03<br>
-smoothspec	no		no<br>
-svspec<br>
-tmat<br>
-tmatfloor	0.0001		1.000000e-04<br>
-topn		4		4<br>
-topn_beam	0		0<br>
-toprule<br>
-transform	legacy		legacy<br>
-unit_area	yes		yes<br>
-upperf		6855.4976	6.855498e+03<br>
-usewdphones	no		no<br>
-uw		1.0		1.000000e+00<br>
-vad_postspeech	50		50<br>
-vad_prespeech	10		10<br>
-vad_threshold	2.0		3.500000e+00<br>
-var<br>
-varfloor	0.0001		1.000000e-04<br>
-varnorm	no		no<br>
-verbose	no		no<br>
-warp_params<br>
-warp_type	inverse_linear	inverse_linear<br>
-wbeam		7e-29		7.000000e-29<br>
-wip		0.65		6.500000e-01<br>
-wlen		0.025625	2.562500e-02</p>
<p>INFO: cmd_ln.c(702): Parsing command line:<br>
\<br>
	-feat s3_1x39 </p>
<p>Current configuration:<br>
[NAME]		[DEFLT]		[VALUE]<br>
-agc		none		none<br>
-agcthresh	2.0		2.000000e+00<br>
-alpha		0.97		9.700000e-01<br>
-ceplen		13		13<br>
-cmn		current		current<br>
-cmninit	8.0		8.0<br>
-dither		no		no<br>
-doublebw	no		no<br>
-feat		1s_c_d_dd	s3_1x39<br>
-frate		100		100<br>
-input_endian	little		little<br>
-lda<br>
-ldadim		0		0<br>
-lifter		0		0<br>
-logspec	no		no<br>
-lowerf		133.33334	1.333333e+02<br>
-ncep		13		13<br>
-nfft		512		512<br>
-nfilt		40		40<br>
-remove_dc	no		no<br>
-remove_noise	yes		yes<br>
-remove_silence	yes		yes<br>
-round_filters	yes		yes<br>
-samprate	16000		1.600000e+04<br>
-seed		-1		-1<br>
-smoothspec	no		no<br>
-svspec<br>
-transform	legacy		legacy<br>
-unit_area	yes		yes<br>
-upperf		6855.4976	6.855498e+03<br>
-vad_postspeech	50		50<br>
-vad_prespeech	10		10<br>
-vad_threshold	2.0		3.500000e+00<br>
-varnorm	no		no<br>
-verbose	no		no<br>
-warp_params<br>
-warp_type	inverse_linear	inverse_linear<br>
-wlen		0.025625	2.562500e-02</p>
<p>INFO: acmod.c(252): Parsed model-specific feature parameters from /var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/OpenEarsSampleApp.app/AcousticModelSpanish.bundle/feat.params<br>
INFO: feat.c(715): Initializing feature stream to type: &#8216;s3_1x39&#8242;, ceplen=13, CMN=&#8217;current&#8217;, VARNORM=&#8217;no&#8217;, AGC=&#8217;none&#8217;<br>
INFO: cmn.c(143): mean[0]= 12.00, mean[1..12]= 0.0<br>
INFO: mdef.c(518): Reading model definition: /var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/OpenEarsSampleApp.app/AcousticModelSpanish.bundle/mdef<br>
INFO: bin_mdef.c(181): Allocating 27954 * 8 bytes (218 KiB) for CD tree<br>
INFO: tmat.c(206): Reading HMM transition probability matrices: /var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/OpenEarsSampleApp.app/AcousticModelSpanish.bundle/transition_matrices<br>
INFO: acmod.c(124): Attempting to use SCHMM computation module<br>
INFO: ms_gauden.c(198): Reading mixture gaussian parameter: /var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/OpenEarsSampleApp.app/AcousticModelSpanish.bundle/means<br>
INFO: ms_gauden.c(292): 2630 codebook, 1 feature, size:<br>
INFO: ms_gauden.c(294):  16&#215;39<br>
INFO: ms_gauden.c(198): Reading mixture gaussian parameter: /var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/OpenEarsSampleApp.app/AcousticModelSpanish.bundle/variances<br>
INFO: ms_gauden.c(292): 2630 codebook, 1 feature, size:<br>
INFO: ms_gauden.c(294):  16&#215;39<br>
INFO: ms_gauden.c(354): 16 variance values floored<br>
INFO: acmod.c(126): Attempting to use PTHMM computation module<br>
INFO: ms_gauden.c(198): Reading mixture gaussian parameter: /var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/OpenEarsSampleApp.app/AcousticModelSpanish.bundle/means<br>
INFO: ms_gauden.c(292): 2630 codebook, 1 feature, size:<br>
INFO: ms_gauden.c(294):  16&#215;39<br>
INFO: ms_gauden.c(198): Reading mixture gaussian parameter: /var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/OpenEarsSampleApp.app/AcousticModelSpanish.bundle/variances<br>
INFO: ms_gauden.c(292): 2630 codebook, 1 feature, size:<br>
INFO: ms_gauden.c(294):  16&#215;39<br>
INFO: ms_gauden.c(354): 16 variance values floored<br>
INFO: ptm_mgau.c(792): Number of codebooks exceeds 256: 2630<br>
INFO: acmod.c(128): Falling back to general multi-stream GMM computation<br>
INFO: ms_gauden.c(198): Reading mixture gaussian parameter: /var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/OpenEarsSampleApp.app/AcousticModelSpanish.bundle/means<br>
INFO: ms_gauden.c(292): 2630 codebook, 1 feature, size:<br>
INFO: ms_gauden.c(294):  16&#215;39<br>
INFO: ms_gauden.c(198): Reading mixture gaussian parameter: /var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/OpenEarsSampleApp.app/AcousticModelSpanish.bundle/variances<br>
INFO: ms_gauden.c(292): 2630 codebook, 1 feature, size:<br>
INFO: ms_gauden.c(294):  16&#215;39<br>
INFO: ms_gauden.c(354): 16 variance values floored<br>
INFO: ms_senone.c(149): Reading senone mixture weights: /var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/OpenEarsSampleApp.app/AcousticModelSpanish.bundle/mixture_weights<br>
INFO: ms_senone.c(200): Truncating senone logs3(pdf) values by 10 bits<br>
INFO: ms_senone.c(207): Not transposing mixture weights in memory<br>
INFO: ms_senone.c(268): Read mixture weights for 2630 senones: 1 features x 16 codewords<br>
INFO: ms_senone.c(320): Mapping senones to individual codebooks<br>
INFO: ms_mgau.c(141): The value of topn: 4<br>
INFO: dict.c(320): Allocating 4106 * 20 bytes (80 KiB) for word entries<br>
INFO: dict.c(333): Reading main dictionary: /var/mobile/Applications/3258C065-2A15-463F-A98B-D502DF01812B/Library/Caches/FirstOpenEarsDynamicLanguageModel.dic<br>
INFO: dict.c(213): Allocated 0 KiB for strings, 0 KiB for phones<br>
INFO: dict.c(336): 10 words read<br>
INFO: dict2pid.c(396): Building PID tables for dictionary<br>
INFO: dict2pid.c(406): Allocating 26^3 * 2 bytes (34 KiB) for word-initial triphones<br>
INFO: dict2pid.c(132): Allocated 8216 bytes (8 KiB) for word-final triphones<br>
INFO: dict2pid.c(196): Allocated 8216 bytes (8 KiB) for single-phone word triphones<br>
INFO: ngram_model_arpa.c(79): No \data\ mark in LM file<br>
INFO: ngram_model_dmp.c(166): Will use memory-mapped I/O for LM file<br>
INFO: ngram_model_dmp.c(220): ngrams 1=12, 2=20, 3=10<br>
INFO: ngram_model_dmp.c(266):       12 = LM.unigrams(+trailer) read<br>
INFO: ngram_model_dmp.c(312):       20 = LM.bigrams(+trailer) read<br>
INFO: ngram_model_dmp.c(338):       10 = LM.trigrams read<br>
INFO: ngram_model_dmp.c(363):        3 = LM.prob2 entries read<br>
INFO: ngram_model_dmp.c(383):        3 = LM.bo_wt2 entries read<br>
INFO: ngram_model_dmp.c(403):        2 = LM.prob3 entries read<br>
INFO: ngram_model_dmp.c(431):        1 = LM.tseg_base entries read<br>
INFO: ngram_model_dmp.c(487):       12 = ascii word strings read<br>
INFO: ngram_search_fwdtree.c(99): 9 unique initial diphones<br>
INFO: ngram_search_fwdtree.c(148): 0 root, 0 non-root channels, 4 single-phone words<br>
INFO: ngram_search_fwdtree.c(186): Creating search tree<br>
INFO: ngram_search_fwdtree.c(192): before: 0 root, 0 non-root channels, 4 single-phone words<br>
INFO: ngram_search_fwdtree.c(326): after: max nonroot chan increased to 163<br>
INFO: ngram_search_fwdtree.c(339): after: 9 root, 35 non-root channels, 3 single-phone words<br>
INFO: ngram_search_fwdflat.c(157): fwdflat: min_ef_width = 4, max_sf_win = 25<br>
2015-01-07 13:01:30.877 OpenEarsSampleApp[842:1803] Listening.<br>
2015-01-07 13:01:30.879 OpenEarsSampleApp[842:1803] Project has these words or phrases in its dictionary:<br>
ADIOS<br>
BARCELONA<br>
CHORIZO<br>
HOLA<br>
HORCHATA<br>
LECHUGA<br>
MADRID<br>
MAINZ<br>
PARIS<br>
ROMA<br>
2015-01-07 13:01:30.880 OpenEarsSampleApp[842:1803] Recognition loop has started<br>
2015-01-07 13:01:30.905 OpenEarsSampleApp[842:60b] Local callback: Pocketsphinx is now listening.<br>
2015-01-07 13:01:30.908 OpenEarsSampleApp[842:60b] Local callback: Pocketsphinx started.<br>
2015-01-07 13:01:31.210 OpenEarsSampleApp[842:1803] Speech detected&#8230;<br>
2015-01-07 13:01:31.212 OpenEarsSampleApp[842:60b] Local callback: Pocketsphinx has detected speech.<br>
2015-01-07 13:01:33.335 OpenEarsSampleApp[842:1803] End of speech detected&#8230;<br>
INFO: cmn_prior.c(131): cmn_prior_update: from &lt;  8.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 &gt;<br>
INFO: cmn_prior.c(149): cmn_prior_update: to   &lt;  9.20  0.70 -0.18 -0.14 -0.30 -0.40 -0.35 -0.32 -0.38 -0.21 -0.22 -0.25 -0.19 &gt;<br>
INFO: ngram_search_fwdtree.c(1550):      277 words recognized (1/fr)<br>
INFO: ngram_search_fwdtree.c(1552):    14886 senones evaluated (66/fr)<br>
INFO: ngram_search_fwdtree.c(1556):     3780 channels searched (16/fr), 505 1st, 2058 last<br>
INFO: ngram_search_fwdtree.c(1559):      304 words for which last channels evaluated (1/fr)<br>
INFO: ngram_search_fwdtree.c(1561):       54 candidate words for entering last phone (0/fr)<br>
INFO: ngram_search_fwdtree.c(1564): fwdtree 0.34 CPU 0.152 xRT<br>
INFO: ngram_search_fwdtree.c(1567): fwdtree 2.44 wall 1.084 xRT<br>
INFO: ngram_search_fwdflat.c(302): Utterance vocabulary contains 2 words<br>
2015-01-07 13:01:33.337 OpenEarsSampleApp[842:60b] Local callback: Pocketsphinx has detected a second of silence, concluding an utterance.<br>
INFO: ngram_search_fwdflat.c(938):      318 words recognized (1/fr)<br>
INFO: ngram_search_fwdflat.c(940):    17608 senones evaluated (78/fr)<br>
INFO: ngram_search_fwdflat.c(942):     5444 channels searched (24/fr)<br>
INFO: ngram_search_fwdflat.c(944):      473 words searched (2/fr)<br>
INFO: ngram_search_fwdflat.c(947):       57 word transitions (0/fr)<br>
INFO: ngram_search_fwdflat.c(950): fwdflat 0.23 CPU 0.103 xRT<br>
INFO: ngram_search_fwdflat.c(953): fwdflat 0.23 wall 0.103 xRT<br>
INFO: ngram_search.c(1215): &lt;/s&gt; not found in last frame, using HOLA.223 instead<br>
INFO: ngram_search.c(1268): lattice start node &lt;s&gt;.0 end node HOLA.2<br>
INFO: ngram_search.c(1294): Eliminated 32 nodes before end node<br>
INFO: ngram_search.c(1399): Lattice has 36 nodes, 1 links<br>
INFO: ps_lattice.c(1368): Normalizer P(O) = alpha(HOLA:2:223) = -478053<br>
INFO: ps_lattice.c(1403): Joint P(O,S) = -478053 P(S|O) = 0<br>
INFO: ngram_search.c(890): bestpath 0.01 CPU 0.002 xRT<br>
INFO: ngram_search.c(893): bestpath 0.00 wall 0.000 xRT<br>
2015-01-07 13:01:33.569 OpenEarsSampleApp[842:1803] Pocketsphinx heard &#8220;HOLA&#8221; with a score of (0) and an utterance ID of 0.<br>
2015-01-07 13:01:33.570 OpenEarsSampleApp[842:60b] Flite sending interrupt speech request.<br>
2015-01-07 13:01:33.572 OpenEarsSampleApp[842:60b] Local callback: The received hypothesis is HOLA with a score of 0 and an ID of 0<br>
2015-01-07 13:01:33.574 OpenEarsSampleApp[842:60b] I&#8217;m running flite<br>
2015-01-07 13:01:33.716 OpenEarsSampleApp[842:60b] I&#8217;m done running flite and it took 0.140953 seconds<br>
2015-01-07 13:01:33.717 OpenEarsSampleApp[842:60b] Flite audio player was nil when referenced so attempting to allocate a new audio player.<br>
2015-01-07 13:01:33.719 OpenEarsSampleApp[842:60b] Loading speech data for Flite concluded successfully.<br>
2015-01-07 13:01:33.770 OpenEarsSampleApp[842:60b] Local callback:  hypothesisArray is (<br>
        {<br>
        Hypothesis = HOLA;<br>
        Score = &#8220;-9037&#8221;;<br>
    }<br>
)<br>
2015-01-07 13:01:33.772 OpenEarsSampleApp[842:60b] Flite sending suspend recognition notification.<br>
2015-01-07 13:01:33.774 OpenEarsSampleApp[842:60b] Local callback: Flite has started speaking<br>
2015-01-07 13:01:33.780 OpenEarsSampleApp[842:60b] Local callback: Pocketsphinx has suspended recognition.<br>
2015-01-07 13:01:34.979 OpenEarsSampleApp[842:60b] AVAudioPlayer did finish playing with success flag of 1<br>
2015-01-07 13:01:35.132 OpenEarsSampleApp[842:60b] Flite sending resume recognition notification.<br>
2015-01-07 13:01:35.635 OpenEarsSampleApp[842:60b] Local callback: Flite has finished speaking<br>
2015-01-07 13:01:35.642 OpenEarsSampleApp[842:60b] Valid setSecondsOfSilence value of 0.500000 will be used.<br>
2015-01-07 13:01:35.643 OpenEarsSampleApp[842:60b] Local callback: Pocketsphinx has resumed recognition.<br>
INFO: cmn_prior.c(131): cmn_prior_update: from &lt;  9.20  0.70 -0.18 -0.14 -0.30 -0.40 -0.35 -0.32 -0.38 -0.21 -0.22 -0.25 -0.19 &gt;<br>
INFO: cmn_prior.c(149): cmn_prior_update: to   &lt;  9.20  0.70 -0.18 -0.14 -0.30 -0.40 -0.35 -0.32 -0.38 -0.21 -0.22 -0.25 -0.19 &gt;<br>
INFO: ngram_search_fwdflat.c(302): Utterance vocabulary contains 0 words<br>
2015-01-07 13:01:37.487 OpenEarsSampleApp[842:1803] Speech detected&#8230;<br>
2015-01-07 13:01:37.489 OpenEarsSampleApp[842:60b] Local callback: Pocketsphinx has detected speech.<br>
2015-01-07 13:01:41.814 OpenEarsSampleApp[842:1803] End of speech detected&#8230;<br>
INFO: cmn_prior.c(131): cmn_prior_update: from &lt;  9.20  0.70 -0.18 -0.14 -0.30 -0.40 -0.35 -0.32 -0.38 -0.21 -0.22 -0.25 -0.19 &gt;<br>
INFO: cmn_prior.c(149): cmn_prior_update: to   &lt;  9.31  0.33 -0.27  0.11 -0.30 -0.45 -0.30 -0.15 -0.25 -0.18 -0.20 -0.17 -0.11 &gt;<br>
INFO: ngram_search_fwdtree.c(1550):     1881 words recognized (4/fr)<br>
INFO: ngram_search_fwdtree.c(1552):    95871 senones evaluated (209/fr)<br>
INFO: ngram_search_fwdtree.c(1556):    29159 channels searched (63/fr), 3690 1st, 15184 last<br>
INFO: ngram_search_fwdtree.c(1559):     2097 words for which last channels evaluated (4/fr)<br>
INFO: ngram_search_fwdtree.c(1561):     1023 candidate words for entering last phone (2/fr)<br>
INFO: ngram_search_fwdtree.c(1564): fwdtree 1.66 CPU 0.362 xRT<br>
INFO: ngram_search_fwdtree.c(1567): fwdtree 6.03 wall 1.316 xRT<br>
INFO: ngram_search_fwdflat.c(302): Utterance vocabulary contains 12 words<br>
2015-01-07 13:01:41.815 OpenEarsSampleApp[842:60b] Local callback: Pocketsphinx has detected a second of silence, concluding an utterance.<br>
INFO: ngram_search_fwdflat.c(938):     1483 words recognized (3/fr)<br>
INFO: ngram_search_fwdflat.c(940):    94703 senones evaluated (207/fr)<br>
INFO: ngram_search_fwdflat.c(942):    37245 channels searched (81/fr)<br>
INFO: ngram_search_fwdflat.c(944):     3256 words searched (7/fr)<br>
INFO: ngram_search_fwdflat.c(947):     1500 word transitions (3/fr)<br>
INFO: ngram_search_fwdflat.c(950): fwdflat 1.24 CPU 0.270 xRT<br>
INFO: ngram_search_fwdflat.c(953): fwdflat 1.22 wall 0.266 xRT<br>
INFO: ngram_search.c(1215): &lt;/s&gt; not found in last frame, using &lt;sil&gt;.456 instead<br>
INFO: ngram_search.c(1268): lattice start node &lt;s&gt;.0 end node &lt;sil&gt;.386<br>
INFO: ngram_search.c(1294): Eliminated 5 nodes before end node<br>
INFO: ngram_search.c(1399): Lattice has 67 nodes, 69 links<br>
INFO: ps_lattice.c(1368): Normalizer P(O) = alpha(&lt;sil&gt;:386:456) = -1095420<br>
INFO: ps_lattice.c(1403): Joint P(O,S) = -1112740 P(S|O) = -17320<br>
INFO: ngram_search.c(890): bestpath 0.00 CPU 0.000 xRT<br>
INFO: ngram_search.c(893): bestpath 0.00 wall 0.000 xRT<br>
2015-01-07 13:01:43.036 OpenEarsSampleApp[842:1803] Pocketsphinx heard &#8220;MAINZ HORCHATA LECHUGA ADIOS&#8221; with a score of (-17320) and an utterance ID of 1.<br>
2015-01-07 13:01:43.037 OpenEarsSampleApp[842:60b] Flite sending interrupt speech request.<br>
2015-01-07 13:01:43.038 OpenEarsSampleApp[842:60b] Local callback: The received hypothesis is MAINZ HORCHATA LECHUGA ADIOS with a score of -17320 and an ID of 1<br>
2015-01-07 13:01:43.041 OpenEarsSampleApp[842:60b] I&#8217;m running flite<br>
2015-01-07 13:01:43.054 OpenEarsSampleApp[842:1803] Speech detected&#8230;<br>
2015-01-07 13:01:43.292 OpenEarsSampleApp[842:60b] I&#8217;m done running flite and it took 0.249377 seconds<br>
2015-01-07 13:01:43.293 OpenEarsSampleApp[842:60b] Flite audio player was nil when referenced so attempting to allocate a new audio player.<br>
2015-01-07 13:01:43.294 OpenEarsSampleApp[842:60b] Loading speech data for Flite concluded successfully.<br>
2015-01-07 13:01:43.337 OpenEarsSampleApp[842:60b] Local callback:  hypothesisArray is (<br>
        {<br>
        Hypothesis = &#8220;MAINZ HORCHATA LECHUGA ADIOS&#8221;;<br>
        Score = &#8220;-20469&#8221;;<br>
    },<br>
        {<br>
        Hypothesis = &#8220;MAINZ HORCHATA LECHUGA PARIS&#8221;;<br>
        Score = &#8220;-20469&#8221;;<br>
    },<br>
        {<br>
        Hypothesis = &#8220;MAINZ HORCHATA LECHUGA MAINZ&#8221;;<br>
        Score = &#8220;-20469&#8221;;<br>
    },<br>
        {<br>
        Hypothesis = &#8220;MAINZ MAINZ HORCHATA LECHUGA ADIOS&#8221;;<br>
        Score = &#8220;-20469&#8221;;<br>
    },<br>
        {<br>
        Hypothesis = &#8220;BARCELONA LECHUGA ADIOS&#8221;;<br>
        Score = &#8220;-20469&#8221;;<br>
    }<br>
)<br>
2015-01-07 13:01:43.339 OpenEarsSampleApp[842:60b] Local callback: Pocketsphinx has detected speech.<br>
2015-01-07 13:01:43.342 OpenEarsSampleApp[842:60b] Flite sending suspend recognition notification.<br>
2015-01-07 13:01:43.344 OpenEarsSampleApp[842:60b] Local callback: Flite has started speaking<br>
2015-01-07 13:01:43.351 OpenEarsSampleApp[842:60b] Local callback: Pocketsphinx has suspended recognition.<br>
2015-01-07 13:01:45.939 OpenEarsSampleApp[842:60b] AVAudioPlayer did finish playing with success flag of 1<br>
2015-01-07 13:01:46.092 OpenEarsSampleApp[842:60b] Flite sending resume recognition notification.<br>
2015-01-07 13:01:46.595 OpenEarsSampleApp[842:60b] Local callback: Flite has finished speaking<br>
2015-01-07 13:01:46.602 OpenEarsSampleApp[842:60b] Valid setSecondsOfSilence value of 0.500000 will be used.<br>
2015-01-07 13:01:46.603 OpenEarsSampleApp[842:60b] Local callback: Pocketsphinx has resumed recognition.<br>
INFO: cmn_prior.c(131): cmn_prior_update: from &lt;  9.31  0.33 -0.27  0.11 -0.30 -0.45 -0.30 -0.15 -0.25 -0.18 -0.20 -0.17 -0.11 &gt;<br>
INFO: cmn_prior.c(149): cmn_prior_update: to   &lt;  9.31  0.35 -0.27  0.13 -0.30 -0.46 -0.30 -0.16 -0.25 -0.19 -0.20 -0.17 -0.12 &gt;<br>
INFO: ngram_search_fwdtree.c(1550):      149 words recognized (3/fr)<br>
INFO: ngram_search_fwdtree.c(1552):     8840 senones evaluated (173/fr)<br>
INFO: ngram_search_fwdtree.c(1556):     2560 channels searched (50/fr), 423 1st, 1352 last<br>
INFO: ngram_search_fwdtree.c(1559):      199 words for which last channels evaluated (3/fr)<br>
INFO: ngram_search_fwdtree.c(1561):       74 candidate words for entering last phone (1/fr)<br>
INFO: ngram_search_fwdtree.c(1564): fwdtree 0.51 CPU 0.994 xRT<br>
INFO: ngram_search_fwdtree.c(1567): fwdtree 3.76 wall 7.378 xRT<br>
INFO: ngram_search_fwdflat.c(302): Utterance vocabulary contains 5 words<br>
INFO: ngram_search_fwdflat.c(938):       55 words recognized (1/fr)<br>
INFO: ngram_search_fwdflat.c(940):     8242 senones evaluated (162/fr)<br>
INFO: ngram_search_fwdflat.c(942):     3030 channels searched (59/fr)<br>
INFO: ngram_search_fwdflat.c(944):      262 words searched (5/fr)<br>
INFO: ngram_search_fwdflat.c(947):      144 word transitions (2/fr)<br>
INFO: ngram_search_fwdflat.c(950): fwdflat 0.12 CPU 0.226 xRT<br>
INFO: ngram_search_fwdflat.c(953): fwdflat 0.12 wall 0.236 xRT<br>
2015-01-07 13:01:48.672 OpenEarsSampleApp[842:1803] Speech detected&#8230;<br>
2015-01-07 13:01:48.673 OpenEarsSampleApp[842:60b] Local callback: Pocketsphinx has detected speech.<br>
INFO: cmn_prior.c(99): cmn_prior_update: from &lt;  9.31  0.35 -0.27  0.13 -0.30 -0.46 -0.30 -0.16 -0.25 -0.19 -0.20 -0.17 -0.12 &gt;<br>
INFO: cmn_prior.c(116): cmn_prior_update: to   &lt;  9.32  0.34 -0.26  0.13 -0.30 -0.46 -0.33 -0.17 -0.25 -0.19 -0.20 -0.17 -0.13 &gt;<br>
2015-01-07 13:01:49.894 OpenEarsSampleApp[842:1803] End of speech detected&#8230;<br>
INFO: cmn_prior.c(131): cmn_prior_update: from &lt;  9.32  0.34 -0.26  0.13 -0.30 -0.46 -0.33 -0.17 -0.25 -0.19 -0.20 -0.17 -0.13 &gt;<br>
INFO: cmn_prior.c(149): cmn_prior_update: to   &lt;  9.41  0.38 -0.27  0.09 -0.32 -0.45 -0.29 -0.18 -0.23 -0.20 -0.20 -0.15 -0.14 &gt;<br>
INFO: ngram_search_fwdtree.c(1550):      440 words recognized (4/fr)<br>
INFO: ngram_search_fwdtree.c(1552):    26366 senones evaluated (214/fr)<br>
INFO: ngram_search_fwdtree.c(1556):     7946 channels searched (64/fr), 1022 1st, 4338 last<br>
INFO: ngram_search_fwdtree.c(1559):      544 words for which last channels evaluated (4/fr)<br>
INFO: ngram_search_fwdtree.c(1561):      279 candidate words for entering last phone (2/fr)<br>
INFO: ngram_search_fwdtree.c(1564): fwdtree 0.53 CPU 0.427 xRT<br>
INFO: ngram_search_fwdtree.c(1567): fwdtree 2.97 wall 2.418 xRT<br>
INFO: ngram_search_fwdflat.c(302): Utterance vocabulary contains 8 words<br>
2015-01-07 13:01:49.896 OpenEarsSampleApp[842:60b] Local callback: Pocketsphinx has detected a second of silence, concluding an utterance.<br>
INFO: ngram_search_fwdflat.c(938):      325 words recognized (3/fr)<br>
INFO: ngram_search_fwdflat.c(940):    25549 senones evaluated (208/fr)<br>
INFO: ngram_search_fwdflat.c(942):     9753 channels searched (79/fr)<br>
INFO: ngram_search_fwdflat.c(944):      826 words searched (6/fr)<br>
INFO: ngram_search_fwdflat.c(947):      472 word transitions (3/fr)<br>
INFO: ngram_search_fwdflat.c(950): fwdflat 0.34 CPU 0.278 xRT<br>
INFO: ngram_search_fwdflat.c(953): fwdflat 0.34 wall 0.276 xRT<br>
INFO: ngram_search.c(1215): &lt;/s&gt; not found in last frame, using HOLA.121 instead<br>
INFO: ngram_search.c(1268): lattice start node &lt;s&gt;.0 end node HOLA.100<br>
INFO: ngram_search.c(1294): Eliminated 1 nodes before end node<br>
INFO: ngram_search.c(1399): Lattice has 31 nodes, 26 links<br>
INFO: ps_lattice.c(1368): Normalizer P(O) = alpha(HOLA:100:121) = -331976<br>
INFO: ps_lattice.c(1403): Joint P(O,S) = -342116 P(S|O) = -10140<br>
INFO: ngram_search.c(890): bestpath 0.01 CPU 0.006 xRT<br>
INFO: ngram_search.c(893): bestpath 0.00 wall 0.000 xRT<br>
2015-01-07 13:01:50.237 OpenEarsSampleApp[842:1803] Pocketsphinx heard &#8220;ADIOS ROMA HOLA&#8221; with a score of (-10140) and an utterance ID of 2.<br>
2015-01-07 13:01:50.238 OpenEarsSampleApp[842:60b] Flite sending interrupt speech request.<br>
2015-01-07 13:01:50.240 OpenEarsSampleApp[842:60b] Local callback: The received hypothesis is ADIOS ROMA HOLA with a score of -10140 and an ID of 2<br>
2015-01-07 13:01:50.243 OpenEarsSampleApp[842:60b] I&#8217;m running flite<br>
2015-01-07 13:01:50.459 OpenEarsSampleApp[842:60b] I&#8217;m done running flite and it took 0.214843 seconds<br>
2015-01-07 13:01:50.460 OpenEarsSampleApp[842:60b] Flite audio player was nil when referenced so attempting to allocate a new audio player.<br>
2015-01-07 13:01:50.461 OpenEarsSampleApp[842:60b] Loading speech data for Flite concluded successfully.<br>
2015-01-07 13:01:50.489 OpenEarsSampleApp[842:60b] Local callback:  hypothesisArray is (<br>
        {<br>
        Hypothesis = &#8220;ADIOS ROMA HOLA&#8221;;<br>
        Score = &#8220;-5782&#8221;;<br>
    },<br>
        {<br>
        Hypothesis = &#8220;MAINZ ROMA HOLA&#8221;;<br>
        Score = &#8220;-5782&#8221;;<br>
    },<br>
        {<br>
        Hypothesis = &#8220;CHORIZO ROMA HOLA&#8221;;<br>
        Score = &#8220;-5782&#8221;;<br>
    },<br>
        {<br>
        Hypothesis = &#8220;ROMA HOLA&#8221;;<br>
        Score = &#8220;-5782&#8221;;<br>
    },<br>
        {<br>
        Hypothesis = &#8220;ADIOS ROMA HOLA&#8221;;<br>
        Score = &#8220;-5782&#8221;;<br>
    }<br>
)<br>
2015-01-07 13:01:50.491 OpenEarsSampleApp[842:60b] Flite sending suspend recognition notification.<br>
2015-01-07 13:01:50.493 OpenEarsSampleApp[842:60b] Local callback: Flite has started speaking<br>
2015-01-07 13:01:50.499 OpenEarsSampleApp[842:60b] Local callback: Pocketsphinx has suspended recognition.<br>
2015-01-07 13:01:52.627 OpenEarsSampleApp[842:60b] AVAudioPlayer did finish playing with success flag of 1<br>
2015-01-07 13:01:52.780 OpenEarsSampleApp[842:60b] Flite sending resume recognition notification.<br>
2015-01-07 13:01:53.283 OpenEarsSampleApp[842:60b] Local callback: Flite has finished speaking<br>
2015-01-07 13:01:53.289 OpenEarsSampleApp[842:60b] Valid setSecondsOfSilence value of 0.500000 will be used.<br>
2015-01-07 13:01:53.290 OpenEarsSampleApp[842:60b] Local callback: Pocketsphinx has resumed recognition.<br>
INFO: cmn_prior.c(131): cmn_prior_update: from &lt;  9.41  0.38 -0.27  0.09 -0.32 -0.45 -0.29 -0.18 -0.23 -0.20 -0.20 -0.15 -0.14 &gt;<br>
INFO: cmn_prior.c(149): cmn_prior_update: to   &lt;  9.41  0.38 -0.27  0.09 -0.32 -0.45 -0.29 -0.18 -0.23 -0.20 -0.20 -0.15 -0.14 &gt;<br>
INFO: ngram_search_fwdflat.c(302): Utterance vocabulary contains 0 words<br>
2015-01-07 13:01:54.077 OpenEarsSampleApp[842:1803] Speech detected&#8230;<br>
2015-01-07 13:01:54.079 OpenEarsSampleApp[842:60b] Local callback: Pocketsphinx has detected speech.<br>
INFO: cmn_prior.c(99): cmn_prior_update: from &lt;  9.41  0.38 -0.27  0.09 -0.32 -0.45 -0.29 -0.18 -0.23 -0.20 -0.20 -0.15 -0.14 &gt;<br>
INFO: cmn_prior.c(116): cmn_prior_update: to   &lt;  9.19  0.49 -0.17  0.08 -0.34 -0.46 -0.31 -0.20 -0.24 -0.22 -0.23 -0.16 -0.18 &gt;<br>
2015-01-07 13:01:57.566 OpenEarsSampleApp[842:1803] End of speech detected&#8230;<br>
INFO: cmn_prior.c(131): cmn_prior_update: from &lt;  9.19  0.49 -0.17  0.08 -0.34 -0.46 -0.31 -0.20 -0.24 -0.22 -0.23 -0.16 -0.18 &gt;<br>
INFO: cmn_prior.c(149): cmn_prior_update: to   &lt;  8.68  0.43 -0.14  0.05 -0.31 -0.46 -0.32 -0.19 -0.22 -0.20 -0.23 -0.15 -0.18 &gt;<br>
INFO: ngram_search_fwdtree.c(1550):     1269 words recognized (4/fr)<br>
INFO: ngram_search_fwdtree.c(1552):    74257 senones evaluated (206/fr)<br>
INFO: ngram_search_fwdtree.c(1556):    21743 channels searched (60/fr), 3037 1st, 11112 last<br>
INFO: ngram_search_fwdtree.c(1559):     1527 words for which last channels evaluated (4/fr)<br>
INFO: ngram_search_fwdtree.c(1561):      803 candidate words for entering last phone (2/fr)<br>
INFO: ngram_search_fwdtree.c(1564): fwdtree 1.27 CPU 0.353 xRT<br>
INFO: ngram_search_fwdtree.c(1567): fwdtree 4.12 wall 1.143 xRT<br>
INFO: ngram_search_fwdflat.c(302): Utterance vocabulary contains 9 words<br>
2015-01-07 13:01:57.567 OpenEarsSampleApp[842:60b] Local callback: Pocketsphinx has detected a second of silence, concluding an utterance.<br>
INFO: ngram_search_fwdflat.c(938):      966 words recognized (3/fr)<br>
INFO: ngram_search_fwdflat.c(940):    65712 senones evaluated (183/fr)<br>
INFO: ngram_search_fwdflat.c(942):    24595 channels searched (68/fr)<br>
INFO: ngram_search_fwdflat.c(944):     2147 words searched (5/fr)<br>
INFO: ngram_search_fwdflat.c(947):      882 word transitions (2/fr)<br>
INFO: ngram_search_fwdflat.c(950): fwdflat 0.87 CPU 0.242 xRT<br>
INFO: ngram_search_fwdflat.c(953): fwdflat 0.88 wall 0.244 xRT<br>
INFO: ngram_search.c(1268): lattice start node &lt;s&gt;.0 end node &lt;/s&gt;.354<br>
INFO: ngram_search.c(1294): Eliminated 0 nodes before end node<br>
INFO: ngram_search.c(1399): Lattice has 77 nodes, 73 links<br>
INFO: ps_lattice.c(1368): Normalizer P(O) = alpha(&lt;/s&gt;:354:358) = -1031992<br>
INFO: ps_lattice.c(1403): Joint P(O,S) = -1042543 P(S|O) = -10551<br>
INFO: ngram_search.c(890): bestpath 0.00 CPU 0.001 xRT<br>
INFO: ngram_search.c(893): bestpath 0.00 wall 0.000 xRT<br>
2015-01-07 13:01:58.446 OpenEarsSampleApp[842:1803] Pocketsphinx heard &#8220;ADIOS ROMA CHORIZO HORCHATA HORCHATA HORCHATA&#8221; with a score of (-10551) and an utterance ID of 3.<br>
2015-01-07 13:01:58.448 OpenEarsSampleApp[842:60b] Flite sending interrupt speech request.<br>
2015-01-07 13:01:58.449 OpenEarsSampleApp[842:60b] Local callback: The received hypothesis is ADIOS ROMA CHORIZO HORCHATA HORCHATA HORCHATA with a score of -10551 and an ID of 3<br>
2015-01-07 13:01:58.452 OpenEarsSampleApp[842:60b] I&#8217;m running flite<br>
2015-01-07 13:01:58.738 OpenEarsSampleApp[842:1803] Speech detected&#8230;<br>
2015-01-07 13:01:58.815 OpenEarsSampleApp[842:60b] I&#8217;m done running flite and it took 0.361972 seconds<br>
2015-01-07 13:01:58.816 OpenEarsSampleApp[842:60b] Flite audio player was nil when referenced so attempting to allocate a new audio player.<br>
2015-01-07 13:01:58.818 OpenEarsSampleApp[842:60b] Loading speech data for Flite concluded successfully.<br>
2015-01-07 13:01:58.847 OpenEarsSampleApp[842:60b] Local callback:  hypothesisArray is (<br>
        {<br>
        Hypothesis = &#8220;ADIOS ROMA CHORIZO HORCHATA HORCHATA HORCHATA&#8221;;<br>
        Score = &#8220;-18498&#8221;;<br>
    },<br>
        {<br>
        Hypothesis = &#8220;ADIOS ROMA CHORIZO LECHUGA ROMA HORCHATA&#8221;;<br>
        Score = &#8220;-18498&#8221;;<br>
    },<br>
        {<br>
        Hypothesis = &#8220;ADIOS ROMA CHORIZO HORCHATA HOLA HORCHATA&#8221;;<br>
        Score = &#8220;-18498&#8221;;<br>
    },<br>
        {<br>
        Hypothesis = &#8220;ADIOS ROMA CHORIZO HORCHATA HORCHATA&#8221;;<br>
        Score = &#8220;-18498&#8221;;<br>
    },<br>
        {<br>
        Hypothesis = &#8220;HOLA ROMA CHORIZO HORCHATA HORCHATA HORCHATA&#8221;;<br>
        Score = &#8220;-18498&#8221;;<br>
    }<br>
)<br>
2015-01-07 13:01:58.848 OpenEarsSampleApp[842:60b] Local callback: Pocketsphinx has detected speech.<br>
2015-01-07 13:01:58.850 OpenEarsSampleApp[842:60b] Flite sending suspend recognition notification.<br>
2015-01-07 13:01:58.852 OpenEarsSampleApp[842:60b] Local callback: Flite has started speaking<br>
2015-01-07 13:01:58.859 OpenEarsSampleApp[842:60b] Local callback: Pocketsphinx has suspended recognition.<br>
2015-01-07 13:02:02.658 OpenEarsSampleApp[842:60b] AVAudioPlayer did finish playing with success flag of 1<br>
2015-01-07 13:02:02.810 OpenEarsSampleApp[842:60b] Flite sending resume recognition notification.<br>
2015-01-07 13:02:03.314 OpenEarsSampleApp[842:60b] Local callback: Flite has finished speaking<br>
2015-01-07 13:02:03.320 OpenEarsSampleApp[842:60b] Valid setSecondsOfSilence value of 0.500000 will be used.<br>
2015-01-07 13:02:03.321 OpenEarsSampleApp[842:60b] Local callback: Pocketsphinx has resumed recognition.<br>
INFO: cmn_prior.c(131): cmn_prior_update: from &lt;  8.68  0.43 -0.14  0.05 -0.31 -0.46 -0.32 -0.19 -0.22 -0.20 -0.23 -0.15 -0.18 &gt;<br>
INFO: cmn_prior.c(149): cmn_prior_update: to   &lt;  8.79  0.45 -0.13  0.05 -0.30 -0.47 -0.36 -0.19 -0.22 -0.21 -0.24 -0.16 -0.19 &gt;<br>
INFO: ngram_search_fwdtree.c(1550):      600 words recognized (5/fr)<br>
INFO: ngram_search_fwdtree.c(1552):    34910 senones evaluated (277/fr)<br>
INFO: ngram_search_fwdtree.c(1556):    10828 channels searched (85/fr), 1096 1st, 6179 last<br>
INFO: ngram_search_fwdtree.c(1559):      736 words for which last channels evaluated (5/fr)<br>
INFO: ngram_search_fwdtree.c(1561):      563 candidate words for entering last phone (4/fr)<br>
INFO: ngram_search_fwdtree.c(1564): fwdtree 0.99 CPU 0.788 xRT<br>
INFO: ngram_search_fwdtree.c(1567): fwdtree 5.01 wall 3.973 xRT<br>
INFO: ngram_search_fwdflat.c(302): Utterance vocabulary contains 10 words<br>
INFO: ngram_search_fwdflat.c(938):      321 words recognized (3/fr)<br>
INFO: ngram_search_fwdflat.c(940):    34110 senones evaluated (271/fr)<br>
INFO: ngram_search_fwdflat.c(942):    14032 channels searched (111/fr)<br>
INFO: ngram_search_fwdflat.c(944):     1130 words searched (8/fr)<br>
INFO: ngram_search_fwdflat.c(947):      464 word transitions (3/fr)<br>
INFO: ngram_search_fwdflat.c(950): fwdflat 0.46 CPU 0.363 xRT<br>
INFO: ngram_search_fwdflat.c(953): fwdflat 0.46 wall 0.362 xRT<br>
</p>
<p>Here the link to download the speech I am using:</p>
<p><a href="https://dl.dropboxusercontent.com/u/6380067/openears4.wav.zip" rel="nofollow">https://dl.dropboxusercontent.com/u/6380067/openears4.wav.zip</a></p>
<p>In this speech I just said two words in Spanish &#8220;MADRID&#8221; in second 10 and &#8220;ROMA&#8221; in second 20. Rest are noises around me.</p>
<p>I hope this helps you again.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1024126" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">January 7, 2015 at 8:14 pm</span>

		
		<a href="/forums/topic/noise-problem-even-setting-setvadthreshold-openears-2-0/#post-1024126" class="bbp-reply-permalink">#1024126</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1024126 -->

<div class="loop-item-14 user-id-4 bbp-parent-forum-3654 bbp-parent-topic-1024045 bbp-reply-position-15 odd  post-1024126 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/halle-2/" title="View Halle Winkler&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Halle Winkler</span></a><div class="bbp-author-role">Politepix</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<blockquote><p>Anyway, regarding your phrase “a vadThreshold as high as 3.5 will suppress actual user speech in testing” is enough to know that something is really wrong with 2.0.1 because here my test and you will see the results:</p></blockquote>
<p>But this recording is not of noises, it is a recording of continuous, human-comprehensible speech without crosstalk which precedes the louder speech by a long time in the recording, so it is the reference point for distinguishing between speech and silence for the engine. It does make sense that it is being detected as speech rather than noise since it is single-speaker speech which is clear enough to be understood by a human listener. A user using an app from a distance could have their speech detected at this power level.</p>
<p>I agree with you that the behavior has changed from 1.70 (or more specifically Pocketsphinx .8), but I don&#8217;t see this is a sign of something being really wrong, since it is speech at a volume which could be user speech, and the recording begins with the quieter speech and carries on for long enough that it makes sense that it is recognized. It is a bit strange that it isn&#8217;t recognized at all in 1.70. </p>
<p>In any case, that isn&#8217;t the behavior you want for your app, which is reasonable. I did a large amount of testing today on your case using the old and new Pocketsphinx on some Ubuntu VMs and the old and new OpenEars, and I did notice that it looks a bit like for Spanish recognition, the vadThreshold values would be more useful if they went higher as you requested earlier. In 2.01 I had similar results to 1.70 when I used a vadThreshold of 4.4 or 4.3, which more similar to lower values with the English model (although I had better accuracy with 2.01).  It seems possible that the ideal vadThreshold values may have some relationship to the acoustic models and when I&#8217;ve had more time to test it I will check in with the Sphinx project and see if they have some ideas about whether that is the case, at which point I may add some kind of vadThreshold multiplier to OpenEars.</p>
<p>For now, so you can get on with things, I&#8217;ve uploaded OpenEars 2.02 which has a maximum vadThreshold of 5.0. When I set it to 4.4 in my test of your audio, it recognized &#8220;MADRID&#8221; and then &#8220;ROMA&#8221; and nothing else. I hope this is helpful.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1024137" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">January 8, 2015 at 12:34 pm</span>

		
		<a href="/forums/topic/noise-problem-even-setting-setvadthreshold-openears-2-0/#post-1024137" class="bbp-reply-permalink">#1024137</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1024137 -->

<div class="loop-item-15 user-id-1735 bbp-parent-forum-3654 bbp-parent-topic-1024045 bbp-reply-position-16 even topic-author  post-1024137 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/maxgarmar/" title="View maxgarmar&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">maxgarmar</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Ok Halle, now I got it. Anyway thanks so much for increasing that value to improve the response with noises in the Spanish language. But now I have a doubt.<br>
Regarding, </p>
<blockquote><p> I did notice that it looks a bit like for Spanish recognition, the vadThreshold values would be more useful if they went higher as you requested earlier. In 2.01 I had similar results to 1.70 when I used a vadThreshold of 4.4 or 4.3, which more similar to lower values with the English model (although I had better accuracy with 2.01)</p></blockquote>
<p>I would like to know then, what are the &#8220;lower&#8221; values for English language to make it like Spanish. I meant, if 4.4 or 4.3 is for Spanish, what do you think is the best for English. My app is also working in English so it would help me.</p>
<p>Thanks and I will give you a feedback about my test of 2.0.2</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1024138" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">January 8, 2015 at 12:52 pm</span>

		
		<a href="/forums/topic/noise-problem-even-setting-setvadthreshold-openears-2-0/#post-1024138" class="bbp-reply-permalink">#1024138</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1024138 -->

<div class="loop-item-16 user-id-4 bbp-parent-forum-3654 bbp-parent-topic-1024045 bbp-reply-position-17 odd  post-1024138 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/halle-2/" title="View Halle Winkler&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Halle Winkler</span></a><div class="bbp-author-role">Politepix</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>That&#8217;s a good question – from what I heard from other developers, they are seeing more noise suppression from 2.5-3.5 and past 3.5 starts to suppress real interactions. I am going to continue testing this out over the coming weeks to see if I can get some more concrete insight into this and perhaps discuss it with the Sphinx project once I have some good tests for their testbed (rather than mine), so there may be a later OpenEars version that has defaults that are more similar to the old behavior for each language. In the meantime, to find out the ideal settings for your own app with each language, testing is your friend :) .</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1024174" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">January 9, 2015 at 9:26 pm</span>

		
		<a href="/forums/topic/noise-problem-even-setting-setvadthreshold-openears-2-0/#post-1024174" class="bbp-reply-permalink">#1024174</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1024174 -->

<div class="loop-item-17 user-id-4 bbp-parent-forum-3654 bbp-parent-topic-1024045 bbp-reply-position-18 even  post-1024174 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/halle-2/" title="View Halle Winkler&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Halle Winkler</span></a><div class="bbp-author-role">Politepix</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>In my testing for version 2.03, a vadThreshold of 2.0 was fine for excluding a normal amount of background noise.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1024183" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">January 12, 2015 at 1:06 pm</span>

		
		<a href="/forums/topic/noise-problem-even-setting-setvadthreshold-openears-2-0/#post-1024183" class="bbp-reply-permalink">#1024183</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1024183 -->

<div class="loop-item-18 user-id-1735 bbp-parent-forum-3654 bbp-parent-topic-1024045 bbp-reply-position-19 odd topic-author  post-1024183 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/maxgarmar/" title="View maxgarmar&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">maxgarmar</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>In which language are you talking about, Halle ?<br>
I saw in changelog that there are many improvements in 2.03. I will test it soon.</p>
<p>Thanks</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1024184" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">January 12, 2015 at 1:11 pm</span>

		
		<a href="/forums/topic/noise-problem-even-setting-setvadthreshold-openears-2-0/#post-1024184" class="bbp-reply-permalink">#1024184</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1024184 -->

<div class="loop-item-19 user-id-4 bbp-parent-forum-3654 bbp-parent-topic-1024045 bbp-reply-position-20 even  post-1024184 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/halle-2/" title="View Halle Winkler&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Halle Winkler</span></a><div class="bbp-author-role">Politepix</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>English worked fine for me at 2.0.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1024197" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">January 13, 2015 at 11:20 am</span>

		
		<a href="/forums/topic/noise-problem-even-setting-setvadthreshold-openears-2-0/#post-1024197" class="bbp-reply-permalink">#1024197</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1024197 -->

<div class="loop-item-20 user-id-4 bbp-parent-forum-3654 bbp-parent-topic-1024045 bbp-reply-position-21 odd  post-1024197 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/halle-2/" title="View Halle Winkler&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Halle Winkler</span></a><div class="bbp-author-role">Politepix</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Just an update that I have verification from CMU Sphinx that needing a higher VAD threshold for the Spanish acoustic model is expected behavior. This may change in the future with the creation of a new Spanish acoustic model (that I would expect can be included in OpenEars at that time). I wanted to thank you sincerely for providing me with the test cases and reports about this, because it wasn&#8217;t showing up in my own tests and it&#8217;s important to know about for usability of the current model with OpenEars 2.x.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1024199" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">January 13, 2015 at 1:15 pm</span>

		
		<a href="/forums/topic/noise-problem-even-setting-setvadthreshold-openears-2-0/#post-1024199" class="bbp-reply-permalink">#1024199</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1024199 -->

<div class="loop-item-21 user-id-1735 bbp-parent-forum-3654 bbp-parent-topic-1024045 bbp-reply-position-22 even topic-author  post-1024199 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/maxgarmar/" title="View maxgarmar&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">maxgarmar</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Not for this. I am using it so I just wanted to improve it as much as I can.<br>
I think we can close this thread (although I will open a new one because other problem :D).</p>
<p>Thanks for helping and supporting</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
		
	</li>
<!-- .bbp-body -->

	<li class="bbp-footer">
		<div class="bbp-reply-author">Author</div>
		<div class="bbp-reply-content">Posts</div>
<!-- .bbp-reply-content -->
	</li>
<!-- .bbp-footer -->
</ul>
<!-- #topic-1024045-replies -->


			
<div class="bbp-pagination">
	<div class="bbp-pagination-count">Viewing 22 posts - 1 through 22 (of 22 total)</div>
	<div class="bbp-pagination-links"></div>
</div>


		
		

	<div id="no-reply-1024045" class="bbp-no-reply">
		<div class="bbp-template-notice">
			<ul>
				<li>You must be logged in to reply to this topic.</li>
			</ul>
		</div>

		
			
<form method="post" action="https://www.politepix.com/wp-login.php" class="bbp-login-form">
	<fieldset class="bbp-form">
		<legend>Log In</legend>

		<div class="bbp-username">
			<label for="user_login">Username: </label>
			<input type="text" name="log" value="" size="20" maxlength="100" id="user_login" autocomplete="off">
		</div>

		<div class="bbp-password">
			<label for="user_pass">Password: </label>
			<input type="password" name="pwd" value="" size="20" id="user_pass" autocomplete="off">
		</div>

		<div class="bbp-remember-me">
			<input type="checkbox" name="rememberme" value="forever" id="rememberme">
			<label for="rememberme">Keep me signed in</label>
		</div>

		
		<div class="bbp-submit-wrapper">

			<button type="submit" name="user-submit" id="user-submit" class="button submit user-submit">Log In</button>

			
	<input type="hidden" name="user-cookie" value="1">

	<input type="hidden" id="bbp_redirect_to" name="redirect_to" value="https://www.politepix.com/forums/topic/noise-problem-even-setting-setvadthreshold-openears-2-0/?simply_static_page=4498"><input type="hidden" id="_wpnonce" name="_wpnonce" value="7d2a08c1e3"><input type="hidden" name="_wp_http_referer" value="/forums/topic/noise-problem-even-setting-setvadthreshold-openears-2-0/?simply_static_page=4498">
		</div>
	</fieldset>
</form>

		
	</div>



	
	

	
</div>

					               	</section><!-- /.entry -->

				                
            </article><!-- /.post -->
            
              
        
		</section><!-- /#main -->
		
		
        	
<aside id="sidebar" class="col-right">

	
	    <div class="primary">
		<div id="bbp_forums_widget-5" class="widget widget_display_forums">
<h3>Forums</h3>
		<ul class="bbp-forums-widget">

			
				<li>
					<a class="bbp-forum-title" href="/forums/forum/openears-plugins/">
						OpenEars plugins					</a>
				</li>

			
				<li class="bbp-forum-widget-current-forum">
					<a class="bbp-forum-title" href="/forums/forum/openearsforum/">
						OpenEars					</a>
				</li>

			
		</ul>

		</div>		           
	</div>        
	   
	
	 
	
</aside><!-- /#sidebar -->

    </div>
<!-- /#content -->
		
		<footer id="footer" class="col-full">
	
			<div id="copyright" class="col-left">
							<p>Politepix &copy; 2024. All Rights Reserved.</p>
						</div>
	
			<div id="credit" class="col-right">
	        <div style="text-align:right;line-height:1.3em">OpenEars® is a registered trademark of Politepix<br>AllHours® is a registered trademark of Politepix<br>The Politepix site uses cookies in order to understand how the website is used by visitors and in order to enable some required functionality. You can learn all about which cookies we use on the <a href="/about/">About</a> page, as well as everything about our privacy policy.<br>
<a href="https://twitter.com/Politepix" rel="me">TWITTER</a> | <a href="/contact" id="impressumlink">CONTACT POLITEPIX</a><a href="/about" id="impressumlink"> | IMPRESSUM | ABOUT | LEGAL | IMPRINT</a>
</div>			</div>
	<a rel="me" href="https://mastodon.social/@Halle">M</a>
		</footer><!-- /#footer  -->

</div>
<!-- /#wrapper -->

<!-- Consent Management powered by Complianz | GDPR/CCPA Cookie Consent https://wordpress.org/plugins/complianz-gdpr -->
<div id="cmplz-cookiebanner-container">
<div class="cmplz-cookiebanner cmplz-hidden banner-1 bottom-right-view-preferences optin cmplz-bottom-right cmplz-categories-type-view-preferences" aria-modal="true" data-nosnippet="true" role="dialog" aria-live="polite" aria-labelledby="cmplz-header-1-optin" aria-describedby="cmplz-message-1-optin">
	<div class="cmplz-header">
		<div class="cmplz-logo"></div>
		<div class="cmplz-title" id="cmplz-header-1-optin">Manage Cookie Consent</div>
		<div class="cmplz-close" tabindex="0" role="button" aria-label="Close dialog">
			<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="times" class="svg-inline--fa fa-times fa-w-11" role="img" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 352 512"><path fill="currentColor" d="M242.72 256l100.07-100.07c12.28-12.28 12.28-32.19 0-44.48l-22.24-22.24c-12.28-12.28-32.19-12.28-44.48 0L176 189.28 75.93 89.21c-12.28-12.28-32.19-12.28-44.48 0L9.21 111.45c-12.28 12.28-12.28 32.19 0 44.48L109.28 256 9.21 356.07c-12.28 12.28-12.28 32.19 0 44.48l22.24 22.24c12.28 12.28 32.2 12.28 44.48 0L176 322.72l100.07 100.07c12.28 12.28 32.2 12.28 44.48 0l22.24-22.24c12.28-12.28 12.28-32.19 0-44.48L242.72 256z"></path></svg>
		</div>
	</div>

	<div class="cmplz-divider cmplz-divider-header"></div>
	<div class="cmplz-body">
		<div class="cmplz-message" id="cmplz-message-1-optin">To provide the best experiences, we use technologies like cookies to store and/or access device information. Consenting to these technologies will allow us to process data such as browsing behavior or unique IDs on this site. Not consenting or withdrawing consent, may adversely affect certain features and functions.</div>
		<!-- categories start -->
		<div class="cmplz-categories">
			<details class="cmplz-category cmplz-functional">
				<summary>
						<span class="cmplz-category-header">
							<span class="cmplz-category-title">Functional</span>
							<span class="cmplz-always-active">
								<span class="cmplz-banner-checkbox">
									<input type="checkbox" id="cmplz-functional-optin" data-category="cmplz_functional" class="cmplz-consent-checkbox cmplz-functional" size="40" value="1">
									<label class="cmplz-label" for="cmplz-functional-optin" tabindex="0"><span class="screen-reader-text">Functional</span></label>
								</span>
								Always active							</span>
							<span class="cmplz-icon cmplz-open">
								<svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 448 512" height="18"><path d="M224 416c-8.188 0-16.38-3.125-22.62-9.375l-192-192c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L224 338.8l169.4-169.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-192 192C240.4 412.9 232.2 416 224 416z"></path></svg>
							</span>
						</span>
				</summary>
				<div class="cmplz-description">
					<span class="cmplz-description-functional">The technical storage or access is strictly necessary for the legitimate purpose of enabling the use of a specific service explicitly requested by the subscriber or user, or for the sole purpose of carrying out the transmission of a communication over an electronic communications network.</span>
				</div>
			</details>

			<details class="cmplz-category cmplz-preferences">
				<summary>
						<span class="cmplz-category-header">
							<span class="cmplz-category-title">Preferences</span>
							<span class="cmplz-banner-checkbox">
								<input type="checkbox" id="cmplz-preferences-optin" data-category="cmplz_preferences" class="cmplz-consent-checkbox cmplz-preferences" size="40" value="1">
								<label class="cmplz-label" for="cmplz-preferences-optin" tabindex="0"><span class="screen-reader-text">Preferences</span></label>
							</span>
							<span class="cmplz-icon cmplz-open">
								<svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 448 512" height="18"><path d="M224 416c-8.188 0-16.38-3.125-22.62-9.375l-192-192c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L224 338.8l169.4-169.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-192 192C240.4 412.9 232.2 416 224 416z"></path></svg>
							</span>
						</span>
				</summary>
				<div class="cmplz-description">
					<span class="cmplz-description-preferences">The technical storage or access is necessary for the legitimate purpose of storing preferences that are not requested by the subscriber or user.</span>
				</div>
			</details>

			<details class="cmplz-category cmplz-statistics">
				<summary>
						<span class="cmplz-category-header">
							<span class="cmplz-category-title">Statistics</span>
							<span class="cmplz-banner-checkbox">
								<input type="checkbox" id="cmplz-statistics-optin" data-category="cmplz_statistics" class="cmplz-consent-checkbox cmplz-statistics" size="40" value="1">
								<label class="cmplz-label" for="cmplz-statistics-optin" tabindex="0"><span class="screen-reader-text">Statistics</span></label>
							</span>
							<span class="cmplz-icon cmplz-open">
								<svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 448 512" height="18"><path d="M224 416c-8.188 0-16.38-3.125-22.62-9.375l-192-192c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L224 338.8l169.4-169.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-192 192C240.4 412.9 232.2 416 224 416z"></path></svg>
							</span>
						</span>
				</summary>
				<div class="cmplz-description">
					<span class="cmplz-description-statistics">The technical storage or access that is used exclusively for statistical purposes.</span>
					<span class="cmplz-description-statistics-anonymous">The technical storage or access that is used exclusively for anonymous statistical purposes. Without a subpoena, voluntary compliance on the part of your Internet Service Provider, or additional records from a third party, information stored or retrieved for this purpose alone cannot usually be used to identify you.</span>
				</div>
			</details>
			<details class="cmplz-category cmplz-marketing">
				<summary>
						<span class="cmplz-category-header">
							<span class="cmplz-category-title">Marketing</span>
							<span class="cmplz-banner-checkbox">
								<input type="checkbox" id="cmplz-marketing-optin" data-category="cmplz_marketing" class="cmplz-consent-checkbox cmplz-marketing" size="40" value="1">
								<label class="cmplz-label" for="cmplz-marketing-optin" tabindex="0"><span class="screen-reader-text">Marketing</span></label>
							</span>
							<span class="cmplz-icon cmplz-open">
								<svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 448 512" height="18"><path d="M224 416c-8.188 0-16.38-3.125-22.62-9.375l-192-192c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L224 338.8l169.4-169.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-192 192C240.4 412.9 232.2 416 224 416z"></path></svg>
							</span>
						</span>
				</summary>
				<div class="cmplz-description">
					<span class="cmplz-description-marketing">The technical storage or access is required to create user profiles to send advertising, or to track the user on a website or across several websites for similar marketing purposes.</span>
				</div>
			</details>
		</div>
<!-- categories end -->
			</div>

	<div class="cmplz-links cmplz-information">
		<a class="cmplz-link cmplz-manage-options cookie-statement" href="#" data-relative_url="#cmplz-manage-consent-container">Manage options</a>
		<a class="cmplz-link cmplz-manage-third-parties cookie-statement" href="#" data-relative_url="#cmplz-cookies-overview">Manage services</a>
		<a class="cmplz-link cmplz-manage-vendors tcf cookie-statement" href="#" data-relative_url="#cmplz-tcf-wrapper">Manage {vendor_count} vendors</a>
		<a class="cmplz-link cmplz-external cmplz-read-more-purposes tcf" target="_blank" rel="noopener noreferrer nofollow" href="https://cookiedatabase.org/tcf/purposes/">Read more about these purposes</a>
			</div>

	<div class="cmplz-divider cmplz-footer"></div>

	<div class="cmplz-buttons">
		<button class="cmplz-btn cmplz-accept">Accept</button>
		<button class="cmplz-btn cmplz-deny">Deny</button>
		<button class="cmplz-btn cmplz-view-preferences">View preferences</button>
		<button class="cmplz-btn cmplz-save-preferences">Save preferences</button>
		<a class="cmplz-btn cmplz-manage-options tcf cookie-statement" href="#" data-relative_url="#cmplz-manage-consent-container">View preferences</a>
			</div>

	<div class="cmplz-links cmplz-documents">
		<a class="cmplz-link cookie-statement" href="#" data-relative_url="">{title}</a>
		<a class="cmplz-link privacy-statement" href="#" data-relative_url="">{title}</a>
		<a class="cmplz-link impressum" href="#" data-relative_url="">{title}</a>
			</div>

</div>
</div>
					<div id="cmplz-manage-consent" data-nosnippet="true">
<button class="cmplz-btn cmplz-hidden cmplz-manage-consent manage-consent-1">Manage consent</button>

</div>
<!--[if lt IE 9]>
<script src="https://www.politepix.com/wp-content/themes/pixelpress/includes/js/respond-IE.js"></script>
<![endif]-->
			<script>jQuery(document).ready(function(){
					jQuery('.images a').attr('rel', 'prettyPhoto[product-gallery]');
				});</script>
			<script type="text/javascript">(function () {
			var c = document.body.className;
			c = c.replace(/woocommerce-no-js/, 'woocommerce-js');
			document.body.className = c;
		})();</script>
	<link rel="stylesheet" id="wc-blocks-style-css" href="https://c0.wp.com/p/woocommerce/8.8.2/assets/client/blocks/wc-blocks.css" type="text/css" media="all">
<script type="text/javascript" src="https://c0.wp.com/p/woocommerce/8.8.2/assets/js/accounting/accounting.min.js" id="accounting-js"></script>
<script type="text/javascript" src="https://c0.wp.com/c/6.5.2/wp-includes/js/jquery/ui/core.min.js" id="jquery-ui-core-js"></script>
<script type="text/javascript" src="https://c0.wp.com/c/6.5.2/wp-includes/js/jquery/ui/datepicker.min.js" id="jquery-ui-datepicker-js"></script>
<script type="text/javascript" id="jquery-ui-datepicker-js-after">
/* <![CDATA[ */
jQuery(function(jQuery){jQuery.datepicker.setDefaults({"closeText":"Close","currentText":"Today","monthNames":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthNamesShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"nextText":"Next","prevText":"Previous","dayNames":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"dayNamesShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"dayNamesMin":["S","M","T","W","T","F","S"],"dateFormat":"MM d, yy","firstDay":1,"isRTL":false});});
/* ]]> */
</script>
<script type="text/javascript" id="woocommerce-addons-js-extra">
/* <![CDATA[ */
var woocommerce_addons_params = {"price_display_suffix":"","tax_enabled":"1","price_include_tax":"","display_include_tax":"","ajax_url":"\/wp-admin\/admin-ajax.php","i18n_validation_required_select":"Please choose an option.","i18n_validation_required_input":"Please enter some text in this field.","i18n_validation_required_number":"Please enter a number in this field.","i18n_validation_required_file":"Please upload a file.","i18n_validation_letters_only":"Please enter letters only.","i18n_validation_numbers_only":"Please enter numbers only.","i18n_validation_letters_and_numbers_only":"Please enter letters and numbers only.","i18n_validation_email_only":"Please enter a valid email address.","i18n_validation_min_characters":"Please enter at least %c characters.","i18n_validation_max_characters":"Please enter up to %c characters.","i18n_validation_min_number":"Please enter %c or more.","i18n_validation_max_number":"Please enter %c or less.","i18n_sub_total":"Subtotal","i18n_remaining":"<span><\/span> characters remaining","currency_format_num_decimals":"2","currency_format_symbol":"€","currency_format_decimal_sep":".","currency_format_thousand_sep":",","trim_trailing_zeros":"","is_bookings":"","trim_user_input_characters":"1000","quantity_symbol":"x ","datepicker_class":"wc_pao_datepicker","datepicker_date_format":"MM d, yy","gmt_offset":"-2","date_input_timezone_reference":"default","currency_format":"%s%v"};
/* ]]> */
</script>
<script type="text/javascript" src="/wp-content/plugins/woocommerce-product-addons/assets/js/frontend/addons.min.js?ver=6.8.2" id="woocommerce-addons-js" defer data-wp-strategy="defer"></script>
<script type="text/javascript" src="/wp-content/plugins/jetpack/jetpack_vendor/automattic/jetpack-image-cdn/dist/image-cdn.js?minify=false&amp;ver=132249e245926ae3e188" id="jetpack-photon-js"></script>
<script type="text/javascript" src="/wp-content/plugins/bbpress/templates/default/js/editor.min.js?ver=2.6.9" id="bbpress-editor-js"></script>
<script type="text/javascript" id="bbpress-engagements-js-extra">
/* <![CDATA[ */
var bbpEngagementJS = {"object_id":"1024045","bbp_ajaxurl":"\/forums\/topic\/noise-problem-even-setting-setvadthreshold-openears-2-0\/?bbp-ajax=true","generic_ajax_error":"Something went wrong. Refresh your browser and try again."};
/* ]]> */
</script>
<script type="text/javascript" src="/wp-content/plugins/bbpress/templates/default/js/engagements.min.js?ver=2.6.9" id="bbpress-engagements-js"></script>
<script type="text/javascript" src="https://c0.wp.com/p/woocommerce/8.8.2/assets/js/sourcebuster/sourcebuster.min.js" id="sourcebuster-js-js"></script>
<script type="text/javascript" id="wc-order-attribution-js-extra">
/* <![CDATA[ */
var wc_order_attribution = {"params":{"lifetime":1.0e-5,"session":30,"ajaxurl":"\/wp-admin\/admin-ajax.php","prefix":"wc_order_attribution_","allowTracking":true},"fields":{"source_type":"current.typ","referrer":"current_add.rf","utm_campaign":"current.cmp","utm_source":"current.src","utm_medium":"current.mdm","utm_content":"current.cnt","utm_id":"current.id","utm_term":"current.trm","session_entry":"current_add.ep","session_start_time":"current_add.fd","session_pages":"session.pgs","session_count":"udata.vst","user_agent":"udata.uag"}};
/* ]]> */
</script>
<script type="text/javascript" src="https://c0.wp.com/p/woocommerce/8.8.2/assets/js/frontend/order-attribution.min.js" id="wc-order-attribution-js"></script>
<script data-service="jetpack-statistics" data-category="statistics" type="text/plain" data-cmplz-src="https://stats.wp.com/e-202417.js" id="jetpack-stats-js" data-wp-strategy="defer"></script>
<script type="text/javascript" id="jetpack-stats-js-after">
/* <![CDATA[ */
_stq = window._stq || [];
_stq.push([ "view", JSON.parse("{\"v\":\"ext\",\"blog\":\"206848719\",\"post\":\"1024045\",\"tz\":\"2\",\"srv\":\"www.politepix.com\",\"j\":\"1:13.3.1\"}") ]);
_stq.push([ "clickTrackerInit", "206848719", "1024045" ]);
/* ]]> */
</script>
<script type="text/javascript" id="cmplz-cookiebanner-js-extra">
/* <![CDATA[ */
var complianz = {"prefix":"cmplz_","user_banner_id":"1","set_cookies":[],"block_ajax_content":"","banner_version":"18","version":"7.0.4","store_consent":"","do_not_track_enabled":"1","consenttype":"optin","region":"eu","geoip":"","dismiss_timeout":"","disable_cookiebanner":"","soft_cookiewall":"","dismiss_on_scroll":"","cookie_expiry":"365","url":"\/wp-json\/complianz\/v1\/","locale":"lang=en&locale=en_US","set_cookies_on_root":"","cookie_domain":"","current_policy_id":"16","cookie_path":"\/","categories":{"statistics":"statistics","marketing":"marketing"},"tcf_active":"","placeholdertext":"Click to accept {category} cookies and enable this content","css_file":"\/wp-content\/uploads\/complianz\/css\/banner-{banner_id}-{type}.css?v=18","page_links":{"eu":{"cookie-statement":{"title":"About &#038; Impressum &#038; Privacy Policy","url":"\/about\/"},"privacy-statement":{"title":"About &#038; Impressum &#038; Privacy Policy","url":"\/about\/"},"impressum":{"title":"About &#038; Impressum &#038; Privacy Policy","url":"\/about\/"}},"us":{"impressum":{"title":"About &#038; Impressum &#038; Privacy Policy","url":"\/about\/"}},"uk":{"impressum":{"title":"About &#038; Impressum &#038; Privacy Policy","url":"\/about\/"}},"ca":{"impressum":{"title":"About &#038; Impressum &#038; Privacy Policy","url":"\/about\/"}},"au":{"impressum":{"title":"About &#038; Impressum &#038; Privacy Policy","url":"\/about\/"}},"za":{"impressum":{"title":"About &#038; Impressum &#038; Privacy Policy","url":"\/about\/"}},"br":{"impressum":{"title":"About &#038; Impressum &#038; Privacy Policy","url":"\/about\/"}}},"tm_categories":"","forceEnableStats":"","preview":"","clean_cookies":"","aria_label":"Click to accept {category} cookies and enable this content"};
/* ]]> */
</script>
<script defer type="text/javascript" src="/wp-content/plugins/complianz-gdpr/cookiebanner/js/complianz.min.js?ver=1710283454" id="cmplz-cookiebanner-js"></script>
</body>
</html>