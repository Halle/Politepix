<!DOCTYPE html>
<html lang="en-US">
<head>

<meta charset="UTF-8">

<title>Topic: RapidEars startRealtimeListeningWithLanguageModelAtPath memory leaks | Politepix</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link href="https://www.politepix.com/wp-content/uploads/omgf/omgf-stylesheet-56/omgf-stylesheet-56.css?ver=1668781124" rel="stylesheet" type="text/css">

<link rel="stylesheet" type="text/css" href="https://www.politepix.com/wp-content/themes/politepix-pixelpress-child-theme/style.css" media="screen">

<link rel="pingback" href="/xmlrpc.php">
<meta name="robots" content="max-image-preview:large">
<script>window._wca = window._wca || [];</script>

<link rel="alternate" type="application/rss+xml" title="Politepix &raquo; Feed" href="http://feeds.feedburner.com/politepixblog">
<link rel="alternate" type="application/rss+xml" title="Politepix &raquo; Comments Feed" href="/comments/feed/">
<script type="text/javascript">
/* <![CDATA[ */
window._wpemojiSettings = {"baseUrl":"https:\/\/s.w.org\/images\/core\/emoji\/15.0.3\/72x72\/","ext":".png","svgUrl":"https:\/\/s.w.org\/images\/core\/emoji\/15.0.3\/svg\/","svgExt":".svg","source":{"concatemoji":"\/wp-includes\/js\/wp-emoji-release.min.js?ver=6.5.2"}};
/*! This file is auto-generated */
!function(i,n){var o,s,e;function c(e){try{var t={supportTests:e,timestamp:(new Date).valueOf()};sessionStorage.setItem(o,JSON.stringify(t))}catch(e){}}function p(e,t,n){e.clearRect(0,0,e.canvas.width,e.canvas.height),e.fillText(t,0,0);var t=new Uint32Array(e.getImageData(0,0,e.canvas.width,e.canvas.height).data),r=(e.clearRect(0,0,e.canvas.width,e.canvas.height),e.fillText(n,0,0),new Uint32Array(e.getImageData(0,0,e.canvas.width,e.canvas.height).data));return t.every(function(e,t){return e===r[t]})}function u(e,t,n){switch(t){case"flag":return n(e,"🏳️‍⚧️","🏳️​⚧️")?!1:!n(e,"🇺🇳","🇺​🇳")&&!n(e,"🏴󠁧󠁢󠁥󠁮󠁧󠁿","🏴​󠁧​󠁢​󠁥​󠁮​󠁧​󠁿");case"emoji":return!n(e,"🐦‍⬛","🐦​⬛")}return!1}function f(e,t,n){var r="undefined"!=typeof WorkerGlobalScope&&self instanceof WorkerGlobalScope?new OffscreenCanvas(300,150):i.createElement("canvas"),a=r.getContext("2d",{willReadFrequently:!0}),o=(a.textBaseline="top",a.font="600 32px Arial",{});return e.forEach(function(e){o[e]=t(a,e,n)}),o}function t(e){var t=i.createElement("script");t.src=e,t.defer=!0,i.head.appendChild(t)}"undefined"!=typeof Promise&&(o="wpEmojiSettingsSupports",s=["flag","emoji"],n.supports={everything:!0,everythingExceptFlag:!0},e=new Promise(function(e){i.addEventListener("DOMContentLoaded",e,{once:!0})}),new Promise(function(t){var n=function(){try{var e=JSON.parse(sessionStorage.getItem(o));if("object"==typeof e&&"number"==typeof e.timestamp&&(new Date).valueOf()<e.timestamp+604800&&"object"==typeof e.supportTests)return e.supportTests}catch(e){}return null}();if(!n){if("undefined"!=typeof Worker&&"undefined"!=typeof OffscreenCanvas&&"undefined"!=typeof URL&&URL.createObjectURL&&"undefined"!=typeof Blob)try{var e="postMessage("+f.toString()+"("+[JSON.stringify(s),u.toString(),p.toString()].join(",")+"));",r=new Blob([e],{type:"text/javascript"}),a=new Worker(URL.createObjectURL(r),{name:"wpTestEmojiSupports"});return void(a.onmessage=function(e){c(n=e.data),a.terminate(),t(n)})}catch(e){}c(n=f(s,u,p))}t(n)}).then(function(e){for(var t in e)n.supports[t]=e[t],n.supports.everything=n.supports.everything&&n.supports[t],"flag"!==t&&(n.supports.everythingExceptFlag=n.supports.everythingExceptFlag&&n.supports[t]);n.supports.everythingExceptFlag=n.supports.everythingExceptFlag&&!n.supports.flag,n.DOMReady=!1,n.readyCallback=function(){n.DOMReady=!0}}).then(function(){return e}).then(function(){var e;n.supports.everything||(n.readyCallback(),(e=n.source||{}).concatemoji?t(e.concatemoji):e.wpemoji&&e.twemoji&&(t(e.twemoji),t(e.wpemoji)))}))}((window,document),window._wpemojiSettings);
/* ]]> */
</script>
<link rel="stylesheet" id="woo-layout-css" href="https://www.politepix.com/wp-content/themes/pixelpress/css/layout.css?ver=6.5.2" type="text/css" media="all">
<link rel="stylesheet" id="woocommerce-css" href="https://www.politepix.com/wp-content/themes/pixelpress/css/woocommerce.css?ver=6.5.2" type="text/css" media="all">
<style id="wp-emoji-styles-inline-css" type="text/css">img.wp-smiley, img.emoji {
		display: inline !important;
		border: none !important;
		box-shadow: none !important;
		height: 1em !important;
		width: 1em !important;
		margin: 0 0.07em !important;
		vertical-align: -0.1em !important;
		background: none !important;
		padding: 0 !important;
	}</style>
<link rel="stylesheet" id="wp-block-library-css" href="https://c0.wp.com/c/6.5.2/wp-includes/css/dist/block-library/style.min.css" type="text/css" media="all">
<style id="wp-block-library-inline-css" type="text/css">.has-text-align-justify{text-align:justify;}</style>
<link rel="stylesheet" id="mediaelement-css" href="https://c0.wp.com/c/6.5.2/wp-includes/js/mediaelement/mediaelementplayer-legacy.min.css" type="text/css" media="all">
<link rel="stylesheet" id="wp-mediaelement-css" href="https://c0.wp.com/c/6.5.2/wp-includes/js/mediaelement/wp-mediaelement.min.css" type="text/css" media="all">
<style id="jetpack-sharing-buttons-style-inline-css" type="text/css">.jetpack-sharing-buttons__services-list{display:flex;flex-direction:row;flex-wrap:wrap;gap:0;list-style-type:none;margin:5px;padding:0}.jetpack-sharing-buttons__services-list.has-small-icon-size{font-size:12px}.jetpack-sharing-buttons__services-list.has-normal-icon-size{font-size:16px}.jetpack-sharing-buttons__services-list.has-large-icon-size{font-size:24px}.jetpack-sharing-buttons__services-list.has-huge-icon-size{font-size:36px}@media print{.jetpack-sharing-buttons__services-list{display:none!important}}.editor-styles-wrapper .wp-block-jetpack-sharing-buttons{gap:0;padding-inline-start:0}ul.jetpack-sharing-buttons__services-list.has-background{padding:1.25em 2.375em}</style>
<style id="classic-theme-styles-inline-css" type="text/css">/*! This file is auto-generated */
.wp-block-button__link{color:#fff;background-color:#32373c;border-radius:9999px;box-shadow:none;text-decoration:none;padding:calc(.667em + 2px) calc(1.333em + 2px);font-size:1.125em}.wp-block-file__button{background:#32373c;color:#fff;text-decoration:none}</style>
<style id="global-styles-inline-css" type="text/css">body{--wp--preset--color--black: #000000;--wp--preset--color--cyan-bluish-gray: #abb8c3;--wp--preset--color--white: #ffffff;--wp--preset--color--pale-pink: #f78da7;--wp--preset--color--vivid-red: #cf2e2e;--wp--preset--color--luminous-vivid-orange: #ff6900;--wp--preset--color--luminous-vivid-amber: #fcb900;--wp--preset--color--light-green-cyan: #7bdcb5;--wp--preset--color--vivid-green-cyan: #00d084;--wp--preset--color--pale-cyan-blue: #8ed1fc;--wp--preset--color--vivid-cyan-blue: #0693e3;--wp--preset--color--vivid-purple: #9b51e0;--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple: linear-gradient(135deg,rgba(6,147,227,1) 0%,rgb(155,81,224) 100%);--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan: linear-gradient(135deg,rgb(122,220,180) 0%,rgb(0,208,130) 100%);--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange: linear-gradient(135deg,rgba(252,185,0,1) 0%,rgba(255,105,0,1) 100%);--wp--preset--gradient--luminous-vivid-orange-to-vivid-red: linear-gradient(135deg,rgba(255,105,0,1) 0%,rgb(207,46,46) 100%);--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray: linear-gradient(135deg,rgb(238,238,238) 0%,rgb(169,184,195) 100%);--wp--preset--gradient--cool-to-warm-spectrum: linear-gradient(135deg,rgb(74,234,220) 0%,rgb(151,120,209) 20%,rgb(207,42,186) 40%,rgb(238,44,130) 60%,rgb(251,105,98) 80%,rgb(254,248,76) 100%);--wp--preset--gradient--blush-light-purple: linear-gradient(135deg,rgb(255,206,236) 0%,rgb(152,150,240) 100%);--wp--preset--gradient--blush-bordeaux: linear-gradient(135deg,rgb(254,205,165) 0%,rgb(254,45,45) 50%,rgb(107,0,62) 100%);--wp--preset--gradient--luminous-dusk: linear-gradient(135deg,rgb(255,203,112) 0%,rgb(199,81,192) 50%,rgb(65,88,208) 100%);--wp--preset--gradient--pale-ocean: linear-gradient(135deg,rgb(255,245,203) 0%,rgb(182,227,212) 50%,rgb(51,167,181) 100%);--wp--preset--gradient--electric-grass: linear-gradient(135deg,rgb(202,248,128) 0%,rgb(113,206,126) 100%);--wp--preset--gradient--midnight: linear-gradient(135deg,rgb(2,3,129) 0%,rgb(40,116,252) 100%);--wp--preset--font-size--small: 13px;--wp--preset--font-size--medium: 20px;--wp--preset--font-size--large: 36px;--wp--preset--font-size--x-large: 42px;--wp--preset--spacing--20: 0.44rem;--wp--preset--spacing--30: 0.67rem;--wp--preset--spacing--40: 1rem;--wp--preset--spacing--50: 1.5rem;--wp--preset--spacing--60: 2.25rem;--wp--preset--spacing--70: 3.38rem;--wp--preset--spacing--80: 5.06rem;--wp--preset--shadow--natural: 6px 6px 9px rgba(0, 0, 0, 0.2);--wp--preset--shadow--deep: 12px 12px 50px rgba(0, 0, 0, 0.4);--wp--preset--shadow--sharp: 6px 6px 0px rgba(0, 0, 0, 0.2);--wp--preset--shadow--outlined: 6px 6px 0px -3px rgba(255, 255, 255, 1), 6px 6px rgba(0, 0, 0, 1);--wp--preset--shadow--crisp: 6px 6px 0px rgba(0, 0, 0, 1);}:where(.is-layout-flex){gap: 0.5em;}:where(.is-layout-grid){gap: 0.5em;}body .is-layout-flow > .alignleft{float: left;margin-inline-start: 0;margin-inline-end: 2em;}body .is-layout-flow > .alignright{float: right;margin-inline-start: 2em;margin-inline-end: 0;}body .is-layout-flow > .aligncenter{margin-left: auto !important;margin-right: auto !important;}body .is-layout-constrained > .alignleft{float: left;margin-inline-start: 0;margin-inline-end: 2em;}body .is-layout-constrained > .alignright{float: right;margin-inline-start: 2em;margin-inline-end: 0;}body .is-layout-constrained > .aligncenter{margin-left: auto !important;margin-right: auto !important;}body .is-layout-constrained > :where(:not(.alignleft):not(.alignright):not(.alignfull)){max-width: var(--wp--style--global--content-size);margin-left: auto !important;margin-right: auto !important;}body .is-layout-constrained > .alignwide{max-width: var(--wp--style--global--wide-size);}body .is-layout-flex{display: flex;}body .is-layout-flex{flex-wrap: wrap;align-items: center;}body .is-layout-flex > *{margin: 0;}body .is-layout-grid{display: grid;}body .is-layout-grid > *{margin: 0;}:where(.wp-block-columns.is-layout-flex){gap: 2em;}:where(.wp-block-columns.is-layout-grid){gap: 2em;}:where(.wp-block-post-template.is-layout-flex){gap: 1.25em;}:where(.wp-block-post-template.is-layout-grid){gap: 1.25em;}.has-black-color{color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-color{color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-color{color: var(--wp--preset--color--white) !important;}.has-pale-pink-color{color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-color{color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-color{color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-color{color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-color{color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-color{color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-color{color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-color{color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-color{color: var(--wp--preset--color--vivid-purple) !important;}.has-black-background-color{background-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-background-color{background-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-background-color{background-color: var(--wp--preset--color--white) !important;}.has-pale-pink-background-color{background-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-background-color{background-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-background-color{background-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-background-color{background-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-background-color{background-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-background-color{background-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-background-color{background-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-background-color{background-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-background-color{background-color: var(--wp--preset--color--vivid-purple) !important;}.has-black-border-color{border-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-border-color{border-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-border-color{border-color: var(--wp--preset--color--white) !important;}.has-pale-pink-border-color{border-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-border-color{border-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-border-color{border-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-border-color{border-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-border-color{border-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-border-color{border-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-border-color{border-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-border-color{border-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-border-color{border-color: var(--wp--preset--color--vivid-purple) !important;}.has-vivid-cyan-blue-to-vivid-purple-gradient-background{background: var(--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple) !important;}.has-light-green-cyan-to-vivid-green-cyan-gradient-background{background: var(--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan) !important;}.has-luminous-vivid-amber-to-luminous-vivid-orange-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange) !important;}.has-luminous-vivid-orange-to-vivid-red-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-orange-to-vivid-red) !important;}.has-very-light-gray-to-cyan-bluish-gray-gradient-background{background: var(--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray) !important;}.has-cool-to-warm-spectrum-gradient-background{background: var(--wp--preset--gradient--cool-to-warm-spectrum) !important;}.has-blush-light-purple-gradient-background{background: var(--wp--preset--gradient--blush-light-purple) !important;}.has-blush-bordeaux-gradient-background{background: var(--wp--preset--gradient--blush-bordeaux) !important;}.has-luminous-dusk-gradient-background{background: var(--wp--preset--gradient--luminous-dusk) !important;}.has-pale-ocean-gradient-background{background: var(--wp--preset--gradient--pale-ocean) !important;}.has-electric-grass-gradient-background{background: var(--wp--preset--gradient--electric-grass) !important;}.has-midnight-gradient-background{background: var(--wp--preset--gradient--midnight) !important;}.has-small-font-size{font-size: var(--wp--preset--font-size--small) !important;}.has-medium-font-size{font-size: var(--wp--preset--font-size--medium) !important;}.has-large-font-size{font-size: var(--wp--preset--font-size--large) !important;}.has-x-large-font-size{font-size: var(--wp--preset--font-size--x-large) !important;}
.wp-block-navigation a:where(:not(.wp-element-button)){color: inherit;}
:where(.wp-block-post-template.is-layout-flex){gap: 1.25em;}:where(.wp-block-post-template.is-layout-grid){gap: 1.25em;}
:where(.wp-block-columns.is-layout-flex){gap: 2em;}:where(.wp-block-columns.is-layout-grid){gap: 2em;}
.wp-block-pullquote{font-size: 1.5em;line-height: 1.6;}</style>
<link rel="stylesheet" id="bbp-default-css" href="https://www.politepix.com/wp-content/plugins/bbpress/templates/default/css/bbpress.min.css?ver=2.6.9" type="text/css" media="all">
<link rel="stylesheet" id="currency_converter_styles-css" href="https://www.politepix.com/wp-content/plugins/woocommerce-currency-converter-widget/assets/css/converter.css?ver=2.2.2" type="text/css" media="all">
<style id="woocommerce-inline-inline-css" type="text/css">.woocommerce form .form-row .required { visibility: visible; }</style>
<link rel="stylesheet" id="cmplz-general-css" href="https://www.politepix.com/wp-content/plugins/complianz-gdpr/assets/css/cookieblocker.min.css?ver=1710283454" type="text/css" media="all">
<link rel="stylesheet" id="jetpack_css-css" href="https://c0.wp.com/p/jetpack/13.3.1/css/jetpack.css" type="text/css" media="all">
<link rel="stylesheet" id="prettyPhoto-css" href="https://www.politepix.com/wp-content/themes/pixelpress/includes/css/prettyPhoto.css?ver=6.5.2" type="text/css" media="all">
<script type="text/template" id="tmpl-variation-template">
	<div class="woocommerce-variation-description">{{{ data.variation.variation_description }}}<\/div>
	<div class="woocommerce-variation-price">{{{ data.variation.price_html }}}<\/div>
	<div class="woocommerce-variation-availability">{{{ data.variation.availability_html }}}<\/div>
</script>
<script type="text/template" id="tmpl-unavailable-variation-template">
	<p>Sorry, this product is unavailable. Please choose a different combination.<\/p>
</script>
<script type="text/javascript" src="https://c0.wp.com/c/6.5.2/wp-includes/js/dist/vendor/wp-polyfill-inert.min.js" id="wp-polyfill-inert-js"></script>
<script type="text/javascript" src="https://c0.wp.com/c/6.5.2/wp-includes/js/dist/vendor/regenerator-runtime.min.js" id="regenerator-runtime-js"></script>
<script type="text/javascript" src="https://c0.wp.com/c/6.5.2/wp-includes/js/dist/vendor/wp-polyfill.min.js" id="wp-polyfill-js"></script>
<script type="text/javascript" src="https://c0.wp.com/c/6.5.2/wp-includes/js/dist/hooks.min.js" id="wp-hooks-js"></script>
<script data-service="jetpack-statistics" data-category="statistics" type="text/plain" data-cmplz-src="https://stats.wp.com/w.js?ver=202417" id="woo-tracks-js"></script>
<script type="text/javascript" src="https://c0.wp.com/c/6.5.2/wp-includes/js/jquery/jquery.min.js" id="jquery-core-js"></script>
<script type="text/javascript" src="https://c0.wp.com/c/6.5.2/wp-includes/js/jquery/jquery-migrate.min.js" id="jquery-migrate-js"></script>
<script type="text/javascript" src="https://c0.wp.com/c/6.5.2/wp-includes/js/underscore.min.js" id="underscore-js"></script>
<script type="text/javascript" id="wp-util-js-extra">
/* <![CDATA[ */
var _wpUtilSettings = {"ajax":{"url":"\/wp-admin\/admin-ajax.php"}};
/* ]]> */
</script>
<script type="text/javascript" src="https://c0.wp.com/c/6.5.2/wp-includes/js/wp-util.min.js" id="wp-util-js"></script>
<script type="text/javascript" src="https://c0.wp.com/p/woocommerce/8.8.2/assets/js/jquery-blockui/jquery.blockUI.min.js" id="jquery-blockui-js" defer data-wp-strategy="defer"></script>
<script type="text/javascript" id="wc-add-to-cart-variation-js-extra">
/* <![CDATA[ */
var wc_add_to_cart_variation_params = {"wc_ajax_url":"\/?wc-ajax=%%endpoint%%","i18n_no_matching_variations_text":"Sorry, no products matched your selection. Please choose a different combination.","i18n_make_a_selection_text":"Please select some product options before adding this product to your cart.","i18n_unavailable_text":"Sorry, this product is unavailable. Please choose a different combination."};
/* ]]> */
</script>
<script type="text/javascript" src="https://c0.wp.com/p/woocommerce/8.8.2/assets/js/frontend/add-to-cart-variation.min.js" id="wc-add-to-cart-variation-js" defer data-wp-strategy="defer"></script>
<script type="text/javascript" src="https://c0.wp.com/p/woocommerce/8.8.2/assets/js/jquery-cookie/jquery.cookie.min.js" id="jquery-cookie-js" defer data-wp-strategy="defer"></script>
<script data-service="jetpack-statistics" data-category="statistics" type="text/plain" data-cmplz-src="https://stats.wp.com/s-202417.js" id="woocommerce-analytics-js" defer data-wp-strategy="defer"></script>
<script type="text/javascript" src="https://www.politepix.com/wp-content/themes/pixelpress/includes/js/third-party.js?ver=6.5.2" id="third party-js"></script>
<script type="text/javascript" src="https://c0.wp.com/p/woocommerce/8.8.2/assets/js/prettyPhoto/jquery.prettyPhoto.min.js" id="prettyPhoto-js" data-wp-strategy="defer"></script>
<script type="text/javascript" src="https://www.politepix.com/wp-content/themes/pixelpress/includes/js/general.js?ver=6.5.2" id="general-js"></script>
<script type="text/javascript" src="https://www.politepix.com/wp-content/themes/pixelpress/includes/js/uniform.js?ver=6.5.2" id="uniform-js"></script>
<link rel="https://api.w.org/" href="/wp-json/">
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="/xmlrpc.php?rsd">
<meta name="generator" content="WordPress 6.5.2">
<meta name="generator" content="WooCommerce 8.8.2">
<link rel="canonical" href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/">
<link rel="shortlink" href="/?p=1029883">
<link rel="alternate" type="application/json+oembed" href="/wp-json/oembed/1.0/embed?url=https%3A%2F%2F%2Fforums%2Ftopic%2Frapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks%2F">
<link rel="alternate" type="text/xml+oembed" href="/wp-json/oembed/1.0/embed?url=https%3A%2F%2F%2Fforums%2Ftopic%2Frapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks%2F#038;format=xml">
	<style>img#wpstats{display:none}</style>
					<style>.cmplz-hidden {
					display: none !important;
				}</style>		<script>( function() {
				window.onpageshow = function( event ) {
					// Defined window.wpforms means that a form exists on a page.
					// If so and back/forward button has been clicked,
					// force reload a page to prevent the submit button state stuck.
					if ( typeof window.wpforms !== 'undefined' && event.persisted ) {
						window.location.reload();
					}
				};
			}() );</script>
		
<!-- Theme version -->
<meta name="generator" content="PixelPress Politepix 1.0">
<meta name="generator" content="PixelPress 1.5.4">
<meta name="generator" content="WooFramework 6.2.9">

<!-- Always force latest IE rendering engine (even in intranet) & Chrome Frame -->
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

<!--  Mobile viewport scale | Disable user zooming as the layout is optimised -->
<meta content="initial-scale=1.0; maximum-scale=1.0; user-scalable=no" name="viewport">
		<!--[if lt IE 9]>
			<script src="https://html5shim.googlecode.com/svn/trunk/html5.js"></script>
		<![endif]-->
			<noscript><style>.woocommerce-product-gallery{ opacity: 1 !important; }</style></noscript>
	
<!-- Google Webfonts -->
<link href="https://www.politepix.com/wp-content/uploads/omgf/omgf-stylesheet-165/omgf-stylesheet-165.css?ver=1668781124" rel="stylesheet" type="text/css">

<!-- Alt Stylesheet -->
<link href="https://www.politepix.com/wp-content/themes/pixelpress/styles/default.css" rel="stylesheet" type="text/css">

<!-- Custom Favicon -->
<link rel="shortcut icon" href="https://www.politepix.com/wp-content/uploads/favicon.png">

<!-- Woo Shortcodes CSS -->
<link href="https://www.politepix.com/wp-content/themes/pixelpress/functions/css/shortcodes.css" rel="stylesheet" type="text/css">

<!-- Custom Stylesheet -->
<link href="https://www.politepix.com/wp-content/themes/pixelpress/custom.css" rel="stylesheet" type="text/css">

<!-- Custom Stylesheet In Child Theme -->
<link href="https://www.politepix.com/wp-content/themes/politepix-pixelpress-child-theme/custom.css" rel="stylesheet" type="text/css">

</head>

<body data-cmplz="1" class="topic bbpress no-js topic-template-default single single-topic postid-1029883 theme-pixelpress woocommerce-no-js unknown alt-style-default has-lightbox layout-left-content">

<div id="wrapper">

	    
        
    <div id="header-wrap">

		<header id="header" class="col-full">
		
			<div id="logo" class="fl">
												    <a id="logo" href="/" title="iOS Frameworks for speech recognition, text to speech and more">
				    	<img src="https://www.politepix.com/wp-content/uploads/logo1.png" alt="Politepix">
				    </a>
			    			    
			    <hgroup>
			        
					<h1 class="site-title"><a href="/">Politepix</a></h1>
					<h2 class="site-description">iOS Frameworks for speech recognition, text to speech and more</h2>
					<h3 class="nav-toggle"><a href="#navigation">Navigation</a></h3>
				      	
				</hgroup>
			</div>
<!-- /#logo -->		
	        
	        	
	        <div id="header-right" class="fr">

				<nav id="navigation" role="navigation">
					
					<ul id="main-nav" class="nav fl">
<li id="menu-item-1175" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-1175">
<a href="/openears/">OpenEars</a>
<ul class="sub-menu">
	<li id="menu-item-11108" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-11108"><a href="/openears/tutorial/">OpenEars Tutorials</a></li>
	<li id="menu-item-1171" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1171"><a href="/openears/support/">OpenEars FAQ/Support</a></li>
	<li id="menu-item-1189" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-1189"><a href="/forums/forum/openearsforum/">OpenEars Support Forum</a></li>
	<li id="menu-item-14947" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-14947"><a href="/openearsplatform/">About the OpenEars Platform</a></li>
</ul>
</li>
<li id="menu-item-11706" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-11706">
<a href="/neatspeech">NeatSpeech</a>
<ul class="sub-menu">
	<li id="menu-item-11707" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-11707"><a href="/forums/forum/openears-plugins/">NeatSpeech support forum</a></li>
</ul>
</li>
<li id="menu-item-9871" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-9871">
<a href="/rapidears/">RapidEars</a>
<ul class="sub-menu">
	<li id="menu-item-10389" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-10389"><a href="/forums/forum/openears-plugins">RapidEars Support Forum</a></li>
</ul>
</li>
<li id="menu-item-11030" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-11030">
<a href="/rejecto/">Rejecto</a>
<ul class="sub-menu">
	<li id="menu-item-11031" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-11031"><a href="/forums/forum/openears-plugins/">Rejecto Support Forum</a></li>
</ul>
</li>
<li id="menu-item-13071" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-13071"><a href="/savethatwave/">SaveThatWave</a></li>
<li id="menu-item-1021000" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-1021000">
<a href="/ruleorama/">RuleORama</a>
<ul class="sub-menu">
	<li id="menu-item-9002" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-9002"><a href="/forums/forum/openears-plugins/">RuleORama Free Support Forum</a></li>
</ul>
</li>
<li id="menu-item-1178" class="menu-item menu-item-type-post_type menu-item-object-page current_page_parent menu-item-1178"><a title="Blog" href="/blog/">Blog</a></li>
<li id="menu-item-1015870" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children menu-item-1015870">
<a href="/shop">Shop</a>
<ul class="sub-menu">
	<li id="menu-item-8499" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-8499"><a href="/shop">View all products</a></li>
	<li id="menu-item-8486" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-8486"><a href="/cart/">Cart</a></li>
	<li id="menu-item-8498" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-8498"><a href="/order-tracking/">Track your order</a></li>
	<li id="menu-item-8539" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-8539"><a href="/termsandconditions/">AGB/Terms and Conditions</a></li>
</ul>
</li>
</ul>			        
			        			        <ul class="mini-cart">
						<li>
							<a href="/cart/" title="View your shopping cart" class="cart-parent">
								<span> 
							<span class="woocommerce-Price-amount amount"><bdi><span class="woocommerce-Price-currencySymbol">&euro;</span>0.00</bdi></span><mark>0</mark>							</span>
							</a>
							<ul class="cart_list"><li class="empty">No products in the cart.</li></ul>						</li>
					</ul>
			      				      	
				</nav><!-- /#navigation -->

				
<div id="header-social" class="social fr">
				<a href="http://feeds.feedburner.com/politepixblog" class="subscribe" title="RSS"></a>

				<a href="http://www.twitter.com/politepix" class="twitter" title="Follow me on Twitter"></a>

		</div>
<!-- /.social -->
				

			</div>
<!-- /#header-right -->
			
					
		</header><!-- /#header -->
	
	</div>
<!-- /#header-wrap -->
	
	
	       
    <div id="content" class="page col-full">
    
    	    	
		<section id="main" class="col-left"> 			

                                                                   
            <article class="post-1029883 topic type-topic status-publish hentry topic-tag-memory-leaks-startrealtimelisteningwithlanguagemodelatpath">
				
				<header>
			    	<h1>RapidEars startRealtimeListeningWithLanguageModelAtPath memory leaks</h1>
				</header>
				
                <section class="entry">
                	
<div id="bbpress-forums" class="bbpress-wrapper">

	<div class="bbp-breadcrumb"><p><a href="/" class="bbp-breadcrumb-home">Home</a> <span class="bbp-breadcrumb-sep">&rsaquo;</span> <a href="/forums/" class="bbp-breadcrumb-root">Forums</a> <span class="bbp-breadcrumb-sep">&rsaquo;</span> <a href="/forums/forum/openears-plugins/" class="bbp-breadcrumb-forum">OpenEars plugins</a> <span class="bbp-breadcrumb-sep">&rsaquo;</span> <span class="bbp-breadcrumb-current">RapidEars startRealtimeListeningWithLanguageModelAtPath memory leaks</span></p></div>
	
	
	
	
		<div class="bbp-topic-tags"><p>Tagged:&nbsp;<a href="/forums/tags/memory-leaks-startrealtimelisteningwithlanguagemodelatpath/" rel="tag">memory leaks startRealtimeListeningWithLanguageModelAtPath</a></p></div>
		<div class="bbp-template-notice info"><ul><li class="bbp-topic-description">This topic has 50 replies, 2 voices, and was last updated <a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1030563" title="Reply To: RapidEars startRealtimeListeningWithLanguageModelAtPath memory leaks">7 years, 10 months ago</a> by <a href="/forums/profile/laurent-oscadi/" title="View Laurent&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Laurent</span></a>.</li></ul></div>
		
		
			
<div class="bbp-pagination">
	<div class="bbp-pagination-count">Viewing 51 posts - 1 through 51 (of 51 total)</div>
	<div class="bbp-pagination-links"></div>
</div>


			<div class="burma">Advertisement: <a href="/rejecto">&ldquo;Rejecto is a plugin for OpenEars&#8482; and RapidEars that lets you ignore speech that isn't in your vocabulary!&rdquo;</a>
</div>
<p>
</p>
<ul id="topic-1029883-replies" class="forums bbp-replies">

	<li class="bbp-header">
		<div class="bbp-reply-author">Author</div>
<!-- .bbp-reply-author -->
		<div class="bbp-reply-content">Posts</div>
<!-- .bbp-reply-content -->
	</li>
<!-- .bbp-header -->

	<li class="bbp-body">

		
			
				
<div id="post-1029883" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">March 31, 2016 at 8:15 am</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1029883" class="bbp-reply-permalink">#1029883</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1029883 -->

<div class="loop-item-0 user-id-2022 bbp-parent-forum-9992 bbp-parent-topic-9992 bbp-reply-position-1 odd  post-1029883 topic type-topic status-publish hentry topic-tag-memory-leaks-startrealtimelisteningwithlanguagemodelatpath">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/laurent-oscadi/" title="View Laurent&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Laurent</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Hello,<br>
I&#8217;m working with openEars, rapidEars and rejecto.<br>
I&#8217;m on Xcode 7.3 and iOS 9.2.1 </p>
<p>In my app, when I start to listen to voice with rapidears and rejecto, my app crashes after 2 hours of listening because of Message from debugger: Terminated due to memory issue.<br>
In order to debug, I have profiled my app with instruments of Xcode.<br>
And I could see that when I launched startRealtimeListeningWithLanguageModelAtPath function,I had memory leaks.</p>
<p>Could you help me please to find a solution to this problem.</p>
<p>Regards,<br>
Laurent.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1029887" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">March 31, 2016 at 12:46 pm</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1029887" class="bbp-reply-permalink">#1029887</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1029887 -->

<div class="loop-item-1 user-id-2022 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-2 even topic-author  post-1029887 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/laurent-oscadi/" title="View Laurent&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Laurent</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Besides, the OpeanEarsSample app included in the OpenEars distribution has memory leaks ( tested with Xcode 7.3 and iOS 9.2.1 and instruments (leaks) of Xcode ) </p>
<p>Regards,<br>
Laurent</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1029896" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 1, 2016 at 6:20 pm</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1029896" class="bbp-reply-permalink">#1029896</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1029896 -->

<div class="loop-item-2 user-id-4 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-3 odd  post-1029896 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/halle-2/" title="View Halle Winkler&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Halle Winkler</span></a><div class="bbp-author-role">Politepix</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Welcome Laurent,</p>
<p>Pocketsphinx and Flite have some little leaks (not big enough to create a problem even over weeks of use IIRC) so the sample app always shows a few leaks. But it&#8217;s possible that there could be a new leak introduced with 2.5 – if you&#8217;d like me to check it out, I&#8217;d need to know what you saw in Instruments that makes you think it originates with OpenEars, which means the names of the objects and their object sizes that Instruments reports as leaking, thanks. Best is to get a report like &#8220;after one minute of operation which included starting listening and stopping listening, Leaks reported a leaked object named __ of size __ and the total size of the leaks for object __ over that time period was ___MB.&#8221; And let me know what you did in that timeframe so I can attempt to replicate it and see the same thing.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1029927" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 5, 2016 at 6:02 am</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1029927" class="bbp-reply-permalink">#1029927</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1029927 -->

<div class="loop-item-3 user-id-2022 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-4 even topic-author  post-1029927 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/laurent-oscadi/" title="View Laurent&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Laurent</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Hello Mr Halle,</p>
<p>I don’t understand why the OpenEarsSampleApp memory increases over time. I launched the sampleApp. And then there are some little leaks like you said. But if we pay attention to the Persistent Bytes All Heap Allocations, we can notice that it is increasing over time. Event when I click on the stop listening button it still growth<br>
Do you know why?<br>
Regards,<br>
Laurent</p>
<p>I’m using rejecto and rapidEars.</p>
<p>this is my OpenEarsSampleApp code :<br>
[spoiler]<br>
//  ViewController.m<br>
//  OpenEarsSampleApp<br>
//<br>
//  ViewController.m demonstrates the use of the OpenEars framework.<br>
//<br>
//  Copyright Politepix UG (haftungsbeschränkt) 2014. All rights reserved.<br>
//  <a href="/">https://www.politepix.com</a><br>
//  Contact at <a href="/contact">https://www.politepix.com/contact</a><br>
//<br>
//  This file is licensed under the Politepix Shared Source license found in the root of the source distribution.</p>
<p>// **************************************************************************************************************************************************************<br>
// **************************************************************************************************************************************************************<br>
// **************************************************************************************************************************************************************<br>
// IMPORTANT NOTE: Audio driver and hardware behavior is completely different between the Simulator and a real device. It is not informative to test OpenEars&#8217; accuracy on the Simulator, and please do not report Simulator-only bugs since I only actively support<br>
// the device driver. Please only do testing/bug reporting based on results on a real device such as an iPhone or iPod Touch. Thanks!<br>
// **************************************************************************************************************************************************************<br>
// **************************************************************************************************************************************************************<br>
// **************************************************************************************************************************************************************</p>
<p>#import &#8220;ViewController.h&#8221;<br>
#import &lt;OpenEars/OEPocketsphinxController.h&gt;<br>
#import &lt;OpenEars/OEFliteController.h&gt;<br>
#import &lt;OpenEars/OELanguageModelGenerator.h&gt;<br>
#import &lt;OpenEars/OELogging.h&gt;<br>
#import &lt;OpenEars/OEAcousticModel.h&gt;<br>
#import &lt;Slt/Slt.h&gt;<br>
#import &lt;RapidEars/OEEventsObserver+RapidEars.h&gt;<br>
#import &lt;Rejecto/OELanguageModelGenerator+Rejecto.h&gt;<br>
#import &lt;RapidEars/OEPocketsphinxController+RapidEars.h&gt;<br>
#import &lt;RapidEars/OEEventsObserver+RapidEars.h&gt;</p>
<p>@interface ViewController()</p>
<p>// UI actions, not specifically related to OpenEars other than the fact that they invoke OpenEars methods.<br>
&#8211; (IBAction) stopButtonAction;<br>
&#8211; (IBAction) startButtonAction;<br>
&#8211; (IBAction) suspendListeningButtonAction;<br>
&#8211; (IBAction) resumeListeningButtonAction;</p>
<p>// Example for reading out the input audio levels without locking the UI using an NSTimer</p>
<p>&#8211; (void) startDisplayingLevels;<br>
&#8211; (void) stopDisplayingLevels;</p>
<p>// These three are the important OpenEars objects that this class demonstrates the use of.<br>
@property (nonatomic, strong) Slt *slt;</p>
<p>@property (nonatomic, strong) OEEventsObserver *openEarsEventsObserver;<br>
@property (nonatomic, strong) OEPocketsphinxController *pocketsphinxController;<br>
@property (nonatomic, strong) OEFliteController *fliteController;</p>
<p>// Some UI, not specifically related to OpenEars.<br>
@property (nonatomic, strong) IBOutlet UIButton *stopButton;<br>
@property (nonatomic, strong) IBOutlet UIButton *startButton;<br>
@property (nonatomic, strong) IBOutlet UIButton *suspendListeningButton;<br>
@property (nonatomic, strong) IBOutlet UIButton *resumeListeningButton;<br>
@property (nonatomic, strong) IBOutlet UITextView *statusTextView;<br>
@property (nonatomic, strong) IBOutlet UITextView *heardTextView;<br>
@property (nonatomic, strong) IBOutlet UILabel *pocketsphinxDbLabel;<br>
@property (nonatomic, strong) IBOutlet UILabel *fliteDbLabel;<br>
@property (nonatomic, assign) BOOL usingStartingLanguageModel;<br>
@property (nonatomic, assign) int restartAttemptsDueToPermissionRequests;<br>
@property (nonatomic, assign) BOOL startupFailedDueToLackOfPermissions;</p>
<p>// Things which help us show off the dynamic language features.<br>
@property (nonatomic, copy) NSString *pathToFirstDynamicallyGeneratedLanguageModel;<br>
@property (nonatomic, copy) NSString *pathToFirstDynamicallyGeneratedDictionary;<br>
@property (nonatomic, copy) NSString *pathToSecondDynamicallyGeneratedLanguageModel;<br>
@property (nonatomic, copy) NSString *pathToSecondDynamicallyGeneratedDictionary;</p>
<p>// Our NSTimer that will help us read and display the input and output levels without locking the UI<br>
@property (nonatomic, strong) 	NSTimer *uiUpdateTimer;</p>
<p>@end</p>
<p>@implementation ViewController</p>
<p>#define kLevelUpdatesPerSecond 18 // We&#8217;ll have the ui update 18 times a second to show some fluidity without hitting the CPU too hard.</p>
<p>//#define kGetNbest // Uncomment this if you want to try out nbest<br>
#pragma mark &#8211;<br>
#pragma mark Memory Management</p>
<p>&#8211; (void)dealloc {<br>
    [self stopDisplayingLevels];<br>
}</p>
<p>#pragma mark &#8211;<br>
#pragma mark View Lifecycle</p>
<p>&#8211; (void)viewDidLoad {<br>
    [super viewDidLoad];<br>
    self.fliteController = [[OEFliteController alloc] init];<br>
    self.openEarsEventsObserver = [[OEEventsObserver alloc] init];<br>
    self.openEarsEventsObserver.delegate = self;<br>
    self.slt = [[Slt alloc] init];</p>
<p>    self.restartAttemptsDueToPermissionRequests = 0;<br>
    self.startupFailedDueToLackOfPermissions = FALSE;</p>
<p>//     [OELogging startOpenEarsLogging]; // Uncomment me for OELogging, which is verbose logging about internal OpenEars operations such as audio settings. If you have issues, show this logging in the forums.<br>
//    [OEPocketsphinxController sharedInstance].verbosePocketSphinx = TRUE; // Uncomment this for much more verbose speech recognition engine output. If you have issues, show this logging in the forums.</p>
<p>    [self.openEarsEventsObserver setDelegate:self]; // Make this class the delegate of OpenEarsObserver so we can get all of the messages about what OpenEars is doing.</p>
<p>    [[OEPocketsphinxController sharedInstance] setActive:TRUE error:nil]; // Call this before setting any OEPocketsphinxController characteristics</p>
<p>    // This is the language model we&#8217;re going to start up with. The only reason I&#8217;m making it a class property is that I reuse it a bunch of times in this example,<br>
    // but you can pass the string contents directly to OEPocketsphinxController:startListeningWithLanguageModelAtPath:dictionaryAtPath:languageModelIsJSGF:</p>
<p>    NSArray *firstLanguageArray = @[@&#8221;BACKWARD&#8221;,<br>
                                    @&#8221;CHANGE&#8221;,<br>
                                    @&#8221;FORWARD&#8221;,<br>
                                    @&#8221;GO&#8221;,<br>
                                    @&#8221;LEFT&#8221;,<br>
                                    @&#8221;MODEL&#8221;,<br>
                                    @&#8221;RIGHT&#8221;,<br>
                                    @&#8221;TURN&#8221;];</p>
<p>    OELanguageModelGenerator *languageModelGenerator = [[OELanguageModelGenerator alloc] init]; </p>
<p>    // languageModelGenerator.verboseLanguageModelGenerator = TRUE; // Uncomment me for verbose language model generator debug output.</p>
<p>//    NSError *error = [languageModelGenerator generateLanguageModelFromArray:firstLanguageArray withFilesNamed:@&#8221;FirstOpenEarsDynamicLanguageModel&#8221; forAcousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;]]; // Change &#8220;AcousticModelEnglish&#8221; to &#8220;AcousticModelSpanish&#8221; in order to create a language model for Spanish recognition instead of English.</p>
<p>    NSError *error = [languageModelGenerator generateRejectingLanguageModelFromArray:firstLanguageArray<br>
                                                         withFilesNamed:@&#8221;FirstOpenEarsDynamicLanguageModel&#8221;<br>
                                                 withOptionalExclusions:nil<br>
                                                        usingVowelsOnly:FALSE<br>
                                                             withWeight:[ NSNumber numberWithFloat:2.0 ]//nil<br>
                                                 forAcousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;]];<br>
    if(error) {<br>
        NSLog(@&#8221;Dynamic language generator reported error %@&#8221;, [error description]);<br>
    } else {<br>
        self.pathToFirstDynamicallyGeneratedLanguageModel = [languageModelGenerator pathToSuccessfullyGeneratedLanguageModelWithRequestedName:@&#8221;FirstOpenEarsDynamicLanguageModel&#8221;];<br>
        self.pathToFirstDynamicallyGeneratedDictionary = [languageModelGenerator pathToSuccessfullyGeneratedDictionaryWithRequestedName:@&#8221;FirstOpenEarsDynamicLanguageModel&#8221;];<br>
    }</p>
<p>    self.usingStartingLanguageModel = TRUE; // This is not an OpenEars thing, this is just so I can switch back and forth between the two models in this sample app.</p>
<p>    // Here is an example of dynamically creating an in-app grammar.</p>
<p>    // We want it to be able to response to the speech &#8220;CHANGE MODEL&#8221; and a few other things.  Items we want to have recognized as a whole phrase (like &#8220;CHANGE MODEL&#8221;)<br>
    // we put into the array as one string (e.g. &#8220;CHANGE MODEL&#8221; instead of &#8220;CHANGE&#8221; and &#8220;MODEL&#8221;). This increases the probability that they will be recognized as a phrase. This works even better starting with version 1.0 of OpenEars.</p>
<p>    NSArray *secondLanguageArray = @[@&#8221;SUNDAY&#8221;,<br>
                                     @&#8221;MONDAY&#8221;,<br>
                                     @&#8221;TUESDAY&#8221;,<br>
                                     @&#8221;WEDNESDAY&#8221;,<br>
                                     @&#8221;THURSDAY&#8221;,<br>
                                     @&#8221;FRIDAY&#8221;,<br>
                                     @&#8221;SATURDAY&#8221;,<br>
                                     @&#8221;QUIDNUNC&#8221;,<br>
                                     @&#8221;CHANGE MODEL&#8221;];</p>
<p>    // The last entry, quidnunc, is an example of a word which will not be found in the lookup dictionary and will be passed to the fallback method. The fallback method is slower,<br>
    // so, for instance, creating a new language model from dictionary words will be pretty fast, but a model that has a lot of unusual names in it or invented/rare/recent-slang<br>
    // words will be slower to generate. You can use this information to give your users good UI feedback about what the expectations for wait times should be.</p>
<p>    // I don&#8217;t think it&#8217;s beneficial to lazily instantiate OELanguageModelGenerator because you only need to give it a single message and then release it.<br>
    // If you need to create a very large model or any size of model that has many unusual words that have to make use of the fallback generation method,<br>
    // you will want to run this on a background thread so you can give the user some UI feedback that the task is in progress.</p>
<p>    // generateLanguageModelFromArray:withFilesNamed returns an NSError which will either have a value of noErr if everything went fine or a specific error if it didn&#8217;t.<br>
    error = [languageModelGenerator generateRejectingLanguageModelFromArray:secondLanguageArray<br>
                                                                      withFilesNamed:@&#8221;SecondOpenEarsDynamicLanguageModel&#8221;<br>
                                                              withOptionalExclusions:nil<br>
                                                                     usingVowelsOnly:FALSE<br>
                                                                          withWeight:[ NSNumber numberWithFloat:2.0 ]//nil<br>
                                                              forAcousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;]];<br>
//    error = [languageModelGenerator generateLanguageModelFromArray:secondLanguageArray withFilesNamed:@&#8221;SecondOpenEarsDynamicLanguageModel&#8221; forAcousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;]]; // Change &#8220;AcousticModelEnglish&#8221; to &#8220;AcousticModelSpanish&#8221; in order to create a language model for Spanish recognition instead of English.</p>
<p>    //    NSError *error = [languageModelGenerator generateLanguageModelFromTextFile:[NSString stringWithFormat:@&#8221;%@/%@&#8221;,[[NSBundle mainBundle] resourcePath], @&#8221;OpenEarsCorpus.txt&#8221;] withFilesNamed:@&#8221;SecondOpenEarsDynamicLanguageModel&#8221; forAcousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;]]; // Try this out to see how generating a language model from a corpus works.</p>
<p>    if(error) {<br>
        NSLog(@&#8221;Dynamic language generator reported error %@&#8221;, [error description]);<br>
    }	else {</p>
<p>        self.pathToSecondDynamicallyGeneratedLanguageModel = [languageModelGenerator pathToSuccessfullyGeneratedLanguageModelWithRequestedName:@&#8221;SecondOpenEarsDynamicLanguageModel&#8221;]; // We&#8217;ll set our new .languagemodel file to be the one to get switched to when the words &#8220;CHANGE MODEL&#8221; are recognized.<br>
        self.pathToSecondDynamicallyGeneratedDictionary = [languageModelGenerator pathToSuccessfullyGeneratedDictionaryWithRequestedName:@&#8221;SecondOpenEarsDynamicLanguageModel&#8221;];; // We&#8217;ll set our new dictionary to be the one to get switched to when the words &#8220;CHANGE MODEL&#8221; are recognized.</p>
<p>        // Next, an informative message.</p>
<p>        NSLog(@&#8221;\n\nWelcome to the OpenEars sample project. This project understands the words:\nBACKWARD,\nCHANGE,\nFORWARD,\nGO,\nLEFT,\nMODEL,\nRIGHT,\nTURN,\nand if you say \&#8221;CHANGE MODEL\&#8221; it will switch to its dynamically-generated model which understands the words:\nCHANGE,\nMODEL,\nMONDAY,\nTUESDAY,\nWEDNESDAY,\nTHURSDAY,\nFRIDAY,\nSATURDAY,\nSUNDAY,\nQUIDNUNC&#8221;);</p>
<p>        // This is how to start the continuous listening loop of an available instance of OEPocketsphinxController. We won&#8217;t do this if the language generation failed since it will be listening for a command to change over to the generated language.</p>
<p>        [[OEPocketsphinxController sharedInstance] setActive:TRUE error:nil]; // Call this once before setting properties of the OEPocketsphinxController instance.</p>
<p>        //   [OEPocketsphinxController sharedInstance].pathToTestFile = [[NSBundle mainBundle] pathForResource:@&#8221;change_model_short&#8221; ofType:@&#8221;wav&#8221;];  // This is how you could use a test WAV (mono/16-bit/16k) rather than live recognition. Don&#8217;t forget to add your WAV to your app bundle.</p>
<p>        if(![OEPocketsphinxController sharedInstance].isListening) {<br>
            [[OEPocketsphinxController sharedInstance] startRealtimeListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel<br>
                                                                                    dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary<br>
                                                                                 acousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;]];<br>
//            [[OEPocketsphinxController sharedInstance] startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;] languageModelIsJSGF:FALSE]; // Start speech recognition if we aren&#8217;t already listening.<br>
        }<br>
        // [self startDisplayingLevels] is not an OpenEars method, just a very simple approach for level reading<br>
        // that I&#8217;ve included with this sample app. My example implementation does make use of two OpenEars<br>
        // methods:	the pocketsphinxInputLevel method of OEPocketsphinxController and the fliteOutputLevel<br>
        // method of fliteController.<br>
        //<br>
        // The example is meant to show one way that you can read those levels continuously without locking the UI,<br>
        // by using an NSTimer, but the OpenEars level-reading methods<br>
        // themselves do not include multithreading code since I believe that you will want to design your own<br>
        // code approaches for level display that are tightly-integrated with your interaction design and the<br>
        // graphics API you choose. </p>
<p>        [self startDisplayingLevels];</p>
<p>        // Here is some UI stuff that has nothing specifically to do with OpenEars implementation<br>
        self.startButton.hidden = TRUE;<br>
        self.stopButton.hidden = TRUE;<br>
        self.suspendListeningButton.hidden = TRUE;<br>
        self.resumeListeningButton.hidden = TRUE;<br>
    }<br>
}</p>
<p>#pragma mark &#8211;<br>
#pragma mark OEEventsObserver delegate methods</p>
<p>// What follows are all of the delegate methods you can optionally use once you&#8217;ve instantiated an OEEventsObserver and set its delegate to self.<br>
// I&#8217;ve provided some pretty granular information about the exact phase of the Pocketsphinx listening loop, the Audio Session, and Flite, but I&#8217;d expect<br>
// that the ones that will really be needed by most projects are the following:<br>
//<br>
//- (void) pocketsphinxDidReceiveHypothesis:(NSString *)hypothesis recognitionScore:(NSString *)recognitionScore utteranceID:(NSString *)utteranceID;<br>
//- (void) audioSessionInterruptionDidBegin;<br>
//- (void) audioSessionInterruptionDidEnd;<br>
//- (void) audioRouteDidChangeToRoute:(NSString *)newRoute;<br>
//- (void) pocketsphinxDidStartListening;<br>
//- (void) pocketsphinxDidStopListening;<br>
//<br>
// It isn&#8217;t necessary to have a OEPocketsphinxController or a OEFliteController instantiated in order to use these methods.  If there isn&#8217;t anything instantiated that will<br>
// send messages to an OEEventsObserver, all that will happen is that these methods will never fire.  You also do not have to create a OEEventsObserver in<br>
// the same class or view controller in which you are doing things with a OEPocketsphinxController or OEFliteController; you can receive updates from those objects in<br>
// any class in which you instantiate an OEEventsObserver and set its delegate to self.</p>
<p>// This is an optional delegate method of OEEventsObserver which delivers the text of speech that Pocketsphinx heard and analyzed, along with its accuracy score and utterance ID.<br>
&#8211; (void) pocketsphinxDidReceiveHypothesis:(NSString *)hypothesis recognitionScore:(NSString *)recognitionScore utteranceID:(NSString *)utteranceID {</p>
<p>    NSLog(@&#8221;Local callback: The received hypothesis is %@ with a score of %@ and an ID of %@&#8221;, hypothesis, recognitionScore, utteranceID); // Log it.<br>
    if([hypothesis isEqualToString:@&#8221;CHANGE MODEL&#8221;]) { // If the user says &#8220;CHANGE MODEL&#8221;, we will switch to the alternate model (which happens to be the dynamically generated model).</p>
<p>        // Here is an example of language model switching in OpenEars. Deciding on what logical basis to switch models is your responsibility.<br>
        // For instance, when you call a customer service line and get a response tree that takes you through different options depending on what you say to it,<br>
        // the models are being switched as you progress through it so that only relevant choices can be understood. The construction of that logical branching and<br>
        // how to react to it is your job; OpenEars just lets you send the signal to switch the language model when you&#8217;ve decided it&#8217;s the right time to do so.</p>
<p>        if(self.usingStartingLanguageModel) { // If we&#8217;re on the starting model, switch to the dynamically generated one.</p>
<p>            [[OEPocketsphinxController sharedInstance] changeLanguageModelToFile:self.pathToSecondDynamicallyGeneratedLanguageModel withDictionary:self.pathToSecondDynamicallyGeneratedDictionary];<br>
            self.usingStartingLanguageModel = FALSE;</p>
<p>        } else { // If we&#8217;re on the dynamically generated model, switch to the start model (this is an example of a trigger and method for switching models).</p>
<p>            [[OEPocketsphinxController sharedInstance] changeLanguageModelToFile:self.pathToFirstDynamicallyGeneratedLanguageModel withDictionary:self.pathToFirstDynamicallyGeneratedDictionary];<br>
            self.usingStartingLanguageModel = TRUE;<br>
        }<br>
    }</p>
<p>    self.heardTextView.text = [NSString stringWithFormat:@&#8221;Heard: \&#8221;%@\&#8221;&#8221;, hypothesis]; // Show it in the status box.</p>
<p>    // This is how to use an available instance of OEFliteController. We&#8217;re going to repeat back the command that we heard with the voice we&#8217;ve chosen.<br>
    [self.fliteController say:[NSString stringWithFormat:@&#8221;You said %@&#8221;,hypothesis] withVoice:self.slt];<br>
}</p>
<p>#ifdef kGetNbest<br>
&#8211; (void) pocketsphinxDidReceiveNBestHypothesisArray:(NSArray *)hypothesisArray { // Pocketsphinx has an n-best hypothesis dictionary.<br>
    NSLog(@&#8221;Local callback:  hypothesisArray is %@&#8221;,hypothesisArray);<br>
}<br>
#endif<br>
// An optional delegate method of OEEventsObserver which informs that there was an interruption to the audio session (e.g. an incoming phone call).<br>
&#8211; (void) audioSessionInterruptionDidBegin {<br>
    NSLog(@&#8221;Local callback:  AudioSession interruption began.&#8221;); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: AudioSession interruption began.&#8221;; // Show it in the status box.<br>
    NSError *error = nil;<br>
    if([OEPocketsphinxController sharedInstance].isListening) {<br>
        error = [[OEPocketsphinxController sharedInstance] stopListening]; // React to it by telling Pocketsphinx to stop listening (if it is listening) since it will need to restart its loop after an interruption.<br>
        if(error) NSLog(@&#8221;Error while stopping listening in audioSessionInterruptionDidBegin: %@&#8221;, error);<br>
    }<br>
}</p>
<p>// An optional delegate method of OEEventsObserver which informs that the interruption to the audio session ended.<br>
&#8211; (void) audioSessionInterruptionDidEnd {<br>
    NSLog(@&#8221;Local callback:  AudioSession interruption ended.&#8221;); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: AudioSession interruption ended.&#8221;; // Show it in the status box.<br>
    // We&#8217;re restarting the previously-stopped listening loop.<br>
    if(![OEPocketsphinxController sharedInstance].isListening){<br>
//        [[OEPocketsphinxController sharedInstance] startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;] languageModelIsJSGF:FALSE]; // Start speech recognition if we aren&#8217;t currently listening.<br>
        [[OEPocketsphinxController sharedInstance] startRealtimeListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel<br>
                                                                                dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary<br>
                                                                             acousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;]];<br>
    }<br>
}</p>
<p>// An optional delegate method of OEEventsObserver which informs that the audio input became unavailable.<br>
&#8211; (void) audioInputDidBecomeUnavailable {<br>
    NSLog(@&#8221;Local callback:  The audio input has become unavailable&#8221;); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: The audio input has become unavailable&#8221;; // Show it in the status box.<br>
    NSError *error = nil;<br>
    if([OEPocketsphinxController sharedInstance].isListening){<br>
        error = [[OEPocketsphinxController sharedInstance] stopListening]; // React to it by telling Pocketsphinx to stop listening since there is no available input (but only if we are listening).<br>
        if(error) NSLog(@&#8221;Error while stopping listening in audioInputDidBecomeUnavailable: %@&#8221;, error);<br>
    }<br>
}</p>
<p>// An optional delegate method of OEEventsObserver which informs that the unavailable audio input became available again.<br>
&#8211; (void) audioInputDidBecomeAvailable {<br>
    NSLog(@&#8221;Local callback: The audio input is available&#8221;); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: The audio input is available&#8221;; // Show it in the status box.<br>
    if(![OEPocketsphinxController sharedInstance].isListening) {<br>
//        [[OEPocketsphinxController sharedInstance] startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;] languageModelIsJSGF:FALSE]; // Start speech recognition, but only if we aren&#8217;t already listening.<br>
        [[OEPocketsphinxController sharedInstance] startRealtimeListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel<br>
                                                                                dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary<br>
                                                                             acousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;]];<br>
    }<br>
}<br>
// An optional delegate method of OEEventsObserver which informs that there was a change to the audio route (e.g. headphones were plugged in or unplugged).<br>
&#8211; (void) audioRouteDidChangeToRoute:(NSString *)newRoute {<br>
    NSLog(@&#8221;Local callback: Audio route change. The new audio route is %@&#8221;, newRoute); // Log it.<br>
    self.statusTextView.text = [NSString stringWithFormat:@&#8221;Status: Audio route change. The new audio route is %@&#8221;,newRoute]; // Show it in the status box.</p>
<p>    NSError *error = [[OEPocketsphinxController sharedInstance] stopListening]; // React to it by telling the Pocketsphinx loop to shut down and then start listening again on the new route</p>
<p>    if(error)NSLog(@&#8221;Local callback: error while stopping listening in audioRouteDidChangeToRoute: %@&#8221;,error);</p>
<p>    if(![OEPocketsphinxController sharedInstance].isListening) {<br>
//<br>
//        [[OEPocketsphinxController sharedInstance] startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;] languageModelIsJSGF:FALSE]; // Start speech recognition if we aren&#8217;t already listening.<br>
        [[OEPocketsphinxController sharedInstance] startRealtimeListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel<br>
                                                                                dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary<br>
                                                                             acousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;]];<br>
    }<br>
}</p>
<p>// An optional delegate method of OEEventsObserver which informs that the Pocketsphinx recognition loop has entered its actual loop.<br>
// This might be useful in debugging a conflict between another sound class and Pocketsphinx.<br>
&#8211; (void) pocketsphinxRecognitionLoopDidStart {</p>
<p>    NSLog(@&#8221;Local callback: Pocketsphinx started.&#8221;); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: Pocketsphinx started.&#8221;; // Show it in the status box.<br>
}</p>
<p>// An optional delegate method of OEEventsObserver which informs that Pocketsphinx is now listening for speech.<br>
&#8211; (void) pocketsphinxDidStartListening {</p>
<p>    NSLog(@&#8221;Local callback: Pocketsphinx is now listening.&#8221;); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: Pocketsphinx is now listening.&#8221;; // Show it in the status box.</p>
<p>    self.startButton.hidden = TRUE; // React to it with some UI changes.<br>
    self.stopButton.hidden = FALSE;<br>
    self.suspendListeningButton.hidden = FALSE;<br>
    self.resumeListeningButton.hidden = TRUE;<br>
}</p>
<p>// An optional delegate method of OEEventsObserver which informs that Pocketsphinx detected speech and is starting to process it.<br>
&#8211; (void) pocketsphinxDidDetectSpeech {<br>
    NSLog(@&#8221;Local callback: Pocketsphinx has detected speech.&#8221;); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: Pocketsphinx has detected speech.&#8221;; // Show it in the status box.<br>
}</p>
<p>// An optional delegate method of OEEventsObserver which informs that Pocketsphinx detected a second of silence, indicating the end of an utterance.<br>
// This was added because developers requested being able to time the recognition speed without the speech time. The processing time is the time between<br>
// this method being called and the hypothesis being returned.<br>
&#8211; (void) pocketsphinxDidDetectFinishedSpeech {<br>
    NSLog(@&#8221;Local callback: Pocketsphinx has detected a second of silence, concluding an utterance.&#8221;); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: Pocketsphinx has detected finished speech.&#8221;; // Show it in the status box.<br>
}</p>
<p>// An optional delegate method of OEEventsObserver which informs that Pocketsphinx has exited its recognition loop, most<br>
// likely in response to the OEPocketsphinxController being told to stop listening via the stopListening method.<br>
&#8211; (void) pocketsphinxDidStopListening {<br>
    NSLog(@&#8221;Local callback: Pocketsphinx has stopped listening.&#8221;); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: Pocketsphinx has stopped listening.&#8221;; // Show it in the status box.<br>
}</p>
<p>// An optional delegate method of OEEventsObserver which informs that Pocketsphinx is still in its listening loop but it is not<br>
// Going to react to speech until listening is resumed.  This can happen as a result of Flite speech being<br>
// in progress on an audio route that doesn&#8217;t support simultaneous Flite speech and Pocketsphinx recognition,<br>
// or as a result of the OEPocketsphinxController being told to suspend recognition via the suspendRecognition method.<br>
&#8211; (void) pocketsphinxDidSuspendRecognition {<br>
    NSLog(@&#8221;Local callback: Pocketsphinx has suspended recognition.&#8221;); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: Pocketsphinx has suspended recognition.&#8221;; // Show it in the status box.<br>
}</p>
<p>// An optional delegate method of OEEventsObserver which informs that Pocketsphinx is still in its listening loop and after recognition<br>
// having been suspended it is now resuming.  This can happen as a result of Flite speech completing<br>
// on an audio route that doesn&#8217;t support simultaneous Flite speech and Pocketsphinx recognition,<br>
// or as a result of the OEPocketsphinxController being told to resume recognition via the resumeRecognition method.<br>
&#8211; (void) pocketsphinxDidResumeRecognition {<br>
    NSLog(@&#8221;Local callback: Pocketsphinx has resumed recognition.&#8221;); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: Pocketsphinx has resumed recognition.&#8221;; // Show it in the status box.<br>
}</p>
<p>// An optional delegate method which informs that Pocketsphinx switched over to a new language model at the given URL in the course of<br>
// recognition. This does not imply that it is a valid file or that recognition will be successful using the file.<br>
&#8211; (void) pocketsphinxDidChangeLanguageModelToFile:(NSString *)newLanguageModelPathAsString andDictionary:(NSString *)newDictionaryPathAsString {<br>
    NSLog(@&#8221;Local callback: Pocketsphinx is now using the following language model: \n%@ and the following dictionary: %@&#8221;,newLanguageModelPathAsString,newDictionaryPathAsString);<br>
}</p>
<p>// An optional delegate method of OEEventsObserver which informs that Flite is speaking, most likely to be useful if debugging a<br>
// complex interaction between sound classes. You don&#8217;t have to do anything yourself in order to prevent Pocketsphinx from listening to Flite talk and trying to recognize the speech.<br>
&#8211; (void) fliteDidStartSpeaking {<br>
    NSLog(@&#8221;Local callback: Flite has started speaking&#8221;); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: Flite has started speaking.&#8221;; // Show it in the status box.<br>
}</p>
<p>// An optional delegate method of OEEventsObserver which informs that Flite is finished speaking, most likely to be useful if debugging a<br>
// complex interaction between sound classes.<br>
&#8211; (void) fliteDidFinishSpeaking {<br>
    NSLog(@&#8221;Local callback: Flite has finished speaking&#8221;); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: Flite has finished speaking.&#8221;; // Show it in the status box.<br>
}</p>
<p>&#8211; (void) pocketSphinxContinuousSetupDidFailWithReason:(NSString *)reasonForFailure { // This can let you know that something went wrong with the recognition loop startup. Turn on [OELogging startOpenEarsLogging] to learn why.<br>
    NSLog(@&#8221;Local callback: Setting up the continuous recognition loop has failed for the reason %@, please turn on [OELogging startOpenEarsLogging] to learn more.&#8221;, reasonForFailure); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: Not possible to start recognition loop.&#8221;; // Show it in the status box.<br>
}</p>
<p>&#8211; (void) pocketSphinxContinuousTeardownDidFailWithReason:(NSString *)reasonForFailure { // This can let you know that something went wrong with the recognition loop startup. Turn on [OELogging startOpenEarsLogging] to learn why.<br>
    NSLog(@&#8221;Local callback: Tearing down the continuous recognition loop has failed for the reason %@, please turn on [OELogging startOpenEarsLogging] to learn more.&#8221;, reasonForFailure); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: Not possible to cleanly end recognition loop.&#8221;; // Show it in the status box.<br>
}</p>
<p>&#8211; (void) testRecognitionCompleted { // A test file which was submitted for direct recognition via the audio driver is done.<br>
    NSLog(@&#8221;Local callback: A test file which was submitted for direct recognition via the audio driver is done.&#8221;); // Log it.<br>
    NSError *error = nil;<br>
    if([OEPocketsphinxController sharedInstance].isListening) { // If we&#8217;re listening, stop listening.<br>
        error = [[OEPocketsphinxController sharedInstance] stopListening];<br>
        if(error) NSLog(@&#8221;Error while stopping listening in testRecognitionCompleted: %@&#8221;, error);<br>
    }</p>
<p>}<br>
/** Pocketsphinx couldn&#8217;t start because it has no mic permissions (will only be returned on iOS7 or later).*/<br>
&#8211; (void) pocketsphinxFailedNoMicPermissions {<br>
    NSLog(@&#8221;Local callback: The user has never set mic permissions or denied permission to this app&#8217;s mic, so listening will not start.&#8221;);<br>
    self.startupFailedDueToLackOfPermissions = TRUE;<br>
    if([OEPocketsphinxController sharedInstance].isListening){<br>
        NSError *error = [[OEPocketsphinxController sharedInstance] stopListening]; // Stop listening if we are listening.<br>
        if(error) NSLog(@&#8221;Error while stopping listening in micPermissionCheckCompleted: %@&#8221;, error);<br>
    }<br>
}</p>
<p>/** The user prompt to get mic permissions, or a check of the mic permissions, has completed with a TRUE or a FALSE result  (will only be returned on iOS7 or later).*/<br>
&#8211; (void) micPermissionCheckCompleted:(BOOL)result {<br>
    if(result) {<br>
        self.restartAttemptsDueToPermissionRequests++;<br>
        if(self.restartAttemptsDueToPermissionRequests == 1 &amp;&amp; self.startupFailedDueToLackOfPermissions) { // If we get here because there was an attempt to start which failed due to lack of permissions, and now permissions have been requested and they returned true, we restart exactly once with the new permissions.</p>
<p>            if(![OEPocketsphinxController sharedInstance].isListening) { // If there was no error and we aren&#8217;t listening, start listening.<br>
//                [[OEPocketsphinxController sharedInstance]<br>
//                 startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel<br>
//                 dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary<br>
//                 acousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;]<br>
//                 languageModelIsJSGF:FALSE]; // Start speech recognition.</p>
<p>//                [[OEPocketsphinxController sharedInstance] startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;] languageModelIsJSGF:FALSE];</p>
<p>                [[OEPocketsphinxController sharedInstance] startRealtimeListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel<br>
                                                                                        dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary<br>
                                                                                     acousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;]];<br>
                self.startupFailedDueToLackOfPermissions = FALSE;<br>
            }<br>
        }<br>
    }<br>
}</p>
<p>#pragma mark &#8211;<br>
#pragma mark UI</p>
<p>// This is not OpenEars-specific stuff, just some UI behavior</p>
<p>&#8211; (IBAction) suspendListeningButtonAction { // This is the action for the button which suspends listening without ending the recognition loop<br>
    [[OEPocketsphinxController sharedInstance] suspendRecognition];	</p>
<p>    self.startButton.hidden = TRUE;<br>
    self.stopButton.hidden = FALSE;<br>
    self.suspendListeningButton.hidden = TRUE;<br>
    self.resumeListeningButton.hidden = FALSE;<br>
}</p>
<p>&#8211; (IBAction) resumeListeningButtonAction { // This is the action for the button which resumes listening if it has been suspended<br>
    [[OEPocketsphinxController sharedInstance] resumeRecognition];</p>
<p>    self.startButton.hidden = TRUE;<br>
    self.stopButton.hidden = FALSE;<br>
    self.suspendListeningButton.hidden = FALSE;<br>
    self.resumeListeningButton.hidden = TRUE;<br>
}</p>
<p>&#8211; (IBAction) stopButtonAction { // This is the action for the button which shuts down the recognition loop.<br>
    NSError *error = nil;<br>
    if([OEPocketsphinxController sharedInstance].isListening) { // Stop if we are currently listening.<br>
        error = [[OEPocketsphinxController sharedInstance] stopListening];<br>
        if(error)NSLog(@&#8221;Error stopping listening in stopButtonAction: %@&#8221;, error);<br>
    }<br>
    self.startButton.hidden = FALSE;<br>
    self.stopButton.hidden = TRUE;<br>
    self.suspendListeningButton.hidden = TRUE;<br>
    self.resumeListeningButton.hidden = TRUE;<br>
}</p>
<p>&#8211; (IBAction) startButtonAction { // This is the action for the button which starts up the recognition loop again if it has been shut down.<br>
    if(![OEPocketsphinxController sharedInstance].isListening) {<br>
        [[OEPocketsphinxController sharedInstance] startRealtimeListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel<br>
                                                                                dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary<br>
                                                                             acousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;]];<br>
//        [[OEPocketsphinxController sharedInstance] startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;] languageModelIsJSGF:FALSE];<br>
//        [[OEPocketsphinxController sharedInstance] startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;] languageModelIsJSGF:FALSE]; // Start speech recognition if we aren&#8217;t already listening.<br>
    }<br>
    self.startButton.hidden = TRUE;<br>
    self.stopButton.hidden = FALSE;<br>
    self.suspendListeningButton.hidden = FALSE;<br>
    self.resumeListeningButton.hidden = TRUE;<br>
}</p>
<p>#pragma mark &#8211;<br>
#pragma mark Example for reading out Pocketsphinx and Flite audio levels without locking the UI by using an NSTimer</p>
<p>// What follows are not OpenEars methods, just an approach for level reading<br>
// that I&#8217;ve included with this sample app. My example implementation does make use of two OpenEars<br>
// methods:	the pocketsphinxInputLevel method of OEPocketsphinxController and the fliteOutputLevel<br>
// method of OEFliteController.<br>
//<br>
// The example is meant to show one way that you can read those levels continuously without locking the UI,<br>
// by using an NSTimer, but the OpenEars level-reading methods<br>
// themselves do not include multithreading code since I believe that you will want to design your own<br>
// code approaches for level display that are tightly-integrated with your interaction design and the<br>
// graphics API you choose.<br>
//<br>
// Please note that if you use my sample approach, you should pay attention to the way that the timer is always stopped in<br>
// dealloc. This should prevent you from having any difficulties with deallocating a class due to a running NSTimer process.</p>
<p>&#8211; (void) startDisplayingLevels { // Start displaying the levels using a timer<br>
    [self stopDisplayingLevels]; // We never want more than one timer valid so we&#8217;ll stop any running timers first.<br>
    self.uiUpdateTimer = [NSTimer scheduledTimerWithTimeInterval:1.0/kLevelUpdatesPerSecond target:self selector:@selector(updateLevelsUI) userInfo:nil repeats:YES];<br>
}</p>
<p>&#8211; (void) stopDisplayingLevels { // Stop displaying the levels by stopping the timer if it&#8217;s running.<br>
    if(self.uiUpdateTimer &amp;&amp; [self.uiUpdateTimer isValid]) { // If there is a running timer, we&#8217;ll stop it here.<br>
        [self.uiUpdateTimer invalidate];<br>
        self.uiUpdateTimer = nil;<br>
    }<br>
}</p>
<p>&#8211; (void) updateLevelsUI { // And here is how we obtain the levels.  This method includes the actual OpenEars methods and uses their results to update the UI of this view controller.</p>
<p>    self.pocketsphinxDbLabel.text = [NSString stringWithFormat:@&#8221;Pocketsphinx Input level:%f&#8221;,[[OEPocketsphinxController sharedInstance] pocketsphinxInputLevel]];  //pocketsphinxInputLevel is an OpenEars method of the class OEPocketsphinxController.</p>
<p>    if(self.fliteController.speechInProgress) {<br>
        self.fliteDbLabel.text = [NSString stringWithFormat:@&#8221;Flite Output level: %f&#8221;,[self.fliteController fliteOutputLevel]]; // fliteOutputLevel is an OpenEars method of the class OEFliteController.<br>
    }<br>
}</p>
<p>&#8211; (void) rapidEarsDidReceiveLiveSpeechHypothesis:(NSString *)hypothesis recognitionScore:(NSString *)recognitionScore {<br>
    NSLog(@&#8221;rapidEarsDidReceiveLiveSpeechHypothesis: %@&#8221;,hypothesis);<br>
}</p>
<p>&#8211; (void) rapidEarsDidReceiveFinishedSpeechHypothesis:(NSString *)hypothesis recognitionScore:(NSString *)recognitionScore {<br>
    NSLog(@&#8221;rapidEarsDidReceiveFinishedSpeechHypothesis: %@&#8221;,hypothesis);<br>
}</p>
<p>@end<br>
[/spoiler]</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1029930" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 5, 2016 at 11:03 am</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1029930" class="bbp-reply-permalink">#1029930</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1029930 -->

<div class="loop-item-4 user-id-4 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-5 odd  post-1029930 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/halle-2/" title="View Halle Winkler&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Halle Winkler</span></a><div class="bbp-author-role">Politepix</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Hi Laurent,</p>
<p>No problem, if you discover something big I will definitely look into it. Thanks for updating me.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1029933" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 5, 2016 at 1:02 pm</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1029933" class="bbp-reply-permalink">#1029933</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1029933 -->

<div class="loop-item-5 user-id-2022 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-6 even topic-author  post-1029933 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/laurent-oscadi/" title="View Laurent&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Laurent</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>I&#8217;ve edited the previous post. I haven&#8217;t seen your answer. I&#8217;m sorry.<br>
Could you take a look into the problem please?</p>
<p>Regards,<br>
Laurent</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1029934" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 5, 2016 at 2:09 pm</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1029934" class="bbp-reply-permalink">#1029934</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1029934 -->

<div class="loop-item-6 user-id-4 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-7 odd  post-1029934 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/halle-2/" title="View Halle Winkler&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Halle Winkler</span></a><div class="bbp-author-role">Politepix</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Hi Laurent,</p>
<p>Take a look at what I wrote previously – to investigate a memory issue when I&#8217;m not aware of any existing issue, I need to hear about specifics that you are seeing using Instruments. That means telling me something about which specific objects you see as being reported as growing indefinitely, why it doesn&#8217;t seem right for them to use the memory they use, and by what amount over what time period they grow.</p>
<p>When I run the sample app with your code, I see correct behavior: there are some micro leaks from Flite and Pocketsphinx, and the OpenEars&#8217; classes memory usage is reasonably frugal during listening, and extra memory is immediately released after a large amount of it is used during the final recognition. When listening is stopped, it is all released. At no time is an unusual amount of memory allocated (given the task). I don&#8217;t see any data in here yet that would lead to a correlation with your crash. I think it&#8217;s important to start by recreating the crash condition and look at the data you see yourself, rather than working from the opposite direction of expecting a correlation without information about it.</p>
<p>A reason that OpenEars or RapidEars will correctly increase the memory footprint over the life of a listening session (not leak) is because the data object where the utterance is held before recognition eventually will become large enough to hold the longest utterance that was spoken in the session. That isn&#8217;t a leak, it&#8217;s just a single buffer finding the ceiling needed in order to hold the full amount of data it is being asked to hold. This isn&#8217;t a sign of a problem since it can be released when it isn&#8217;t being used. If you watch the persistent heap bytes for a while in Instruments you will also see the numbers decrease when there is no speech (and decrease dramatically when listening is stopped), if you wait a while.</p>
<p>I am still up for looking into very specific reports (which include full logging, specific objects, etc) if you can recreate your crash and have some evidence that it relates to one of the libraries.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1029935" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 5, 2016 at 2:14 pm</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1029935" class="bbp-reply-permalink">#1029935</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1029935 -->

<div class="loop-item-7 user-id-4 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-8 even  post-1029935 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/halle-2/" title="View Halle Winkler&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Halle Winkler</span></a><div class="bbp-author-role">Politepix</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Keep in mind that if you are running RapidEars for extremely long sessions (some apps run it for days), it is probably a good idea to occasionally stop and restart listening so any big buffers from unusually long utterances can be cleaned up.</p>
<p>Does your app respond to low memory conditions (for instance, by stopping listening)? They can also be caused by any number of other events on the device.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1029936" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 5, 2016 at 2:58 pm</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1029936" class="bbp-reply-permalink">#1029936</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1029936 -->

<div class="loop-item-8 user-id-2022 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-9 odd topic-author  post-1029936 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/laurent-oscadi/" title="View Laurent&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Laurent</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Hi Halle,<br>
I share with you some screenshots of my test in instruments.<br>
I&#8217;ve tested the sampleApp during 6min30 and I click on the button stop listening at 5 min 30<br>
I&#8217;ve make some screenshots of my instruments screen in order to show you what I see.</p>
<p>This screenshot is the start of the app :<br>
<a href="https://www.dropbox.com/s/vj2rd97mb1xkdka/start.png?dl=0" rel="nofollow">https://www.dropbox.com/s/vj2rd97mb1xkdka/start.png?dl=0</a><br>
With this state :<br>
<a href="https://www.dropbox.com/s/2kowup4bgsyki8w/Capture%20start.png?dl=0" rel="nofollow">https://www.dropbox.com/s/2kowup4bgsyki8w/Capture%20start.png?dl=0</a></p>
<p>And this is the end of the test:<br>
<a href="https://www.dropbox.com/s/gkmi1fooep0xbqv/end.png?dl=0" rel="nofollow">https://www.dropbox.com/s/gkmi1fooep0xbqv/end.png?dl=0</a></p>
<p>With this memory state:<br>
You can see a lot of malloc by openEarsSampleApp but there are other malloc of the app also that I haven&#8217;t screenshot.<br>
You can count about (762-7) 755 malloc of 256kb = 193Mb<br>
<a href="https://www.dropbox.com/s/63y8qa9hujwkz6u/Capture%20end%20.png?dl=0" rel="nofollow">https://www.dropbox.com/s/63y8qa9hujwkz6u/Capture%20end%20.png?dl=0</a><br>
<a href="https://www.dropbox.com/s/3kamesws515ni8n/Capture%20end2.png?dl=0" rel="nofollow">https://www.dropbox.com/s/3kamesws515ni8n/Capture%20end2.png?dl=0</a></p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1029937" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 5, 2016 at 3:00 pm</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1029937" class="bbp-reply-permalink">#1029937</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1029937 -->

<div class="loop-item-9 user-id-2022 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-10 even topic-author  post-1029937 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/laurent-oscadi/" title="View Laurent&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Laurent</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>For the moment I&#8217;m not managing low memory condition in my app for the voice recognition. I will do it. Thank you for this tips.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1029942" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 6, 2016 at 6:41 am</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1029942" class="bbp-reply-permalink">#1029942</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1029942 -->

<div class="loop-item-10 user-id-2022 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-12 odd topic-author  post-1029942 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/laurent-oscadi/" title="View Laurent&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Laurent</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Hi Mr Halle,</p>
<p>This is the link of a new instruments test:</p>
<p><a href="https://www.dropbox.com/s/erzn8sjmr43lmjr/Instruments_report_for_MrHalle.trace.zip?dl=0" rel="nofollow">https://www.dropbox.com/s/erzn8sjmr43lmjr/Instruments_report_for_MrHalle.trace.zip?dl=0</a></p>
<p>I clicked on stop listening button at 5min10.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1029946" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 6, 2016 at 9:55 am</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1029946" class="bbp-reply-permalink">#1029946</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1029946 -->

<div class="loop-item-11 user-id-4 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-15 even  post-1029946 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/halle-2/" title="View Halle Winkler&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Halle Winkler</span></a><div class="bbp-author-role">Politepix</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Super, thanks. A couple of questions – is it a particularly noisy area? It&#8217;s unusual to see continuous empty utterances like this log has. </p>
<p>My second question is that I noticed that your firm is one of the customers who received my email on March 8th 2016 about needing to replace your licensed copy of RapidEars 2.5. Did you replace it? The email was sent to df@ your company name since that was the address used for the purchase.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1029947" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 6, 2016 at 10:02 am</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1029947" class="bbp-reply-permalink">#1029947</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1029947 -->

<div class="loop-item-12 user-id-2022 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-16 odd topic-author  post-1029947 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/laurent-oscadi/" title="View Laurent&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Laurent</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>It is an office. So sometimes people are speaking.</p>
<p>Yes, we did the changes about the license.Thank you.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1029949" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 6, 2016 at 10:07 am</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1029949" class="bbp-reply-permalink">#1029949</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1029949 -->

<div class="loop-item-13 user-id-4 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-17 even  post-1029949 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/halle-2/" title="View Halle Winkler&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Halle Winkler</span></a><div class="bbp-author-role">Politepix</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>OK, I will replicate this and figure out what&#8217;s happening. You can also remove your downloads that you linked to in this discussion if you want to since I have a copy now.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1029950" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 6, 2016 at 10:08 am</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1029950" class="bbp-reply-permalink">#1029950</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1029950 -->

<div class="loop-item-14 user-id-2022 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-18 odd topic-author  post-1029950 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/laurent-oscadi/" title="View Laurent&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Laurent</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Thank you!</p>
<p>Regards,<br>
Laurent</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1029951" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 6, 2016 at 12:21 pm</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1029951" class="bbp-reply-permalink">#1029951</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1029951 -->

<div class="loop-item-15 user-id-4 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-19 even  post-1029951 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/halle-2/" title="View Halle Winkler&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Halle Winkler</span></a><div class="bbp-author-role">Politepix</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>OK, with the Instruments file and the full logging I can see what is happening here (and why I missed it) – it looks like only if the session can&#8217;t be gracefully stopped due to audio conflicts it orphans the decoder in memory as an alternative to causing an exception, and this is an edge case missing from my testbed. I&#8217;ll add a test and fix this in the next update as a high-priority fix, thank you for your report. With some luck it could be this week or next. For your own info, it&#8217;s a issue with OpenEars and not with the plugins.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1030050" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 14, 2016 at 6:13 am</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1030050" class="bbp-reply-permalink">#1030050</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1030050 -->

<div class="loop-item-16 user-id-2022 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-22 odd topic-author  post-1030050 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/laurent-oscadi/" title="View Laurent&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Laurent</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Hi Mr Halle,</p>
<p>We will wait for the upgrade then! Please notify us when it is available.</p>
<p>Thank you.<br>
Regards,<br>
Laurent</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1030105" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 20, 2016 at 3:41 pm</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1030105" class="bbp-reply-permalink">#1030105</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1030105 -->

<div class="loop-item-17 user-id-4 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-23 even  post-1030105 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/halle-2/" title="View Halle Winkler&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Halle Winkler</span></a><div class="bbp-author-role">Politepix</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Hi Laurent,</p>
<p>You can read and subscribe to update information for all Politepix frameworks/plugins here so you can get that information automatically: <a href="http://changelogs.politepix.com/" rel="nofollow">http://changelogs.politepix.com</a></p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1030106" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 20, 2016 at 5:09 pm</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1030106" class="bbp-reply-permalink">#1030106</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1030106 -->

<div class="loop-item-18 user-id-4 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-24 odd  post-1030106 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/halle-2/" title="View Halle Winkler&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Halle Winkler</span></a><div class="bbp-author-role">Politepix</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Hi Laurent,</p>
<p>Does this still happen if you set </p>
<p><code>[OEPocketsphinxController sharedInstance].legacy3rdPassMode = TRUE;</code></p>
<p>at the time that you are otherwise configuring OEPocketsphinxController?</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1030116" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 21, 2016 at 6:31 am</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1030116" class="bbp-reply-permalink">#1030116</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1030116 -->

<div class="loop-item-19 user-id-2022 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-20 even topic-author  post-1030116 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/laurent-oscadi/" title="View Laurent&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Laurent</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Hi Mr Halle,<br>
I have tested it and it still has the same behaviour.<br>
Hope you will find the problem!<br>
Thank you.<br>
Regards,<br>
Laurent</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1030121" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 21, 2016 at 9:08 am</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1030121" class="bbp-reply-permalink">#1030121</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1030121 -->

<div class="loop-item-20 user-id-4 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-21 odd  post-1030121 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/halle-2/" title="View Halle Winkler&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Halle Winkler</span></a><div class="bbp-author-role">Politepix</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Greetings Laurent,</p>
<p>Unfortunately I haven&#8217;t been able to replicate the issue locally although I&#8217;ve been trying. Now I&#8217;m a bit confused that you say that [OEPocketsphinxController sharedInstance].legacy3rdPassMode = TRUE doesn&#8217;t affect it, since the issue in the logging shouldn&#8217;t be possible with that setting since it appears to be caused by an overly-long 3rd-pass search, and using legacy3rdPassMode with RapidEars turns 3rd-pass searches off. That suggests that it could be a local issue to your install.</p>
<p>When I examine your logs more closely, the Rejecto version linked is 2.5, but it looks like the RapidEars version linked is older than 2.5. I think that somehow your project is not really linking to the current version of RapidEars, though I believe you downloaded it – perhaps the linked version is in a different location from the downloaded version.</p>
<p>Can you do some troubleshooting of why the current version of RapidEars doesn&#8217;t seem to be linked to the project and then let me know whether you still have this issue? I&#8217;m currently at a dead end in demonstrating it in my own testbed and it could be due to linking to an old RapidEars version.</p>
<p>When you&#8217;ve successfully linked to RapidEars 2.5, there will be a line of logging in your OELogging output giving the version number of your RapidEars framework, right at the beginning. Thanks!</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1030131" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 21, 2016 at 10:51 am</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1030131" class="bbp-reply-permalink">#1030131</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1030131 -->

<div class="loop-item-21 user-id-4 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-27 even  post-1030131 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/halle-2/" title="View Halle Winkler&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Halle Winkler</span></a><div class="bbp-author-role">Politepix</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Some hints for the troubleshooting process: Xcode links to the framework both via the file navigator and via the Framework Search Paths entry of Build Settings. It is possible that they don&#8217;t both point to the same thing. I would probably start by searching my system for RapidEars.framework and removing all copies of it found, making sure that the app project breaks, removing the RapidEars.framework entry from Framework Search Paths, and then downloading your RapidEars 2.5 framework from the licensee site and installing it to the app project fresh. At that point you should see the line of logging stating the version number of RapidEars.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1030132" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 21, 2016 at 12:33 pm</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1030132" class="bbp-reply-permalink">#1030132</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1030132 -->

<div class="loop-item-22 user-id-2022 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-28 odd topic-author  post-1030132 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/laurent-oscadi/" title="View Laurent&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Laurent</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>In my app I can see RapidEars 2.5 in the OELogging.<br>
Even with the legacy 3rdPassMode at TRUE I still have the same behaviour.</p>
<p>But in the sample App it does not work I&#8217;ve tried with the demo one and with our rapidEars framework .<br>
I have checked it is the same path for both navigator and framework build path.<br>
This is my code . Perhaps you can help me to find why?<br>
[spoiler]<br>
#import &#8220;ViewController.h&#8221;<br>
#import &lt;OpenEars/OEPocketsphinxController.h&gt;<br>
#import &lt;RapidEars/OEPocketsphinxController+RapidEars.h&gt;<br>
#import &lt;OpenEars/OEFliteController.h&gt;<br>
#import &lt;OpenEars/OELanguageModelGenerator.h&gt;<br>
#import &lt;Rejecto/OELanguageModelGenerator+Rejecto.h&gt;<br>
#import &lt;OpenEars/OELogging.h&gt;<br>
#import &lt;OpenEars/OEAcousticModel.h&gt;<br>
#import &lt;Slt/Slt.h&gt;<br>
#import &lt;OpenEars/OELanguageModelGenerator.h&gt;<br>
#import &lt;OpenEars/OEEventsObserver.h&gt;<br>
#import &lt;RapidEars/OEEventsObserver+RapidEars.h&gt;<br>
@interface ViewController()</p>
<p>// UI actions, not specifically related to OpenEars other than the fact that they invoke OpenEars methods.<br>
&#8211; (IBAction) stopButtonAction;<br>
&#8211; (IBAction) startButtonAction;<br>
&#8211; (IBAction) suspendListeningButtonAction;<br>
&#8211; (IBAction) resumeListeningButtonAction;</p>
<p>// Example for reading out the input audio levels without locking the UI using an NSTimer</p>
<p>&#8211; (void) startDisplayingLevels;<br>
&#8211; (void) stopDisplayingLevels;</p>
<p>// These three are the important OpenEars objects that this class demonstrates the use of.<br>
@property (nonatomic, strong) Slt *slt;</p>
<p>@property (nonatomic, strong) OEEventsObserver *openEarsEventsObserver;<br>
@property (nonatomic, strong) OEPocketsphinxController *pocketsphinxController;<br>
@property (nonatomic, strong) OEFliteController *fliteController;</p>
<p>// Some UI, not specifically related to OpenEars.<br>
@property (nonatomic, strong) IBOutlet UIButton *stopButton;<br>
@property (nonatomic, strong) IBOutlet UIButton *startButton;<br>
@property (nonatomic, strong) IBOutlet UIButton *suspendListeningButton;<br>
@property (nonatomic, strong) IBOutlet UIButton *resumeListeningButton;<br>
@property (nonatomic, strong) IBOutlet UITextView *statusTextView;<br>
@property (nonatomic, strong) IBOutlet UITextView *heardTextView;<br>
@property (nonatomic, strong) IBOutlet UILabel *pocketsphinxDbLabel;<br>
@property (nonatomic, strong) IBOutlet UILabel *fliteDbLabel;<br>
@property (nonatomic, assign) BOOL usingStartingLanguageModel;<br>
@property (nonatomic, assign) int restartAttemptsDueToPermissionRequests;<br>
@property (nonatomic, assign) BOOL startupFailedDueToLackOfPermissions;</p>
<p>// Things which help us show off the dynamic language features.<br>
@property (nonatomic, copy) NSString *pathToFirstDynamicallyGeneratedLanguageModel;<br>
@property (nonatomic, copy) NSString *pathToFirstDynamicallyGeneratedDictionary;<br>
@property (nonatomic, copy) NSString *pathToSecondDynamicallyGeneratedLanguageModel;<br>
@property (nonatomic, copy) NSString *pathToSecondDynamicallyGeneratedDictionary;</p>
<p>// Our NSTimer that will help us read and display the input and output levels without locking the UI<br>
@property (nonatomic, strong) 	NSTimer *uiUpdateTimer;</p>
<p>@end</p>
<p>@implementation ViewController</p>
<p>#define kLevelUpdatesPerSecond 18 // We&#8217;ll have the ui update 18 times a second to show some fluidity without hitting the CPU too hard.</p>
<p>//#define kGetNbest // Uncomment this if you want to try out nbest<br>
#pragma mark &#8211;<br>
#pragma mark Memory Management</p>
<p>&#8211; (void)dealloc {<br>
    [self stopDisplayingLevels];<br>
}</p>
<p>#pragma mark &#8211;<br>
#pragma mark View Lifecycle</p>
<p>&#8211; (void)viewDidLoad {<br>
    [super viewDidLoad];<br>
    self.fliteController = [[OEFliteController alloc] init];<br>
    self.openEarsEventsObserver = [[OEEventsObserver alloc] init];<br>
    self.openEarsEventsObserver.delegate = self;<br>
    self.slt = [[Slt alloc] init];</p>
<p>    self.restartAttemptsDueToPermissionRequests = 0;<br>
    self.startupFailedDueToLackOfPermissions = FALSE;</p>
<p>    [OEPocketsphinxController sharedInstance].verbosePocketSphinx = TRUE; // Uncomment this for much more verbose speech recognition engine output. If you have issues, show this logging in the forums.</p>
<p>    [self.openEarsEventsObserver setDelegate:self]; // Make this class the delegate of OpenEarsObserver so we can get all of the messages about what OpenEars is doing.</p>
<p>    [[OEPocketsphinxController sharedInstance] setActive:TRUE error:nil]; // Call this before setting any OEPocketsphinxController characteristics</p>
<p>    // This is the language model we&#8217;re going to start up with. The only reason I&#8217;m making it a class property is that I reuse it a bunch of times in this example,<br>
//     but you can pass the string contents directly to OEPocketsphinxController:startListeningWithLanguageModelAtPath:dictionaryAtPath:languageModelIsJSGF:</p>
<p>    NSArray *firstLanguageArray = @[@&#8221;BACKWARD&#8221;,<br>
                                    @&#8221;CHANGE&#8221;,<br>
                                    @&#8221;FORWARD&#8221;,<br>
                                    @&#8221;GO&#8221;,<br>
                                    @&#8221;LEFT&#8221;,<br>
                                    @&#8221;MODEL&#8221;,<br>
                                    @&#8221;RIGHT&#8221;,<br>
                                    @&#8221;TURN&#8221;];</p>
<p>    OELanguageModelGenerator *languageModelGenerator = [[OELanguageModelGenerator alloc] init];<br>
    [OELogging startOpenEarsLogging]; // Uncomment me for OELogging, which is verbose logging about internal OpenEars operations such as audio settings. If you have issues, show this logging in the forums.</p>
<p>    // languageModelGenerator.verboseLanguageModelGenerator = TRUE; // Uncomment me for verbose language model generator debug output.</p>
<p>    NSError *error = [languageModelGenerator generateRejectingLanguageModelFromArray:firstLanguageArray withFilesNamed:@&#8221;FirstOpenEarsDynamicLanguageModel&#8221; withOptionalExclusions:nil<br>
                                                                     usingVowelsOnly:FALSE<br>
                                                                          withWeight:[ NSNumber numberWithFloat:2.0 ]<br>
                                                              forAcousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;]]; // Change &#8220;AcousticModelEnglish&#8221; to &#8220;AcousticModelSpanish&#8221; in order to create a language model for Spanish recognition instead of English.<br>
        if(error) {<br>
        NSLog(@&#8221;Dynamic language generator reported error %@&#8221;, [error description]);<br>
    } else {<br>
        self.pathToFirstDynamicallyGeneratedLanguageModel = [languageModelGenerator pathToSuccessfullyGeneratedLanguageModelWithRequestedName:@&#8221;FirstOpenEarsDynamicLanguageModel&#8221;];<br>
        self.pathToFirstDynamicallyGeneratedDictionary = [languageModelGenerator pathToSuccessfullyGeneratedDictionaryWithRequestedName:@&#8221;FirstOpenEarsDynamicLanguageModel&#8221;];<br>
    }</p>
<p>    self.usingStartingLanguageModel = TRUE; // This is not an OpenEars thing, this is just so I can switch back and forth between the two models in this sample app.</p>
<p>    // Here is an example of dynamically creating an in-app grammar.</p>
<p>    // We want it to be able to response to the speech &#8220;CHANGE MODEL&#8221; and a few other things.  Items we want to have recognized as a whole phrase (like &#8220;CHANGE MODEL&#8221;)<br>
    // we put into the array as one string (e.g. &#8220;CHANGE MODEL&#8221; instead of &#8220;CHANGE&#8221; and &#8220;MODEL&#8221;). This increases the probability that they will be recognized as a phrase. This works even better starting with version 1.0 of OpenEars.</p>
<p>    NSArray *secondLanguageArray = @[@&#8221;SUNDAY&#8221;,<br>
                                     @&#8221;MONDAY&#8221;,<br>
                                     @&#8221;TUESDAY&#8221;,<br>
                                     @&#8221;WEDNESDAY&#8221;,<br>
                                     @&#8221;THURSDAY&#8221;,<br>
                                     @&#8221;FRIDAY&#8221;,<br>
                                     @&#8221;SATURDAY&#8221;,<br>
                                     @&#8221;QUIDNUNC&#8221;,<br>
                                     @&#8221;CHANGE MODEL&#8221;];</p>
<p>    // The last entry, quidnunc, is an example of a word which will not be found in the lookup dictionary and will be passed to the fallback method. The fallback method is slower,<br>
    // so, for instance, creating a new language model from dictionary words will be pretty fast, but a model that has a lot of unusual names in it or invented/rare/recent-slang<br>
    // words will be slower to generate. You can use this information to give your users good UI feedback about what the expectations for wait times should be.</p>
<p>    // I don&#8217;t think it&#8217;s beneficial to lazily instantiate OELanguageModelGenerator because you only need to give it a single message and then release it.<br>
    // If you need to create a very large model or any size of model that has many unusual words that have to make use of the fallback generation method,<br>
    // you will want to run this on a background thread so you can give the user some UI feedback that the task is in progress.</p>
<p>    // generateLanguageModelFromArray:withFilesNamed returns an NSError which will either have a value of noErr if everything went fine or a specific error if it didn&#8217;t.<br>
    error = [languageModelGenerator generateRejectingLanguageModelFromArray:secondLanguageArray withFilesNamed:@&#8221;SecondOpenEarsDynamicLanguageModel&#8221;  withOptionalExclusions:nil<br>
                                                            usingVowelsOnly:FALSE<br>
                                                                 withWeight:[ NSNumber numberWithFloat:2.0 ]<br>
                                                     forAcousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;]]; // Change &#8220;AcousticModelEnglish&#8221; to &#8220;AcousticModelSpanish&#8221; in order to create a language model for Spanish recognition instead of English.</p>
<p>    //    NSError *error = [languageModelGenerator generateLanguageModelFromTextFile:[NSString stringWithFormat:@&#8221;%@/%@&#8221;,[[NSBundle mainBundle] resourcePath], @&#8221;OpenEarsCorpus.txt&#8221;] withFilesNamed:@&#8221;SecondOpenEarsDynamicLanguageModel&#8221; forAcousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;]]; // Try this out to see how generating a language model from a corpus works.</p>
<p>    if(error) {<br>
        NSLog(@&#8221;Dynamic language generator reported error %@&#8221;, [error description]);<br>
    }	else {</p>
<p>        self.pathToSecondDynamicallyGeneratedLanguageModel = [languageModelGenerator pathToSuccessfullyGeneratedLanguageModelWithRequestedName:@&#8221;SecondOpenEarsDynamicLanguageModel&#8221;]; // We&#8217;ll set our new .languagemodel file to be the one to get switched to when the words &#8220;CHANGE MODEL&#8221; are recognized.<br>
        self.pathToSecondDynamicallyGeneratedDictionary = [languageModelGenerator pathToSuccessfullyGeneratedDictionaryWithRequestedName:@&#8221;SecondOpenEarsDynamicLanguageModel&#8221;];; // We&#8217;ll set our new dictionary to be the one to get switched to when the words &#8220;CHANGE MODEL&#8221; are recognized.</p>
<p>        // Next, an informative message.</p>
<p>        NSLog(@&#8221;\n\nWelcome to the OpenEars sample project. This project understands the words:\nBACKWARD,\nCHANGE,\nFORWARD,\nGO,\nLEFT,\nMODEL,\nRIGHT,\nTURN,\nand if you say \&#8221;CHANGE MODEL\&#8221; it will switch to its dynamically-generated model which understands the words:\nCHANGE,\nMODEL,\nMONDAY,\nTUESDAY,\nWEDNESDAY,\nTHURSDAY,\nFRIDAY,\nSATURDAY,\nSUNDAY,\nQUIDNUNC&#8221;);</p>
<p>        // This is how to start the continuous listening loop of an available instance of OEPocketsphinxController. We won&#8217;t do this if the language generation failed since it will be listening for a command to change over to the generated language.</p>
<p>        [[OEPocketsphinxController sharedInstance] setActive:TRUE error:nil]; // Call this once before setting properties of the OEPocketsphinxController instance.</p>
<p>        //   [OEPocketsphinxController sharedInstance].pathToTestFile = [[NSBundle mainBundle] pathForResource:@&#8221;change_model_short&#8221; ofType:@&#8221;wav&#8221;];  // This is how you could use a test WAV (mono/16-bit/16k) rather than live recognition. Don&#8217;t forget to add your WAV to your app bundle.</p>
<p>        if(![OEPocketsphinxController sharedInstance].isListening) {<br>
            [[OEPocketsphinxController sharedInstance] startRealtimeListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;]];</p>
<p>//            [[OEPocketsphinxController sharedInstance] startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;] languageModelIsJSGF:FALSE]; // Start speech recognition if we aren&#8217;t already listening.<br>
        }<br>
        // [self startDisplayingLevels] is not an OpenEars method, just a very simple approach for level reading<br>
        // that I&#8217;ve included with this sample app. My example implementation does make use of two OpenEars<br>
        // methods:	the pocketsphinxInputLevel method of OEPocketsphinxController and the fliteOutputLevel<br>
        // method of fliteController.<br>
        //<br>
        // The example is meant to show one way that you can read those levels continuously without locking the UI,<br>
        // by using an NSTimer, but the OpenEars level-reading methods<br>
        // themselves do not include multithreading code since I believe that you will want to design your own<br>
        // code approaches for level display that are tightly-integrated with your interaction design and the<br>
        // graphics API you choose. </p>
<p>        [self startDisplayingLevels];</p>
<p>        // Here is some UI stuff that has nothing specifically to do with OpenEars implementation<br>
        self.startButton.hidden = TRUE;<br>
        self.stopButton.hidden = TRUE;<br>
        self.suspendListeningButton.hidden = TRUE;<br>
        self.resumeListeningButton.hidden = TRUE;<br>
    }<br>
}</p>
<p>#pragma mark &#8211;<br>
#pragma mark OEEventsObserver delegate methods</p>
<p>// What follows are all of the delegate methods you can optionally use once you&#8217;ve instantiated an OEEventsObserver and set its delegate to self.<br>
// I&#8217;ve provided some pretty granular information about the exact phase of the Pocketsphinx listening loop, the Audio Session, and Flite, but I&#8217;d expect<br>
// that the ones that will really be needed by most projects are the following:<br>
//<br>
//- (void) pocketsphinxDidReceiveHypothesis:(NSString *)hypothesis recognitionScore:(NSString *)recognitionScore utteranceID:(NSString *)utteranceID;<br>
//- (void) audioSessionInterruptionDidBegin;<br>
//- (void) audioSessionInterruptionDidEnd;<br>
//- (void) audioRouteDidChangeToRoute:(NSString *)newRoute;<br>
//- (void) pocketsphinxDidStartListening;<br>
//- (void) pocketsphinxDidStopListening;<br>
//<br>
// It isn&#8217;t necessary to have a OEPocketsphinxController or a OEFliteController instantiated in order to use these methods.  If there isn&#8217;t anything instantiated that will<br>
// send messages to an OEEventsObserver, all that will happen is that these methods will never fire.  You also do not have to create a OEEventsObserver in<br>
// the same class or view controller in which you are doing things with a OEPocketsphinxController or OEFliteController; you can receive updates from those objects in<br>
// any class in which you instantiate an OEEventsObserver and set its delegate to self.</p>
<p>// This is an optional delegate method of OEEventsObserver which delivers the text of speech that Pocketsphinx heard and analyzed, along with its accuracy score and utterance ID.<br>
&#8211; (void) pocketsphinxDidReceiveHypothesis:(NSString *)hypothesis recognitionScore:(NSString *)recognitionScore utteranceID:(NSString *)utteranceID {</p>
<p>    NSLog(@&#8221;Local callback: The received hypothesis is %@ with a score of %@ and an ID of %@&#8221;, hypothesis, recognitionScore, utteranceID); // Log it.<br>
    if([hypothesis isEqualToString:@&#8221;CHANGE MODEL&#8221;]) { // If the user says &#8220;CHANGE MODEL&#8221;, we will switch to the alternate model (which happens to be the dynamically generated model).</p>
<p>        // Here is an example of language model switching in OpenEars. Deciding on what logical basis to switch models is your responsibility.<br>
        // For instance, when you call a customer service line and get a response tree that takes you through different options depending on what you say to it,<br>
        // the models are being switched as you progress through it so that only relevant choices can be understood. The construction of that logical branching and<br>
        // how to react to it is your job; OpenEars just lets you send the signal to switch the language model when you&#8217;ve decided it&#8217;s the right time to do so.</p>
<p>        if(self.usingStartingLanguageModel) { // If we&#8217;re on the starting model, switch to the dynamically generated one.</p>
<p>            [[OEPocketsphinxController sharedInstance] changeLanguageModelToFile:self.pathToSecondDynamicallyGeneratedLanguageModel withDictionary:self.pathToSecondDynamicallyGeneratedDictionary];<br>
            self.usingStartingLanguageModel = FALSE;</p>
<p>        } else { // If we&#8217;re on the dynamically generated model, switch to the start model (this is an example of a trigger and method for switching models).</p>
<p>            [[OEPocketsphinxController sharedInstance] changeLanguageModelToFile:self.pathToFirstDynamicallyGeneratedLanguageModel withDictionary:self.pathToFirstDynamicallyGeneratedDictionary];<br>
            self.usingStartingLanguageModel = TRUE;<br>
        }<br>
    }</p>
<p>    self.heardTextView.text = [NSString stringWithFormat:@&#8221;Heard: \&#8221;%@\&#8221;&#8221;, hypothesis]; // Show it in the status box.</p>
<p>    // This is how to use an available instance of OEFliteController. We&#8217;re going to repeat back the command that we heard with the voice we&#8217;ve chosen.<br>
    [self.fliteController say:[NSString stringWithFormat:@&#8221;You said %@&#8221;,hypothesis] withVoice:self.slt];<br>
}</p>
<p>#ifdef kGetNbest<br>
&#8211; (void) pocketsphinxDidReceiveNBestHypothesisArray:(NSArray *)hypothesisArray { // Pocketsphinx has an n-best hypothesis dictionary.<br>
    NSLog(@&#8221;Local callback:  hypothesisArray is %@&#8221;,hypothesisArray);<br>
}<br>
#endif<br>
// An optional delegate method of OEEventsObserver which informs that there was an interruption to the audio session (e.g. an incoming phone call).<br>
&#8211; (void) audioSessionInterruptionDidBegin {<br>
    NSLog(@&#8221;Local callback:  AudioSession interruption began.&#8221;); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: AudioSession interruption began.&#8221;; // Show it in the status box.<br>
    NSError *error = nil;<br>
    if([OEPocketsphinxController sharedInstance].isListening) {<br>
        error = [[OEPocketsphinxController sharedInstance] stopListening]; // React to it by telling Pocketsphinx to stop listening (if it is listening) since it will need to restart its loop after an interruption.<br>
        if(error) NSLog(@&#8221;Error while stopping listening in audioSessionInterruptionDidBegin: %@&#8221;, error);<br>
    }<br>
}</p>
<p>// An optional delegate method of OEEventsObserver which informs that the interruption to the audio session ended.<br>
&#8211; (void) audioSessionInterruptionDidEnd {<br>
    NSLog(@&#8221;Local callback:  AudioSession interruption ended.&#8221;); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: AudioSession interruption ended.&#8221;; // Show it in the status box.<br>
    // We&#8217;re restarting the previously-stopped listening loop.<br>
    if(![OEPocketsphinxController sharedInstance].isListening){<br>
            [[OEPocketsphinxController sharedInstance] startRealtimeListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;]];<br>
//        [[OEPocketsphinxController sharedInstance] startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;] languageModelIsJSGF:FALSE]; // Start speech recognition if we aren&#8217;t currently listening.<br>
    }<br>
}</p>
<p>// An optional delegate method of OEEventsObserver which informs that the audio input became unavailable.<br>
&#8211; (void) audioInputDidBecomeUnavailable {<br>
    NSLog(@&#8221;Local callback:  The audio input has become unavailable&#8221;); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: The audio input has become unavailable&#8221;; // Show it in the status box.<br>
    NSError *error = nil;<br>
    if([OEPocketsphinxController sharedInstance].isListening){<br>
        error = [[OEPocketsphinxController sharedInstance] stopListening]; // React to it by telling Pocketsphinx to stop listening since there is no available input (but only if we are listening).<br>
        if(error) NSLog(@&#8221;Error while stopping listening in audioInputDidBecomeUnavailable: %@&#8221;, error);<br>
    }<br>
}</p>
<p>// An optional delegate method of OEEventsObserver which informs that the unavailable audio input became available again.<br>
&#8211; (void) audioInputDidBecomeAvailable {<br>
    NSLog(@&#8221;Local callback: The audio input is available&#8221;); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: The audio input is available&#8221;; // Show it in the status box.<br>
    if(![OEPocketsphinxController sharedInstance].isListening) {<br>
            [[OEPocketsphinxController sharedInstance] startRealtimeListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;]];<br>
//        [[OEPocketsphinxController sharedInstance] startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;] languageModelIsJSGF:FALSE]; // Start speech recognition, but only if we aren&#8217;t already listening.<br>
    }<br>
}<br>
// An optional delegate method of OEEventsObserver which informs that there was a change to the audio route (e.g. headphones were plugged in or unplugged).<br>
&#8211; (void) audioRouteDidChangeToRoute:(NSString *)newRoute {<br>
    NSLog(@&#8221;Local callback: Audio route change. The new audio route is %@&#8221;, newRoute); // Log it.<br>
    self.statusTextView.text = [NSString stringWithFormat:@&#8221;Status: Audio route change. The new audio route is %@&#8221;,newRoute]; // Show it in the status box.</p>
<p>    NSError *error = [[OEPocketsphinxController sharedInstance] stopListening]; // React to it by telling the Pocketsphinx loop to shut down and then start listening again on the new route</p>
<p>    if(error)NSLog(@&#8221;Local callback: error while stopping listening in audioRouteDidChangeToRoute: %@&#8221;,error);</p>
<p>    if(![OEPocketsphinxController sharedInstance].isListening) {<br>
            [[OEPocketsphinxController sharedInstance] startRealtimeListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;]];<br>
//        [[OEPocketsphinxController sharedInstance] startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;] languageModelIsJSGF:FALSE]; // Start speech recognition if we aren&#8217;t already listening.<br>
    }<br>
}</p>
<p>// An optional delegate method of OEEventsObserver which informs that the Pocketsphinx recognition loop has entered its actual loop.<br>
// This might be useful in debugging a conflict between another sound class and Pocketsphinx.<br>
&#8211; (void) pocketsphinxRecognitionLoopDidStart {</p>
<p>    NSLog(@&#8221;Local callback: Pocketsphinx started.&#8221;); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: Pocketsphinx started.&#8221;; // Show it in the status box.<br>
}</p>
<p>// An optional delegate method of OEEventsObserver which informs that Pocketsphinx is now listening for speech.<br>
&#8211; (void) pocketsphinxDidStartListening {</p>
<p>    NSLog(@&#8221;Local callback: Pocketsphinx is now listening.&#8221;); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: Pocketsphinx is now listening.&#8221;; // Show it in the status box.</p>
<p>    self.startButton.hidden = TRUE; // React to it with some UI changes.<br>
    self.stopButton.hidden = FALSE;<br>
    self.suspendListeningButton.hidden = FALSE;<br>
    self.resumeListeningButton.hidden = TRUE;<br>
}</p>
<p>// An optional delegate method of OEEventsObserver which informs that Pocketsphinx detected speech and is starting to process it.<br>
&#8211; (void) pocketsphinxDidDetectSpeech {<br>
    NSLog(@&#8221;Local callback: Pocketsphinx has detected speech.&#8221;); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: Pocketsphinx has detected speech.&#8221;; // Show it in the status box.<br>
}</p>
<p>// An optional delegate method of OEEventsObserver which informs that Pocketsphinx detected a second of silence, indicating the end of an utterance.<br>
// This was added because developers requested being able to time the recognition speed without the speech time. The processing time is the time between<br>
// this method being called and the hypothesis being returned.<br>
&#8211; (void) pocketsphinxDidDetectFinishedSpeech {<br>
    NSLog(@&#8221;Local callback: Pocketsphinx has detected a second of silence, concluding an utterance.&#8221;); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: Pocketsphinx has detected finished speech.&#8221;; // Show it in the status box.<br>
}</p>
<p>// An optional delegate method of OEEventsObserver which informs that Pocketsphinx has exited its recognition loop, most<br>
// likely in response to the OEPocketsphinxController being told to stop listening via the stopListening method.<br>
&#8211; (void) pocketsphinxDidStopListening {<br>
    NSLog(@&#8221;Local callback: Pocketsphinx has stopped listening.&#8221;); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: Pocketsphinx has stopped listening.&#8221;; // Show it in the status box.<br>
}</p>
<p>// An optional delegate method of OEEventsObserver which informs that Pocketsphinx is still in its listening loop but it is not<br>
// Going to react to speech until listening is resumed.  This can happen as a result of Flite speech being<br>
// in progress on an audio route that doesn&#8217;t support simultaneous Flite speech and Pocketsphinx recognition,<br>
// or as a result of the OEPocketsphinxController being told to suspend recognition via the suspendRecognition method.<br>
&#8211; (void) pocketsphinxDidSuspendRecognition {<br>
    NSLog(@&#8221;Local callback: Pocketsphinx has suspended recognition.&#8221;); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: Pocketsphinx has suspended recognition.&#8221;; // Show it in the status box.<br>
}</p>
<p>// An optional delegate method of OEEventsObserver which informs that Pocketsphinx is still in its listening loop and after recognition<br>
// having been suspended it is now resuming.  This can happen as a result of Flite speech completing<br>
// on an audio route that doesn&#8217;t support simultaneous Flite speech and Pocketsphinx recognition,<br>
// or as a result of the OEPocketsphinxController being told to resume recognition via the resumeRecognition method.<br>
&#8211; (void) pocketsphinxDidResumeRecognition {<br>
    NSLog(@&#8221;Local callback: Pocketsphinx has resumed recognition.&#8221;); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: Pocketsphinx has resumed recognition.&#8221;; // Show it in the status box.<br>
}</p>
<p>// An optional delegate method which informs that Pocketsphinx switched over to a new language model at the given URL in the course of<br>
// recognition. This does not imply that it is a valid file or that recognition will be successful using the file.<br>
&#8211; (void) pocketsphinxDidChangeLanguageModelToFile:(NSString *)newLanguageModelPathAsString andDictionary:(NSString *)newDictionaryPathAsString {<br>
    NSLog(@&#8221;Local callback: Pocketsphinx is now using the following language model: \n%@ and the following dictionary: %@&#8221;,newLanguageModelPathAsString,newDictionaryPathAsString);<br>
}</p>
<p>// An optional delegate method of OEEventsObserver which informs that Flite is speaking, most likely to be useful if debugging a<br>
// complex interaction between sound classes. You don&#8217;t have to do anything yourself in order to prevent Pocketsphinx from listening to Flite talk and trying to recognize the speech.<br>
&#8211; (void) fliteDidStartSpeaking {<br>
    NSLog(@&#8221;Local callback: Flite has started speaking&#8221;); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: Flite has started speaking.&#8221;; // Show it in the status box.<br>
}</p>
<p>// An optional delegate method of OEEventsObserver which informs that Flite is finished speaking, most likely to be useful if debugging a<br>
// complex interaction between sound classes.<br>
&#8211; (void) fliteDidFinishSpeaking {<br>
    NSLog(@&#8221;Local callback: Flite has finished speaking&#8221;); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: Flite has finished speaking.&#8221;; // Show it in the status box.<br>
}</p>
<p>&#8211; (void) pocketSphinxContinuousSetupDidFailWithReason:(NSString *)reasonForFailure { // This can let you know that something went wrong with the recognition loop startup. Turn on [OELogging startOpenEarsLogging] to learn why.<br>
    NSLog(@&#8221;Local callback: Setting up the continuous recognition loop has failed for the reason %@, please turn on [OELogging startOpenEarsLogging] to learn more.&#8221;, reasonForFailure); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: Not possible to start recognition loop.&#8221;; // Show it in the status box.<br>
}</p>
<p>&#8211; (void) pocketSphinxContinuousTeardownDidFailWithReason:(NSString *)reasonForFailure { // This can let you know that something went wrong with the recognition loop startup. Turn on [OELogging startOpenEarsLogging] to learn why.<br>
    NSLog(@&#8221;Local callback: Tearing down the continuous recognition loop has failed for the reason %@, please turn on [OELogging startOpenEarsLogging] to learn more.&#8221;, reasonForFailure); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: Not possible to cleanly end recognition loop.&#8221;; // Show it in the status box.<br>
}</p>
<p>&#8211; (void) testRecognitionCompleted { // A test file which was submitted for direct recognition via the audio driver is done.<br>
    NSLog(@&#8221;Local callback: A test file which was submitted for direct recognition via the audio driver is done.&#8221;); // Log it.<br>
    NSError *error = nil;<br>
    if([OEPocketsphinxController sharedInstance].isListening) { // If we&#8217;re listening, stop listening.<br>
        error = [[OEPocketsphinxController sharedInstance] stopListening];<br>
        if(error) NSLog(@&#8221;Error while stopping listening in testRecognitionCompleted: %@&#8221;, error);<br>
    }</p>
<p>}<br>
&#8211; (void) rapidEarsDidReceiveLiveSpeechHypothesis:(NSString *)hypothesis recognitionScore:(NSString *)recognitionScore {<br>
    NSLog(@&#8221;rapidEarsDidReceiveLiveSpeechHypothesis: %@&#8221;,hypothesis);<br>
}</p>
<p>&#8211; (void) rapidEarsDidReceiveFinishedSpeechHypothesis:(NSString *)hypothesis recognitionScore:(NSString *)recognitionScore {<br>
    NSLog(@&#8221;rapidEarsDidReceiveFinishedSpeechHypothesis: %@&#8221;,hypothesis);<br>
}<br>
/** Pocketsphinx couldn&#8217;t start because it has no mic permissions (will only be returned on iOS7 or later).*/<br>
&#8211; (void) pocketsphinxFailedNoMicPermissions {<br>
    NSLog(@&#8221;Local callback: The user has never set mic permissions or denied permission to this app&#8217;s mic, so listening will not start.&#8221;);<br>
    self.startupFailedDueToLackOfPermissions = TRUE;<br>
    if([OEPocketsphinxController sharedInstance].isListening){<br>
        NSError *error = [[OEPocketsphinxController sharedInstance] stopListening]; // Stop listening if we are listening.<br>
        if(error) NSLog(@&#8221;Error while stopping listening in micPermissionCheckCompleted: %@&#8221;, error);<br>
    }<br>
}</p>
<p>/** The user prompt to get mic permissions, or a check of the mic permissions, has completed with a TRUE or a FALSE result  (will only be returned on iOS7 or later).*/<br>
&#8211; (void) micPermissionCheckCompleted:(BOOL)result {<br>
    if(result) {<br>
        self.restartAttemptsDueToPermissionRequests++;<br>
        if(self.restartAttemptsDueToPermissionRequests == 1 &amp;&amp; self.startupFailedDueToLackOfPermissions) { // If we get here because there was an attempt to start which failed due to lack of permissions, and now permissions have been requested and they returned true, we restart exactly once with the new permissions.</p>
<p>            if(![OEPocketsphinxController sharedInstance].isListening) { // If there was no error and we aren&#8217;t listening, start listening.<br>
                     [[OEPocketsphinxController sharedInstance] startRealtimeListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;]];<br>
//                 startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel<br>
//                 dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary<br>
//                 acousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;]<br>
//                 languageModelIsJSGF:FALSE]; // Start speech recognition.</p>
<p>                self.startupFailedDueToLackOfPermissions = FALSE;<br>
            }<br>
        }<br>
    }<br>
}</p>
<p>#pragma mark &#8211;<br>
#pragma mark UI</p>
<p>// This is not OpenEars-specific stuff, just some UI behavior</p>
<p>&#8211; (IBAction) suspendListeningButtonAction { // This is the action for the button which suspends listening without ending the recognition loop<br>
    [[OEPocketsphinxController sharedInstance] suspendRecognition];	</p>
<p>    self.startButton.hidden = TRUE;<br>
    self.stopButton.hidden = FALSE;<br>
    self.suspendListeningButton.hidden = TRUE;<br>
    self.resumeListeningButton.hidden = FALSE;<br>
}</p>
<p>&#8211; (IBAction) resumeListeningButtonAction { // This is the action for the button which resumes listening if it has been suspended<br>
    [[OEPocketsphinxController sharedInstance] resumeRecognition];</p>
<p>    self.startButton.hidden = TRUE;<br>
    self.stopButton.hidden = FALSE;<br>
    self.suspendListeningButton.hidden = FALSE;<br>
    self.resumeListeningButton.hidden = TRUE;<br>
}</p>
<p>&#8211; (IBAction) stopButtonAction { // This is the action for the button which shuts down the recognition loop.<br>
    NSError *error = nil;<br>
    if([OEPocketsphinxController sharedInstance].isListening) { // Stop if we are currently listening.<br>
        error = [[OEPocketsphinxController sharedInstance] stopListening];<br>
        if(error)NSLog(@&#8221;Error stopping listening in stopButtonAction: %@&#8221;, error);<br>
    }<br>
    self.startButton.hidden = FALSE;<br>
    self.stopButton.hidden = TRUE;<br>
    self.suspendListeningButton.hidden = TRUE;<br>
    self.resumeListeningButton.hidden = TRUE;<br>
}</p>
<p>&#8211; (IBAction) startButtonAction { // This is the action for the button which starts up the recognition loop again if it has been shut down.<br>
    if(![OEPocketsphinxController sharedInstance].isListening) {<br>
        [[OEPocketsphinxController sharedInstance] startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;] languageModelIsJSGF:FALSE]; // Start speech recognition if we aren&#8217;t already listening.<br>
    }<br>
    self.startButton.hidden = TRUE;<br>
    self.stopButton.hidden = FALSE;<br>
    self.suspendListeningButton.hidden = FALSE;<br>
    self.resumeListeningButton.hidden = TRUE;<br>
}</p>
<p>#pragma mark &#8211;<br>
#pragma mark Example for reading out Pocketsphinx and Flite audio levels without locking the UI by using an NSTimer</p>
<p>// What follows are not OpenEars methods, just an approach for level reading<br>
// that I&#8217;ve included with this sample app. My example implementation does make use of two OpenEars<br>
// methods:	the pocketsphinxInputLevel method of OEPocketsphinxController and the fliteOutputLevel<br>
// method of OEFliteController.<br>
//<br>
// The example is meant to show one way that you can read those levels continuously without locking the UI,<br>
// by using an NSTimer, but the OpenEars level-reading methods<br>
// themselves do not include multithreading code since I believe that you will want to design your own<br>
// code approaches for level display that are tightly-integrated with your interaction design and the<br>
// graphics API you choose.<br>
//<br>
// Please note that if you use my sample approach, you should pay attention to the way that the timer is always stopped in<br>
// dealloc. This should prevent you from having any difficulties with deallocating a class due to a running NSTimer process.</p>
<p>&#8211; (void) startDisplayingLevels { // Start displaying the levels using a timer<br>
    [self stopDisplayingLevels]; // We never want more than one timer valid so we&#8217;ll stop any running timers first.<br>
    self.uiUpdateTimer = [NSTimer scheduledTimerWithTimeInterval:1.0/kLevelUpdatesPerSecond target:self selector:@selector(updateLevelsUI) userInfo:nil repeats:YES];<br>
}</p>
<p>&#8211; (void) stopDisplayingLevels { // Stop displaying the levels by stopping the timer if it&#8217;s running.<br>
    if(self.uiUpdateTimer &amp;&amp; [self.uiUpdateTimer isValid]) { // If there is a running timer, we&#8217;ll stop it here.<br>
        [self.uiUpdateTimer invalidate];<br>
        self.uiUpdateTimer = nil;<br>
    }<br>
}</p>
<p>&#8211; (void) updateLevelsUI { // And here is how we obtain the levels.  This method includes the actual OpenEars methods and uses their results to update the UI of this view controller.</p>
<p>    self.pocketsphinxDbLabel.text = [NSString stringWithFormat:@&#8221;Pocketsphinx Input level:%f&#8221;,[[OEPocketsphinxController sharedInstance] pocketsphinxInputLevel]];  //pocketsphinxInputLevel is an OpenEars method of the class OEPocketsphinxController.</p>
<p>    if(self.fliteController.speechInProgress) {<br>
        self.fliteDbLabel.text = [NSString stringWithFormat:@&#8221;Flite Output level: %f&#8221;,[self.fliteController fliteOutputLevel]]; // fliteOutputLevel is an OpenEars method of the class OEFliteController.<br>
    }<br>
}</p>
<p>@end<br>
[/spoiler]</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1030134" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 21, 2016 at 12:45 pm</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1030134" class="bbp-reply-permalink">#1030134</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1030134 -->

<div class="loop-item-23 user-id-4 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-29 even  post-1030134 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/halle-2/" title="View Halle Winkler&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Halle Winkler</span></a><div class="bbp-author-role">Politepix</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>It is definitely not in the logging for the Instruments case you sent so I&#8217;m not sure how to proceed when I can&#8217;t replicate it (I&#8217;ve run it with every example of challenging audio that I have and I&#8217;m out of ideas for how to get it to happen – this was an issue with much older OpenEars versions but a fix was added for it, so this is a bit mysterious, especially in combination with it being due to a 3rd-pass search in your previously-sent logs and then apparently still happening when you turn off 3rd-pass searches). Can you create a full replication case according to this post, that you can run and see behave in the same way: </p>
<p><a href="/forums/topic/how-to-create-a-minimal-case-for-replication/">https://www.politepix.com/forums/topic/how-to-create-a-minimal-case-for-replication/</a></p>
<p>And send me a link via the contact form? Thank you.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1030135" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 21, 2016 at 12:56 pm</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1030135" class="bbp-reply-permalink">#1030135</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1030135 -->

<div class="loop-item-24 user-id-4 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-25 odd  post-1030135 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/halle-2/" title="View Halle Winkler&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Halle Winkler</span></a><div class="bbp-author-role">Politepix</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>What you&#8217;re looking for in a replication case is that when you use your recorded audio from the SaveThatWave demo as a test audio file using pathToTestFile, the call to stop listening gets stuck unable to stop gracefully and then afterwards (like a minute afterwards) the full amount of memory used at the time of attempting to stop is still allocated.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1030136" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 21, 2016 at 1:00 pm</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1030136" class="bbp-reply-permalink">#1030136</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1030136 -->

<div class="loop-item-25 user-id-2022 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-26 even topic-author  post-1030136 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/laurent-oscadi/" title="View Laurent&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Laurent</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>I don&#8217;t see any rapidEars 2.5 in OELogging in the sample app with the rejecto demo and rapidEars demo why? it is with the code I just posted</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1030137" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 21, 2016 at 1:07 pm</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1030137" class="bbp-reply-permalink">#1030137</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1030137 -->

<div class="loop-item-26 user-id-4 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-32 odd  post-1030137 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/halle-2/" title="View Halle Winkler&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Halle Winkler</span></a><div class="bbp-author-role">Politepix</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>The code you just posted won&#8217;t work with the RapidEars demo at all, it links to a licensed version.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1030138" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 21, 2016 at 1:10 pm</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1030138" class="bbp-reply-permalink">#1030138</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1030138 -->

<div class="loop-item-27 user-id-2022 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-33 even topic-author  post-1030138 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/laurent-oscadi/" title="View Laurent&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Laurent</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Yes. I have changed the imports to the demo one. Sorry  This is the good one:<br>
But it still not log RapidEars 2.5 with the demo<br>
[spoiler]</p>
<p>#import &#8220;ViewController.h&#8221;<br>
#import &lt;OpenEars/OEPocketsphinxController.h&gt;<br>
#import &lt;RapidEarsDemo/OEPocketsphinxController+RapidEars.h&gt;<br>
#import &lt;OpenEars/OEFliteController.h&gt;<br>
#import &lt;OpenEars/OELanguageModelGenerator.h&gt;<br>
#import &lt;RejectoDemo/OELanguageModelGenerator+Rejecto.h&gt;<br>
#import &lt;OpenEars/OELogging.h&gt;<br>
#import &lt;OpenEars/OEAcousticModel.h&gt;<br>
#import &lt;Slt/Slt.h&gt;<br>
#import &lt;OpenEars/OELanguageModelGenerator.h&gt;<br>
#import &lt;OpenEars/OEEventsObserver.h&gt;<br>
#import &lt;RapidEarsDemo/OEEventsObserver+RapidEars.h&gt;<br>
@interface ViewController()</p>
<p>// UI actions, not specifically related to OpenEars other than the fact that they invoke OpenEars methods.<br>
&#8211; (IBAction) stopButtonAction;<br>
&#8211; (IBAction) startButtonAction;<br>
&#8211; (IBAction) suspendListeningButtonAction;<br>
&#8211; (IBAction) resumeListeningButtonAction;</p>
<p>// Example for reading out the input audio levels without locking the UI using an NSTimer</p>
<p>&#8211; (void) startDisplayingLevels;<br>
&#8211; (void) stopDisplayingLevels;</p>
<p>// These three are the important OpenEars objects that this class demonstrates the use of.<br>
@property (nonatomic, strong) Slt *slt;</p>
<p>@property (nonatomic, strong) OEEventsObserver *openEarsEventsObserver;<br>
@property (nonatomic, strong) OEPocketsphinxController *pocketsphinxController;<br>
@property (nonatomic, strong) OEFliteController *fliteController;</p>
<p>// Some UI, not specifically related to OpenEars.<br>
@property (nonatomic, strong) IBOutlet UIButton *stopButton;<br>
@property (nonatomic, strong) IBOutlet UIButton *startButton;<br>
@property (nonatomic, strong) IBOutlet UIButton *suspendListeningButton;<br>
@property (nonatomic, strong) IBOutlet UIButton *resumeListeningButton;<br>
@property (nonatomic, strong) IBOutlet UITextView *statusTextView;<br>
@property (nonatomic, strong) IBOutlet UITextView *heardTextView;<br>
@property (nonatomic, strong) IBOutlet UILabel *pocketsphinxDbLabel;<br>
@property (nonatomic, strong) IBOutlet UILabel *fliteDbLabel;<br>
@property (nonatomic, assign) BOOL usingStartingLanguageModel;<br>
@property (nonatomic, assign) int restartAttemptsDueToPermissionRequests;<br>
@property (nonatomic, assign) BOOL startupFailedDueToLackOfPermissions;</p>
<p>// Things which help us show off the dynamic language features.<br>
@property (nonatomic, copy) NSString *pathToFirstDynamicallyGeneratedLanguageModel;<br>
@property (nonatomic, copy) NSString *pathToFirstDynamicallyGeneratedDictionary;<br>
@property (nonatomic, copy) NSString *pathToSecondDynamicallyGeneratedLanguageModel;<br>
@property (nonatomic, copy) NSString *pathToSecondDynamicallyGeneratedDictionary;</p>
<p>// Our NSTimer that will help us read and display the input and output levels without locking the UI<br>
@property (nonatomic, strong) 	NSTimer *uiUpdateTimer;</p>
<p>@end</p>
<p>@implementation ViewController</p>
<p>#define kLevelUpdatesPerSecond 18 // We&#8217;ll have the ui update 18 times a second to show some fluidity without hitting the CPU too hard.</p>
<p>//#define kGetNbest // Uncomment this if you want to try out nbest<br>
#pragma mark &#8211;<br>
#pragma mark Memory Management</p>
<p>&#8211; (void)dealloc {<br>
    [self stopDisplayingLevels];<br>
}</p>
<p>#pragma mark &#8211;<br>
#pragma mark View Lifecycle</p>
<p>&#8211; (void)viewDidLoad {<br>
    [super viewDidLoad];<br>
    self.fliteController = [[OEFliteController alloc] init];<br>
    self.openEarsEventsObserver = [[OEEventsObserver alloc] init];<br>
    self.openEarsEventsObserver.delegate = self;<br>
    self.slt = [[Slt alloc] init];</p>
<p>    self.restartAttemptsDueToPermissionRequests = 0;<br>
    self.startupFailedDueToLackOfPermissions = FALSE;</p>
<p>    [OEPocketsphinxController sharedInstance].verbosePocketSphinx = TRUE; // Uncomment this for much more verbose speech recognition engine output. If you have issues, show this logging in the forums.</p>
<p>    [self.openEarsEventsObserver setDelegate:self]; // Make this class the delegate of OpenEarsObserver so we can get all of the messages about what OpenEars is doing.</p>
<p>    [[OEPocketsphinxController sharedInstance] setActive:TRUE error:nil]; // Call this before setting any OEPocketsphinxController characteristics</p>
<p>    // This is the language model we&#8217;re going to start up with. The only reason I&#8217;m making it a class property is that I reuse it a bunch of times in this example,<br>
//     but you can pass the string contents directly to OEPocketsphinxController:startListeningWithLanguageModelAtPath:dictionaryAtPath:languageModelIsJSGF:</p>
<p>    NSArray *firstLanguageArray = @[@&#8221;BACKWARD&#8221;,<br>
                                    @&#8221;CHANGE&#8221;,<br>
                                    @&#8221;FORWARD&#8221;,<br>
                                    @&#8221;GO&#8221;,<br>
                                    @&#8221;LEFT&#8221;,<br>
                                    @&#8221;MODEL&#8221;,<br>
                                    @&#8221;RIGHT&#8221;,<br>
                                    @&#8221;TURN&#8221;];</p>
<p>    OELanguageModelGenerator *languageModelGenerator = [[OELanguageModelGenerator alloc] init];<br>
    [OELogging startOpenEarsLogging]; // Uncomment me for OELogging, which is verbose logging about internal OpenEars operations such as audio settings. If you have issues, show this logging in the forums.</p>
<p>    // languageModelGenerator.verboseLanguageModelGenerator = TRUE; // Uncomment me for verbose language model generator debug output.</p>
<p>    NSError *error = [languageModelGenerator generateRejectingLanguageModelFromArray:firstLanguageArray withFilesNamed:@&#8221;FirstOpenEarsDynamicLanguageModel&#8221; withOptionalExclusions:nil<br>
                                                                     usingVowelsOnly:FALSE<br>
                                                                          withWeight:[ NSNumber numberWithFloat:2.0 ]<br>
                                                              forAcousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;]]; // Change &#8220;AcousticModelEnglish&#8221; to &#8220;AcousticModelSpanish&#8221; in order to create a language model for Spanish recognition instead of English.<br>
        if(error) {<br>
        NSLog(@&#8221;Dynamic language generator reported error %@&#8221;, [error description]);<br>
    } else {<br>
        self.pathToFirstDynamicallyGeneratedLanguageModel = [languageModelGenerator pathToSuccessfullyGeneratedLanguageModelWithRequestedName:@&#8221;FirstOpenEarsDynamicLanguageModel&#8221;];<br>
        self.pathToFirstDynamicallyGeneratedDictionary = [languageModelGenerator pathToSuccessfullyGeneratedDictionaryWithRequestedName:@&#8221;FirstOpenEarsDynamicLanguageModel&#8221;];<br>
    }</p>
<p>    self.usingStartingLanguageModel = TRUE; // This is not an OpenEars thing, this is just so I can switch back and forth between the two models in this sample app.</p>
<p>    // Here is an example of dynamically creating an in-app grammar.</p>
<p>    // We want it to be able to response to the speech &#8220;CHANGE MODEL&#8221; and a few other things.  Items we want to have recognized as a whole phrase (like &#8220;CHANGE MODEL&#8221;)<br>
    // we put into the array as one string (e.g. &#8220;CHANGE MODEL&#8221; instead of &#8220;CHANGE&#8221; and &#8220;MODEL&#8221;). This increases the probability that they will be recognized as a phrase. This works even better starting with version 1.0 of OpenEars.</p>
<p>    NSArray *secondLanguageArray = @[@&#8221;SUNDAY&#8221;,<br>
                                     @&#8221;MONDAY&#8221;,<br>
                                     @&#8221;TUESDAY&#8221;,<br>
                                     @&#8221;WEDNESDAY&#8221;,<br>
                                     @&#8221;THURSDAY&#8221;,<br>
                                     @&#8221;FRIDAY&#8221;,<br>
                                     @&#8221;SATURDAY&#8221;,<br>
                                     @&#8221;QUIDNUNC&#8221;,<br>
                                     @&#8221;CHANGE MODEL&#8221;];</p>
<p>    // The last entry, quidnunc, is an example of a word which will not be found in the lookup dictionary and will be passed to the fallback method. The fallback method is slower,<br>
    // so, for instance, creating a new language model from dictionary words will be pretty fast, but a model that has a lot of unusual names in it or invented/rare/recent-slang<br>
    // words will be slower to generate. You can use this information to give your users good UI feedback about what the expectations for wait times should be.</p>
<p>    // I don&#8217;t think it&#8217;s beneficial to lazily instantiate OELanguageModelGenerator because you only need to give it a single message and then release it.<br>
    // If you need to create a very large model or any size of model that has many unusual words that have to make use of the fallback generation method,<br>
    // you will want to run this on a background thread so you can give the user some UI feedback that the task is in progress.</p>
<p>    // generateLanguageModelFromArray:withFilesNamed returns an NSError which will either have a value of noErr if everything went fine or a specific error if it didn&#8217;t.<br>
    error = [languageModelGenerator generateRejectingLanguageModelFromArray:secondLanguageArray withFilesNamed:@&#8221;SecondOpenEarsDynamicLanguageModel&#8221;  withOptionalExclusions:nil<br>
                                                            usingVowelsOnly:FALSE<br>
                                                                 withWeight:[ NSNumber numberWithFloat:2.0 ]<br>
                                                     forAcousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;]]; // Change &#8220;AcousticModelEnglish&#8221; to &#8220;AcousticModelSpanish&#8221; in order to create a language model for Spanish recognition instead of English.</p>
<p>    //    NSError *error = [languageModelGenerator generateLanguageModelFromTextFile:[NSString stringWithFormat:@&#8221;%@/%@&#8221;,[[NSBundle mainBundle] resourcePath], @&#8221;OpenEarsCorpus.txt&#8221;] withFilesNamed:@&#8221;SecondOpenEarsDynamicLanguageModel&#8221; forAcousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;]]; // Try this out to see how generating a language model from a corpus works.</p>
<p>    if(error) {<br>
        NSLog(@&#8221;Dynamic language generator reported error %@&#8221;, [error description]);<br>
    }	else {</p>
<p>        self.pathToSecondDynamicallyGeneratedLanguageModel = [languageModelGenerator pathToSuccessfullyGeneratedLanguageModelWithRequestedName:@&#8221;SecondOpenEarsDynamicLanguageModel&#8221;]; // We&#8217;ll set our new .languagemodel file to be the one to get switched to when the words &#8220;CHANGE MODEL&#8221; are recognized.<br>
        self.pathToSecondDynamicallyGeneratedDictionary = [languageModelGenerator pathToSuccessfullyGeneratedDictionaryWithRequestedName:@&#8221;SecondOpenEarsDynamicLanguageModel&#8221;];; // We&#8217;ll set our new dictionary to be the one to get switched to when the words &#8220;CHANGE MODEL&#8221; are recognized.</p>
<p>        // Next, an informative message.</p>
<p>        NSLog(@&#8221;\n\nWelcome to the OpenEars sample project. This project understands the words:\nBACKWARD,\nCHANGE,\nFORWARD,\nGO,\nLEFT,\nMODEL,\nRIGHT,\nTURN,\nand if you say \&#8221;CHANGE MODEL\&#8221; it will switch to its dynamically-generated model which understands the words:\nCHANGE,\nMODEL,\nMONDAY,\nTUESDAY,\nWEDNESDAY,\nTHURSDAY,\nFRIDAY,\nSATURDAY,\nSUNDAY,\nQUIDNUNC&#8221;);</p>
<p>        // This is how to start the continuous listening loop of an available instance of OEPocketsphinxController. We won&#8217;t do this if the language generation failed since it will be listening for a command to change over to the generated language.</p>
<p>        [[OEPocketsphinxController sharedInstance] setActive:TRUE error:nil]; // Call this once before setting properties of the OEPocketsphinxController instance.</p>
<p>        //   [OEPocketsphinxController sharedInstance].pathToTestFile = [[NSBundle mainBundle] pathForResource:@&#8221;change_model_short&#8221; ofType:@&#8221;wav&#8221;];  // This is how you could use a test WAV (mono/16-bit/16k) rather than live recognition. Don&#8217;t forget to add your WAV to your app bundle.</p>
<p>        if(![OEPocketsphinxController sharedInstance].isListening) {<br>
            [[OEPocketsphinxController sharedInstance] startRealtimeListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;]];</p>
<p>//            [[OEPocketsphinxController sharedInstance] startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;] languageModelIsJSGF:FALSE]; // Start speech recognition if we aren&#8217;t already listening.<br>
        }<br>
        // [self startDisplayingLevels] is not an OpenEars method, just a very simple approach for level reading<br>
        // that I&#8217;ve included with this sample app. My example implementation does make use of two OpenEars<br>
        // methods:	the pocketsphinxInputLevel method of OEPocketsphinxController and the fliteOutputLevel<br>
        // method of fliteController.<br>
        //<br>
        // The example is meant to show one way that you can read those levels continuously without locking the UI,<br>
        // by using an NSTimer, but the OpenEars level-reading methods<br>
        // themselves do not include multithreading code since I believe that you will want to design your own<br>
        // code approaches for level display that are tightly-integrated with your interaction design and the<br>
        // graphics API you choose. </p>
<p>        [self startDisplayingLevels];</p>
<p>        // Here is some UI stuff that has nothing specifically to do with OpenEars implementation<br>
        self.startButton.hidden = TRUE;<br>
        self.stopButton.hidden = TRUE;<br>
        self.suspendListeningButton.hidden = TRUE;<br>
        self.resumeListeningButton.hidden = TRUE;<br>
    }<br>
}</p>
<p>#pragma mark &#8211;<br>
#pragma mark OEEventsObserver delegate methods</p>
<p>// What follows are all of the delegate methods you can optionally use once you&#8217;ve instantiated an OEEventsObserver and set its delegate to self.<br>
// I&#8217;ve provided some pretty granular information about the exact phase of the Pocketsphinx listening loop, the Audio Session, and Flite, but I&#8217;d expect<br>
// that the ones that will really be needed by most projects are the following:<br>
//<br>
//- (void) pocketsphinxDidReceiveHypothesis:(NSString *)hypothesis recognitionScore:(NSString *)recognitionScore utteranceID:(NSString *)utteranceID;<br>
//- (void) audioSessionInterruptionDidBegin;<br>
//- (void) audioSessionInterruptionDidEnd;<br>
//- (void) audioRouteDidChangeToRoute:(NSString *)newRoute;<br>
//- (void) pocketsphinxDidStartListening;<br>
//- (void) pocketsphinxDidStopListening;<br>
//<br>
// It isn&#8217;t necessary to have a OEPocketsphinxController or a OEFliteController instantiated in order to use these methods.  If there isn&#8217;t anything instantiated that will<br>
// send messages to an OEEventsObserver, all that will happen is that these methods will never fire.  You also do not have to create a OEEventsObserver in<br>
// the same class or view controller in which you are doing things with a OEPocketsphinxController or OEFliteController; you can receive updates from those objects in<br>
// any class in which you instantiate an OEEventsObserver and set its delegate to self.</p>
<p>// This is an optional delegate method of OEEventsObserver which delivers the text of speech that Pocketsphinx heard and analyzed, along with its accuracy score and utterance ID.<br>
&#8211; (void) pocketsphinxDidReceiveHypothesis:(NSString *)hypothesis recognitionScore:(NSString *)recognitionScore utteranceID:(NSString *)utteranceID {</p>
<p>    NSLog(@&#8221;Local callback: The received hypothesis is %@ with a score of %@ and an ID of %@&#8221;, hypothesis, recognitionScore, utteranceID); // Log it.<br>
    if([hypothesis isEqualToString:@&#8221;CHANGE MODEL&#8221;]) { // If the user says &#8220;CHANGE MODEL&#8221;, we will switch to the alternate model (which happens to be the dynamically generated model).</p>
<p>        // Here is an example of language model switching in OpenEars. Deciding on what logical basis to switch models is your responsibility.<br>
        // For instance, when you call a customer service line and get a response tree that takes you through different options depending on what you say to it,<br>
        // the models are being switched as you progress through it so that only relevant choices can be understood. The construction of that logical branching and<br>
        // how to react to it is your job; OpenEars just lets you send the signal to switch the language model when you&#8217;ve decided it&#8217;s the right time to do so.</p>
<p>        if(self.usingStartingLanguageModel) { // If we&#8217;re on the starting model, switch to the dynamically generated one.</p>
<p>            [[OEPocketsphinxController sharedInstance] changeLanguageModelToFile:self.pathToSecondDynamicallyGeneratedLanguageModel withDictionary:self.pathToSecondDynamicallyGeneratedDictionary];<br>
            self.usingStartingLanguageModel = FALSE;</p>
<p>        } else { // If we&#8217;re on the dynamically generated model, switch to the start model (this is an example of a trigger and method for switching models).</p>
<p>            [[OEPocketsphinxController sharedInstance] changeLanguageModelToFile:self.pathToFirstDynamicallyGeneratedLanguageModel withDictionary:self.pathToFirstDynamicallyGeneratedDictionary];<br>
            self.usingStartingLanguageModel = TRUE;<br>
        }<br>
    }</p>
<p>    self.heardTextView.text = [NSString stringWithFormat:@&#8221;Heard: \&#8221;%@\&#8221;&#8221;, hypothesis]; // Show it in the status box.</p>
<p>    // This is how to use an available instance of OEFliteController. We&#8217;re going to repeat back the command that we heard with the voice we&#8217;ve chosen.<br>
    [self.fliteController say:[NSString stringWithFormat:@&#8221;You said %@&#8221;,hypothesis] withVoice:self.slt];<br>
}</p>
<p>#ifdef kGetNbest<br>
&#8211; (void) pocketsphinxDidReceiveNBestHypothesisArray:(NSArray *)hypothesisArray { // Pocketsphinx has an n-best hypothesis dictionary.<br>
    NSLog(@&#8221;Local callback:  hypothesisArray is %@&#8221;,hypothesisArray);<br>
}<br>
#endif<br>
// An optional delegate method of OEEventsObserver which informs that there was an interruption to the audio session (e.g. an incoming phone call).<br>
&#8211; (void) audioSessionInterruptionDidBegin {<br>
    NSLog(@&#8221;Local callback:  AudioSession interruption began.&#8221;); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: AudioSession interruption began.&#8221;; // Show it in the status box.<br>
    NSError *error = nil;<br>
    if([OEPocketsphinxController sharedInstance].isListening) {<br>
        error = [[OEPocketsphinxController sharedInstance] stopListening]; // React to it by telling Pocketsphinx to stop listening (if it is listening) since it will need to restart its loop after an interruption.<br>
        if(error) NSLog(@&#8221;Error while stopping listening in audioSessionInterruptionDidBegin: %@&#8221;, error);<br>
    }<br>
}</p>
<p>// An optional delegate method of OEEventsObserver which informs that the interruption to the audio session ended.<br>
&#8211; (void) audioSessionInterruptionDidEnd {<br>
    NSLog(@&#8221;Local callback:  AudioSession interruption ended.&#8221;); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: AudioSession interruption ended.&#8221;; // Show it in the status box.<br>
    // We&#8217;re restarting the previously-stopped listening loop.<br>
    if(![OEPocketsphinxController sharedInstance].isListening){<br>
            [[OEPocketsphinxController sharedInstance] startRealtimeListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;]];<br>
//        [[OEPocketsphinxController sharedInstance] startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;] languageModelIsJSGF:FALSE]; // Start speech recognition if we aren&#8217;t currently listening.<br>
    }<br>
}</p>
<p>// An optional delegate method of OEEventsObserver which informs that the audio input became unavailable.<br>
&#8211; (void) audioInputDidBecomeUnavailable {<br>
    NSLog(@&#8221;Local callback:  The audio input has become unavailable&#8221;); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: The audio input has become unavailable&#8221;; // Show it in the status box.<br>
    NSError *error = nil;<br>
    if([OEPocketsphinxController sharedInstance].isListening){<br>
        error = [[OEPocketsphinxController sharedInstance] stopListening]; // React to it by telling Pocketsphinx to stop listening since there is no available input (but only if we are listening).<br>
        if(error) NSLog(@&#8221;Error while stopping listening in audioInputDidBecomeUnavailable: %@&#8221;, error);<br>
    }<br>
}</p>
<p>// An optional delegate method of OEEventsObserver which informs that the unavailable audio input became available again.<br>
&#8211; (void) audioInputDidBecomeAvailable {<br>
    NSLog(@&#8221;Local callback: The audio input is available&#8221;); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: The audio input is available&#8221;; // Show it in the status box.<br>
    if(![OEPocketsphinxController sharedInstance].isListening) {<br>
            [[OEPocketsphinxController sharedInstance] startRealtimeListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;]];<br>
//        [[OEPocketsphinxController sharedInstance] startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;] languageModelIsJSGF:FALSE]; // Start speech recognition, but only if we aren&#8217;t already listening.<br>
    }<br>
}<br>
// An optional delegate method of OEEventsObserver which informs that there was a change to the audio route (e.g. headphones were plugged in or unplugged).<br>
&#8211; (void) audioRouteDidChangeToRoute:(NSString *)newRoute {<br>
    NSLog(@&#8221;Local callback: Audio route change. The new audio route is %@&#8221;, newRoute); // Log it.<br>
    self.statusTextView.text = [NSString stringWithFormat:@&#8221;Status: Audio route change. The new audio route is %@&#8221;,newRoute]; // Show it in the status box.</p>
<p>    NSError *error = [[OEPocketsphinxController sharedInstance] stopListening]; // React to it by telling the Pocketsphinx loop to shut down and then start listening again on the new route</p>
<p>    if(error)NSLog(@&#8221;Local callback: error while stopping listening in audioRouteDidChangeToRoute: %@&#8221;,error);</p>
<p>    if(![OEPocketsphinxController sharedInstance].isListening) {<br>
            [[OEPocketsphinxController sharedInstance] startRealtimeListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;]];<br>
//        [[OEPocketsphinxController sharedInstance] startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;] languageModelIsJSGF:FALSE]; // Start speech recognition if we aren&#8217;t already listening.<br>
    }<br>
}</p>
<p>// An optional delegate method of OEEventsObserver which informs that the Pocketsphinx recognition loop has entered its actual loop.<br>
// This might be useful in debugging a conflict between another sound class and Pocketsphinx.<br>
&#8211; (void) pocketsphinxRecognitionLoopDidStart {</p>
<p>    NSLog(@&#8221;Local callback: Pocketsphinx started.&#8221;); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: Pocketsphinx started.&#8221;; // Show it in the status box.<br>
}</p>
<p>// An optional delegate method of OEEventsObserver which informs that Pocketsphinx is now listening for speech.<br>
&#8211; (void) pocketsphinxDidStartListening {</p>
<p>    NSLog(@&#8221;Local callback: Pocketsphinx is now listening.&#8221;); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: Pocketsphinx is now listening.&#8221;; // Show it in the status box.</p>
<p>    self.startButton.hidden = TRUE; // React to it with some UI changes.<br>
    self.stopButton.hidden = FALSE;<br>
    self.suspendListeningButton.hidden = FALSE;<br>
    self.resumeListeningButton.hidden = TRUE;<br>
}</p>
<p>// An optional delegate method of OEEventsObserver which informs that Pocketsphinx detected speech and is starting to process it.<br>
&#8211; (void) pocketsphinxDidDetectSpeech {<br>
    NSLog(@&#8221;Local callback: Pocketsphinx has detected speech.&#8221;); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: Pocketsphinx has detected speech.&#8221;; // Show it in the status box.<br>
}</p>
<p>// An optional delegate method of OEEventsObserver which informs that Pocketsphinx detected a second of silence, indicating the end of an utterance.<br>
// This was added because developers requested being able to time the recognition speed without the speech time. The processing time is the time between<br>
// this method being called and the hypothesis being returned.<br>
&#8211; (void) pocketsphinxDidDetectFinishedSpeech {<br>
    NSLog(@&#8221;Local callback: Pocketsphinx has detected a second of silence, concluding an utterance.&#8221;); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: Pocketsphinx has detected finished speech.&#8221;; // Show it in the status box.<br>
}</p>
<p>// An optional delegate method of OEEventsObserver which informs that Pocketsphinx has exited its recognition loop, most<br>
// likely in response to the OEPocketsphinxController being told to stop listening via the stopListening method.<br>
&#8211; (void) pocketsphinxDidStopListening {<br>
    NSLog(@&#8221;Local callback: Pocketsphinx has stopped listening.&#8221;); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: Pocketsphinx has stopped listening.&#8221;; // Show it in the status box.<br>
}</p>
<p>// An optional delegate method of OEEventsObserver which informs that Pocketsphinx is still in its listening loop but it is not<br>
// Going to react to speech until listening is resumed.  This can happen as a result of Flite speech being<br>
// in progress on an audio route that doesn&#8217;t support simultaneous Flite speech and Pocketsphinx recognition,<br>
// or as a result of the OEPocketsphinxController being told to suspend recognition via the suspendRecognition method.<br>
&#8211; (void) pocketsphinxDidSuspendRecognition {<br>
    NSLog(@&#8221;Local callback: Pocketsphinx has suspended recognition.&#8221;); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: Pocketsphinx has suspended recognition.&#8221;; // Show it in the status box.<br>
}</p>
<p>// An optional delegate method of OEEventsObserver which informs that Pocketsphinx is still in its listening loop and after recognition<br>
// having been suspended it is now resuming.  This can happen as a result of Flite speech completing<br>
// on an audio route that doesn&#8217;t support simultaneous Flite speech and Pocketsphinx recognition,<br>
// or as a result of the OEPocketsphinxController being told to resume recognition via the resumeRecognition method.<br>
&#8211; (void) pocketsphinxDidResumeRecognition {<br>
    NSLog(@&#8221;Local callback: Pocketsphinx has resumed recognition.&#8221;); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: Pocketsphinx has resumed recognition.&#8221;; // Show it in the status box.<br>
}</p>
<p>// An optional delegate method which informs that Pocketsphinx switched over to a new language model at the given URL in the course of<br>
// recognition. This does not imply that it is a valid file or that recognition will be successful using the file.<br>
&#8211; (void) pocketsphinxDidChangeLanguageModelToFile:(NSString *)newLanguageModelPathAsString andDictionary:(NSString *)newDictionaryPathAsString {<br>
    NSLog(@&#8221;Local callback: Pocketsphinx is now using the following language model: \n%@ and the following dictionary: %@&#8221;,newLanguageModelPathAsString,newDictionaryPathAsString);<br>
}</p>
<p>// An optional delegate method of OEEventsObserver which informs that Flite is speaking, most likely to be useful if debugging a<br>
// complex interaction between sound classes. You don&#8217;t have to do anything yourself in order to prevent Pocketsphinx from listening to Flite talk and trying to recognize the speech.<br>
&#8211; (void) fliteDidStartSpeaking {<br>
    NSLog(@&#8221;Local callback: Flite has started speaking&#8221;); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: Flite has started speaking.&#8221;; // Show it in the status box.<br>
}</p>
<p>// An optional delegate method of OEEventsObserver which informs that Flite is finished speaking, most likely to be useful if debugging a<br>
// complex interaction between sound classes.<br>
&#8211; (void) fliteDidFinishSpeaking {<br>
    NSLog(@&#8221;Local callback: Flite has finished speaking&#8221;); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: Flite has finished speaking.&#8221;; // Show it in the status box.<br>
}</p>
<p>&#8211; (void) pocketSphinxContinuousSetupDidFailWithReason:(NSString *)reasonForFailure { // This can let you know that something went wrong with the recognition loop startup. Turn on [OELogging startOpenEarsLogging] to learn why.<br>
    NSLog(@&#8221;Local callback: Setting up the continuous recognition loop has failed for the reason %@, please turn on [OELogging startOpenEarsLogging] to learn more.&#8221;, reasonForFailure); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: Not possible to start recognition loop.&#8221;; // Show it in the status box.<br>
}</p>
<p>&#8211; (void) pocketSphinxContinuousTeardownDidFailWithReason:(NSString *)reasonForFailure { // This can let you know that something went wrong with the recognition loop startup. Turn on [OELogging startOpenEarsLogging] to learn why.<br>
    NSLog(@&#8221;Local callback: Tearing down the continuous recognition loop has failed for the reason %@, please turn on [OELogging startOpenEarsLogging] to learn more.&#8221;, reasonForFailure); // Log it.<br>
    self.statusTextView.text = @&#8221;Status: Not possible to cleanly end recognition loop.&#8221;; // Show it in the status box.<br>
}</p>
<p>&#8211; (void) testRecognitionCompleted { // A test file which was submitted for direct recognition via the audio driver is done.<br>
    NSLog(@&#8221;Local callback: A test file which was submitted for direct recognition via the audio driver is done.&#8221;); // Log it.<br>
    NSError *error = nil;<br>
    if([OEPocketsphinxController sharedInstance].isListening) { // If we&#8217;re listening, stop listening.<br>
        error = [[OEPocketsphinxController sharedInstance] stopListening];<br>
        if(error) NSLog(@&#8221;Error while stopping listening in testRecognitionCompleted: %@&#8221;, error);<br>
    }</p>
<p>}<br>
&#8211; (void) rapidEarsDidReceiveLiveSpeechHypothesis:(NSString *)hypothesis recognitionScore:(NSString *)recognitionScore {<br>
    NSLog(@&#8221;rapidEarsDidReceiveLiveSpeechHypothesis: %@&#8221;,hypothesis);<br>
}</p>
<p>&#8211; (void) rapidEarsDidReceiveFinishedSpeechHypothesis:(NSString *)hypothesis recognitionScore:(NSString *)recognitionScore {<br>
    NSLog(@&#8221;rapidEarsDidReceiveFinishedSpeechHypothesis: %@&#8221;,hypothesis);<br>
}<br>
/** Pocketsphinx couldn&#8217;t start because it has no mic permissions (will only be returned on iOS7 or later).*/<br>
&#8211; (void) pocketsphinxFailedNoMicPermissions {<br>
    NSLog(@&#8221;Local callback: The user has never set mic permissions or denied permission to this app&#8217;s mic, so listening will not start.&#8221;);<br>
    self.startupFailedDueToLackOfPermissions = TRUE;<br>
    if([OEPocketsphinxController sharedInstance].isListening){<br>
        NSError *error = [[OEPocketsphinxController sharedInstance] stopListening]; // Stop listening if we are listening.<br>
        if(error) NSLog(@&#8221;Error while stopping listening in micPermissionCheckCompleted: %@&#8221;, error);<br>
    }<br>
}</p>
<p>/** The user prompt to get mic permissions, or a check of the mic permissions, has completed with a TRUE or a FALSE result  (will only be returned on iOS7 or later).*/<br>
&#8211; (void) micPermissionCheckCompleted:(BOOL)result {<br>
    if(result) {<br>
        self.restartAttemptsDueToPermissionRequests++;<br>
        if(self.restartAttemptsDueToPermissionRequests == 1 &amp;&amp; self.startupFailedDueToLackOfPermissions) { // If we get here because there was an attempt to start which failed due to lack of permissions, and now permissions have been requested and they returned true, we restart exactly once with the new permissions.</p>
<p>            if(![OEPocketsphinxController sharedInstance].isListening) { // If there was no error and we aren&#8217;t listening, start listening.<br>
                     [[OEPocketsphinxController sharedInstance] startRealtimeListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;]];<br>
//                 startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel<br>
//                 dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary<br>
//                 acousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;]<br>
//                 languageModelIsJSGF:FALSE]; // Start speech recognition.</p>
<p>                self.startupFailedDueToLackOfPermissions = FALSE;<br>
            }<br>
        }<br>
    }<br>
}</p>
<p>#pragma mark &#8211;<br>
#pragma mark UI</p>
<p>// This is not OpenEars-specific stuff, just some UI behavior</p>
<p>&#8211; (IBAction) suspendListeningButtonAction { // This is the action for the button which suspends listening without ending the recognition loop<br>
    [[OEPocketsphinxController sharedInstance] suspendRecognition];	</p>
<p>    self.startButton.hidden = TRUE;<br>
    self.stopButton.hidden = FALSE;<br>
    self.suspendListeningButton.hidden = TRUE;<br>
    self.resumeListeningButton.hidden = FALSE;<br>
}</p>
<p>&#8211; (IBAction) resumeListeningButtonAction { // This is the action for the button which resumes listening if it has been suspended<br>
    [[OEPocketsphinxController sharedInstance] resumeRecognition];</p>
<p>    self.startButton.hidden = TRUE;<br>
    self.stopButton.hidden = FALSE;<br>
    self.suspendListeningButton.hidden = FALSE;<br>
    self.resumeListeningButton.hidden = TRUE;<br>
}</p>
<p>&#8211; (IBAction) stopButtonAction { // This is the action for the button which shuts down the recognition loop.<br>
    NSError *error = nil;<br>
    if([OEPocketsphinxController sharedInstance].isListening) { // Stop if we are currently listening.<br>
        error = [[OEPocketsphinxController sharedInstance] stopListening];<br>
        if(error)NSLog(@&#8221;Error stopping listening in stopButtonAction: %@&#8221;, error);<br>
    }<br>
    self.startButton.hidden = FALSE;<br>
    self.stopButton.hidden = TRUE;<br>
    self.suspendListeningButton.hidden = TRUE;<br>
    self.resumeListeningButton.hidden = TRUE;<br>
}</p>
<p>&#8211; (IBAction) startButtonAction { // This is the action for the button which starts up the recognition loop again if it has been shut down.<br>
    if(![OEPocketsphinxController sharedInstance].isListening) {<br>
        [[OEPocketsphinxController sharedInstance] startListeningWithLanguageModelAtPath:self.pathToFirstDynamicallyGeneratedLanguageModel dictionaryAtPath:self.pathToFirstDynamicallyGeneratedDictionary acousticModelAtPath:[OEAcousticModel pathToModel:@&#8221;AcousticModelEnglish&#8221;] languageModelIsJSGF:FALSE]; // Start speech recognition if we aren&#8217;t already listening.<br>
    }<br>
    self.startButton.hidden = TRUE;<br>
    self.stopButton.hidden = FALSE;<br>
    self.suspendListeningButton.hidden = FALSE;<br>
    self.resumeListeningButton.hidden = TRUE;<br>
}</p>
<p>#pragma mark &#8211;<br>
#pragma mark Example for reading out Pocketsphinx and Flite audio levels without locking the UI by using an NSTimer</p>
<p>// What follows are not OpenEars methods, just an approach for level reading<br>
// that I&#8217;ve included with this sample app. My example implementation does make use of two OpenEars<br>
// methods:	the pocketsphinxInputLevel method of OEPocketsphinxController and the fliteOutputLevel<br>
// method of OEFliteController.<br>
//<br>
// The example is meant to show one way that you can read those levels continuously without locking the UI,<br>
// by using an NSTimer, but the OpenEars level-reading methods<br>
// themselves do not include multithreading code since I believe that you will want to design your own<br>
// code approaches for level display that are tightly-integrated with your interaction design and the<br>
// graphics API you choose.<br>
//<br>
// Please note that if you use my sample approach, you should pay attention to the way that the timer is always stopped in<br>
// dealloc. This should prevent you from having any difficulties with deallocating a class due to a running NSTimer process.</p>
<p>&#8211; (void) startDisplayingLevels { // Start displaying the levels using a timer<br>
    [self stopDisplayingLevels]; // We never want more than one timer valid so we&#8217;ll stop any running timers first.<br>
    self.uiUpdateTimer = [NSTimer scheduledTimerWithTimeInterval:1.0/kLevelUpdatesPerSecond target:self selector:@selector(updateLevelsUI) userInfo:nil repeats:YES];<br>
}</p>
<p>&#8211; (void) stopDisplayingLevels { // Stop displaying the levels by stopping the timer if it&#8217;s running.<br>
    if(self.uiUpdateTimer &amp;&amp; [self.uiUpdateTimer isValid]) { // If there is a running timer, we&#8217;ll stop it here.<br>
        [self.uiUpdateTimer invalidate];<br>
        self.uiUpdateTimer = nil;<br>
    }<br>
}</p>
<p>&#8211; (void) updateLevelsUI { // And here is how we obtain the levels.  This method includes the actual OpenEars methods and uses their results to update the UI of this view controller.</p>
<p>    self.pocketsphinxDbLabel.text = [NSString stringWithFormat:@&#8221;Pocketsphinx Input level:%f&#8221;,[[OEPocketsphinxController sharedInstance] pocketsphinxInputLevel]];  //pocketsphinxInputLevel is an OpenEars method of the class OEPocketsphinxController.</p>
<p>    if(self.fliteController.speechInProgress) {<br>
        self.fliteDbLabel.text = [NSString stringWithFormat:@&#8221;Flite Output level: %f&#8221;,[self.fliteController fliteOutputLevel]]; // fliteOutputLevel is an OpenEars method of the class OEFliteController.<br>
    }<br>
}</p>
<p>@end<br>
[/spoiler]</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1030139" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 21, 2016 at 1:11 pm</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1030139" class="bbp-reply-permalink">#1030139</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1030139 -->

<div class="loop-item-28 user-id-4 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-34 odd  post-1030139 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/halle-2/" title="View Halle Winkler&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Halle Winkler</span></a><div class="bbp-author-role">Politepix</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>OK, I&#8217;ll check it out and get back to you.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1030140" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 21, 2016 at 1:20 pm</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1030140" class="bbp-reply-permalink">#1030140</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1030140 -->

<div class="loop-item-29 user-id-4 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-30 even  post-1030140 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/halle-2/" title="View Halle Winkler&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Halle Winkler</span></a><div class="bbp-author-role">Politepix</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>It&#8217;s in there after the logging line &#8220;Attempting to start listening session from startRealtimeListeningWithLanguageModelAtPath&#8221;.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1030141" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 21, 2016 at 1:31 pm</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1030141" class="bbp-reply-permalink">#1030141</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1030141 -->

<div class="loop-item-30 user-id-2022 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-31 odd topic-author  post-1030141 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/laurent-oscadi/" title="View Laurent&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Laurent</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>I don&#8217;t have this line. I don&#8217;t have also the Creating shared instance of OEPocketsphinxController line in my logs</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1030142" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 21, 2016 at 1:38 pm</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1030142" class="bbp-reply-permalink">#1030142</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1030142 -->

<div class="loop-item-31 user-id-4 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-37 even  post-1030142 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/halle-2/" title="View Halle Winkler&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Halle Winkler</span></a><div class="bbp-author-role">Politepix</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>I downloaded a new copy of the Rejecto and RapidEars demos from the same link in your demo download email from this morning to make sure we were linking to the identical binaries, and the RapidEars demo definitely prints both of these lines when I copy and paste your code above into the sample app using the shipped version of OpenEars 2.051. Old versions of RapidEars don&#8217;t print either of those lines, so the issue is going to be related to that somehow.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1030143" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 21, 2016 at 1:39 pm</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1030143" class="bbp-reply-permalink">#1030143</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1030143 -->

<div class="loop-item-32 user-id-4 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-38 odd  post-1030143 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/halle-2/" title="View Halle Winkler&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Halle Winkler</span></a><div class="bbp-author-role">Politepix</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Double check that you are testing this with OELogging started, and started at the time that the view first loads (like in your code above).</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1030144" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 21, 2016 at 1:55 pm</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1030144" class="bbp-reply-permalink">#1030144</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1030144 -->

<div class="loop-item-33 user-id-4 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-39 even  post-1030144 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/halle-2/" title="View Halle Winkler&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Halle Winkler</span></a><div class="bbp-author-role">Politepix</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<blockquote><p>I don’t have also the Creating shared instance of OEPocketsphinxController line in my logs</p></blockquote>
<p>I&#8217;ve gone back and checked again, and the &#8220;Creating shared instance of OEPocketsphinxController&#8221; line is in the logging in the Instruments output that you sent me (and was in my run of your latest sample app code), it&#8217;s only the new RapidEars 2.5 logging which is not visible in your Instruments log. </p>
<p>I think it&#8217;s probably somewhere in your local logging as well if OELogging is turned on before doing anything else since that line has been in OpenEars for a couple of years, it&#8217;s probably just an oversight due to the large amount of logging output, or maybe a case-sensitive search or similar. I&#8217;m not aware of any conditions which suppress the standard OpenEars logging output.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1030145" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 21, 2016 at 1:58 pm</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1030145" class="bbp-reply-permalink">#1030145</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1030145 -->

<div class="loop-item-34 user-id-2022 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-35 odd topic-author  post-1030145 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/laurent-oscadi/" title="View Laurent&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Laurent</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>this is my logs for the code above:<br>
[spoiler]<br>
2016-04-21 15:54:12.434 OpenEarsSampleApp[4162:1540274] Starting OpenEars logging for OpenEars version 2.501 on 64-bit device (or build): iPad running iOS version: 9.300000<br>
2016-04-21 15:54:12.440 OpenEarsSampleApp[4162:1540274] Creating shared instance of OEPocketsphinxController<br>
2016-04-21 15:54:12.445 OpenEarsSampleApp[4162:1540274] Rejecto version 2.500000<br>
2016-04-21 15:54:12.479 OpenEarsSampleApp[4162:1540274] Since there is no cached version, loading the language model lookup list for the acoustic model called AcousticModelEnglish<br>
2016-04-21 15:54:12.485 OpenEarsSampleApp[4162:1540274] Returning a cached version of LanguageModelGeneratorLookupList.text<br>
2016-04-21 15:54:12.517 OpenEarsSampleApp[4162:1540274] I&#8217;m done running performDictionaryLookup and it took 0.031893 seconds<br>
2016-04-21 15:54:12.517 OpenEarsSampleApp[4162:1540274] I&#8217;m done running performDictionaryLookup and it took 0.033159 seconds<br>
2016-04-21 15:54:12.526 OpenEarsSampleApp[4162:1540274] Starting dynamic language model generation</p>
<p>INFO: ngram_model_arpa_legacy.c(504): ngrams 1=49, 2=94, 3=47<br>
INFO: ngram_model_arpa_legacy.c(136): Reading unigrams<br>
INFO: ngram_model_arpa_legacy.c(543):       49 = #unigrams created<br>
INFO: ngram_model_arpa_legacy.c(196): Reading bigrams<br>
INFO: ngram_model_arpa_legacy.c(561):       94 = #bigrams created<br>
INFO: ngram_model_arpa_legacy.c(562):        3 = #prob2 entries<br>
INFO: ngram_model_arpa_legacy.c(570):        3 = #bo_wt2 entries<br>
INFO: ngram_model_arpa_legacy.c(293): Reading trigrams<br>
INFO: ngram_model_arpa_legacy.c(583):       47 = #trigrams created<br>
INFO: ngram_model_arpa_legacy.c(584):        2 = #prob3 entries<br>
INFO: ngram_model_dmp_legacy.c(521): Building DMP model&#8230;<br>
INFO: ngram_model_dmp_legacy.c(551):       49 = #unigrams created<br>
INFO: ngram_model_dmp_legacy.c(652):       94 = #bigrams created<br>
INFO: ngram_model_dmp_legacy.c(653):        3 = #prob2 entries<br>
INFO: ngram_model_dmp_legacy.c(660):        3 = #bo_wt2 entries<br>
INFO: ngram_model_dmp_legacy.c(664):       47 = #trigrams created<br>
INFO: ngram_model_dmp_legacy.c(665):        2 = #prob3 entries<br>
2016-04-21 15:54:12.599 OpenEarsSampleApp[4162:1540274] Done creating language model with CMUCLMTK in 0.073387 seconds.<br>
INFO: ngram_model_arpa_legacy.c(504): ngrams 1=49, 2=94, 3=47<br>
INFO: ngram_model_arpa_legacy.c(136): Reading unigrams<br>
INFO: ngram_model_arpa_legacy.c(543):       49 = #unigrams created<br>
INFO: ngram_model_arpa_legacy.c(196): Reading bigrams<br>
INFO: ngram_model_arpa_legacy.c(561):       94 = #bigrams created<br>
INFO: ngram_model_arpa_legacy.c(562):        5 = #prob2 entries<br>
INFO: ngram_model_arpa_legacy.c(570):        3 = #bo_wt2 entries<br>
INFO: ngram_model_arpa_legacy.c(293): Reading trigrams<br>
INFO: ngram_model_arpa_legacy.c(583):       47 = #trigrams created<br>
INFO: ngram_model_arpa_legacy.c(584):        3 = #prob3 entries<br>
INFO: ngram_model_dmp_legacy.c(521): Building DMP model&#8230;<br>
INFO: ngram_model_dmp_legacy.c(551):       49 = #unigrams created<br>
INFO: ngram_model_dmp_legacy.c(652):       94 = #bigrams created<br>
INFO: ngram_model_dmp_legacy.c(653):        5 = #prob2 entries<br>
INFO: ngram_model_dmp_legacy.c(660):        3 = #bo_wt2 entries<br>
INFO: ngram_model_dmp_legacy.c(664):       47 = #trigrams created<br>
INFO: ngram_model_dmp_legacy.c(665):        3 = #prob3 entries<br>
2016-04-21 15:54:12.630 OpenEarsSampleApp[4162:1540274] I&#8217;m done running dynamic language model generation and it took 0.183863 seconds<br>
2016-04-21 15:54:12.639 OpenEarsSampleApp[4162:1540274] Returning a cached version of LanguageModelGeneratorLookupList.text<br>
2016-04-21 15:54:12.639 OpenEarsSampleApp[4162:1540274] Returning a cached version of LanguageModelGeneratorLookupList.text<br>
2016-04-21 15:54:12.672 OpenEarsSampleApp[4162:1540274] The word QUIDNUNC was not found in the dictionary of the acoustic model /var/containers/Bundle/Application/07CBAD80-E067-459E-9522-567D8DF68E72/OpenEarsSampleApp.app/AcousticModelEnglish.bundle. Now using the fallback method to look it up. If this is happening more frequently than you would expect, likely causes can be that you are entering words in another language from the one you are recognizing, or that there are symbols (including numbers) that need to be spelled out or cleaned up, or you are using your own acoustic model and there is an issue with either its phonetic dictionary or it lacks a g2p file. Please get in touch at the forums for assistance with the last two possible issues.<br>
2016-04-21 15:54:12.673 OpenEarsSampleApp[4162:1540274] Using convertGraphemes for the word or phrase quidnunc which doesn&#8217;t appear in the dictionary<br>
2016-04-21 15:54:12.685 OpenEarsSampleApp[4162:1540274] the graphemes &#8220;K W IH D N AH NG K&#8221; were created for the word QUIDNUNC using the fallback method.<br>
2016-04-21 15:54:12.698 OpenEarsSampleApp[4162:1540274] I&#8217;m done running performDictionaryLookup and it took 0.058571 seconds<br>
2016-04-21 15:54:12.698 OpenEarsSampleApp[4162:1540274] I&#8217;m done running performDictionaryLookup and it took 0.059407 seconds<br>
2016-04-21 15:54:12.709 OpenEarsSampleApp[4162:1540274] Starting dynamic language model generation</p>
<p>INFO: ngram_model_arpa_legacy.c(504): ngrams 1=51, 2=97, 3=49<br>
INFO: ngram_model_arpa_legacy.c(136): Reading unigrams<br>
INFO: ngram_model_arpa_legacy.c(543):       51 = #unigrams created<br>
INFO: ngram_model_arpa_legacy.c(196): Reading bigrams<br>
INFO: ngram_model_arpa_legacy.c(561):       97 = #bigrams created<br>
INFO: ngram_model_arpa_legacy.c(562):        3 = #prob2 entries<br>
INFO: ngram_model_arpa_legacy.c(570):        3 = #bo_wt2 entries<br>
INFO: ngram_model_arpa_legacy.c(293): Reading trigrams<br>
INFO: ngram_model_arpa_legacy.c(583):       49 = #trigrams created<br>
INFO: ngram_model_arpa_legacy.c(584):        2 = #prob3 entries<br>
INFO: ngram_model_dmp_legacy.c(521): Building DMP model&#8230;<br>
INFO: ngram_model_dmp_legacy.c(551):       51 = #unigrams created<br>
INFO: ngram_model_dmp_legacy.c(652):       97 = #bigrams created<br>
INFO: ngram_model_dmp_legacy.c(653):        3 = #prob2 entries<br>
INFO: ngram_model_dmp_legacy.c(660):        3 = #bo_wt2 entries<br>
INFO: ngram_model_dmp_legacy.c(664):       49 = #trigrams created<br>
INFO: ngram_model_dmp_legacy.c(665):        2 = #prob3 entries<br>
2016-04-21 15:54:12.784 OpenEarsSampleApp[4162:1540274] Done creating language model with CMUCLMTK in 0.074753 seconds.<br>
INFO: ngram_model_arpa_legacy.c(504): ngrams 1=51, 2=97, 3=49<br>
INFO: ngram_model_arpa_legacy.c(136): Reading unigrams<br>
INFO: ngram_model_arpa_legacy.c(543):       51 = #unigrams created<br>
INFO: ngram_model_arpa_legacy.c(196): Reading bigrams<br>
INFO: ngram_model_arpa_legacy.c(561):       97 = #bigrams created<br>
INFO: ngram_model_arpa_legacy.c(562):        5 = #prob2 entries<br>
INFO: ngram_model_arpa_legacy.c(570):        3 = #bo_wt2 entries<br>
INFO: ngram_model_arpa_legacy.c(293): Reading trigrams<br>
INFO: ngram_model_arpa_legacy.c(583):       49 = #trigrams created<br>
INFO: ngram_model_arpa_legacy.c(584):        3 = #prob3 entries<br>
INFO: ngram_model_dmp_legacy.c(521): Building DMP model&#8230;<br>
INFO: ngram_model_dmp_legacy.c(551):       51 = #unigrams created<br>
INFO: ngram_model_dmp_legacy.c(652):       97 = #bigrams created<br>
INFO: ngram_model_dmp_legacy.c(653):        5 = #prob2 entries<br>
INFO: ngram_model_dmp_legacy.c(660):        3 = #bo_wt2 entries<br>
INFO: ngram_model_dmp_legacy.c(664):       49 = #trigrams created<br>
INFO: ngram_model_dmp_legacy.c(665):        3 = #prob3 entries<br>
2016-04-21 15:54:12.822 OpenEarsSampleApp[4162:1540274] I&#8217;m done running dynamic language model generation and it took 0.191083 seconds<br>
2016-04-21 15:54:12.822 OpenEarsSampleApp[4162:1540274] </p>
<p>Welcome to the OpenEars sample project. This project understands the words:<br>
BACKWARD,<br>
CHANGE,<br>
FORWARD,<br>
GO,<br>
LEFT,<br>
MODEL,<br>
RIGHT,<br>
TURN,<br>
and if you say &#8220;CHANGE MODEL&#8221; it will switch to its dynamically-generated model which understands the words:<br>
CHANGE,<br>
MODEL,<br>
MONDAY,<br>
TUESDAY,<br>
WEDNESDAY,<br>
THURSDAY,<br>
FRIDAY,<br>
SATURDAY,<br>
SUNDAY,<br>
QUIDNUNC<br>
2016-04-21 15:54:12.827 OpenEarsSampleApp[4162:1540274] User gave mic permission for this app.<br>
2016-04-21 15:54:12.828 OpenEarsSampleApp[4162:1540274] setSecondsOfSilence wasn&#8217;t set, using default of 0.700000.<br>
2016-04-21 15:54:12.829 OpenEarsSampleApp[4162:1540292] Starting listening.<br>
2016-04-21 15:54:12.829 OpenEarsSampleApp[4162:1540292] about to set up audio session<br>
2016-04-21 15:54:12.830 OpenEarsSampleApp[4162:1540292] Creating audio session with default settings.<br>
2016-04-21 15:54:12.923 OpenEarsSampleApp[4162:1540302] Audio route has changed for the following reason:<br>
2016-04-21 15:54:12.927 OpenEarsSampleApp[4162:1540302] There was a category change. The new category is AVAudioSessionCategoryPlayAndRecord<br>
2016-04-21 15:54:12.930 OpenEarsSampleApp[4162:1540302] This is not a case in which OpenEars notifies of a route change. At the close of this function, the new audio route is &#8212;SpeakerMicrophoneBuiltIn&#8212;. The previous route before changing to this route was &lt;AVAudioSessionRouteDescription: 0x156d72d30,<br>
inputs = (<br>
    &#8220;&lt;AVAudioSessionPortDescription: 0x156eb8f00, type = MicrophoneBuiltIn; name = iPad Micro; UID = Built-In Microphone; selectedDataSource = Avant&gt;&#8221;<br>
);<br>
outputs = (<br>
    &#8220;&lt;AVAudioSessionPortDescription: 0x156e7a060, type = Speaker; name = Haut-parleur; UID = Built-In Speaker; selectedDataSource = (null)&gt;&#8221;<br>
)&gt;.<br>
2016-04-21 15:54:13.067 OpenEarsSampleApp[4162:1540292] done starting audio unit<br>
INFO: pocketsphinx.c(145): Parsed model-specific feature parameters from /var/containers/Bundle/Application/07CBAD80-E067-459E-9522-567D8DF68E72/OpenEarsSampleApp.app/AcousticModelEnglish.bundle/feat.params<br>
Current configuration:<br>
[NAME]			[DEFLT]		[VALUE]<br>
-agc			none		none<br>
-agcthresh		2.0		2.000000e+00<br>
-allphone<br>
-allphone_ci		no		no<br>
-alpha			0.97		9.700000e-01<br>
-ascale			20.0		2.000000e+01<br>
-aw			1		1<br>
-backtrace		no		no<br>
-beam			1e-48		1.000000e-48<br>
-bestpath		yes		yes<br>
-bestpathlw		9.5		9.500000e+00<br>
-ceplen			13		13<br>
-cmn			current		current<br>
-cmninit		8.0		40<br>
-compallsen		no		no<br>
-debug					0<br>
-dict					/var/mobile/Containers/Data/Application/CEFFF7C3-6DBE-4C65-83EF-A49F8024AC52/Library/Caches/FirstOpenEarsDynamicLanguageModel.dic<br>
-dictcase		no		no<br>
-dither			no		no<br>
-doublebw		no		no<br>
-ds			1		1<br>
-fdict					/var/containers/Bundle/Application/07CBAD80-E067-459E-9522-567D8DF68E72/OpenEarsSampleApp.app/AcousticModelEnglish.bundle/noisedict<br>
-feat			1s_c_d_dd	1s_c_d_dd<br>
-featparams				/var/containers/Bundle/Application/07CBAD80-E067-459E-9522-567D8DF68E72/OpenEarsSampleApp.app/AcousticModelEnglish.bundle/feat.params<br>
-fillprob		1e-8		1.000000e-08<br>
-frate			100		100<br>
-fsg<br>
-fsgusealtpron		yes		yes<br>
-fsgusefiller		yes		yes<br>
-fwdflat		yes		yes<br>
-fwdflatbeam		1e-64		1.000000e-64<br>
-fwdflatefwid		4		4<br>
-fwdflatlw		8.5		8.500000e+00<br>
-fwdflatsfwin		25		25<br>
-fwdflatwbeam		7e-29		7.000000e-29<br>
-fwdtree		yes		yes<br>
-hmm					/var/containers/Bundle/Application/07CBAD80-E067-459E-9522-567D8DF68E72/OpenEarsSampleApp.app/AcousticModelEnglish.bundle<br>
-input_endian		little		little<br>
-jsgf<br>
-keyphrase<br>
-kws<br>
-kws_delay		10		10<br>
-kws_plp		1e-1		1.000000e-01<br>
-kws_threshold		1		1.000000e+00<br>
-latsize		5000		5000<br>
-lda<br>
-ldadim			0		0<br>
-lifter			0		22<br>
-lm					/var/mobile/Containers/Data/Application/CEFFF7C3-6DBE-4C65-83EF-A49F8024AC52/Library/Caches/FirstOpenEarsDynamicLanguageModel.DMP<br>
-lmctl<br>
-lmname<br>
-logbase		1.0001		1.000100e+00<br>
-logfn<br>
-logspec		no		no<br>
-lowerf			133.33334	1.300000e+02<br>
-lpbeam			1e-40		1.000000e-40<br>
-lponlybeam		7e-29		7.000000e-29<br>
-lw			6.5		6.500000e+00<br>
-maxhmmpf		30000		30000<br>
-maxwpf			-1		-1<br>
-mdef					/var/containers/Bundle/Application/07CBAD80-E067-459E-9522-567D8DF68E72/OpenEarsSampleApp.app/AcousticModelEnglish.bundle/mdef<br>
-mean					/var/containers/Bundle/Application/07CBAD80-E067-459E-9522-567D8DF68E72/OpenEarsSampleApp.app/AcousticModelEnglish.bundle/means<br>
-mfclogdir<br>
-min_endfr		0		0<br>
-mixw<br>
-mixwfloor		0.0000001	1.000000e-07<br>
-mllr<br>
-mmap			yes		yes<br>
-ncep			13		13<br>
-nfft			512		512<br>
-nfilt			40		25<br>
-nwpen			1.0		1.000000e+00<br>
-pbeam			1e-48		1.000000e-48<br>
-pip			1.0		1.000000e+00<br>
-pl_beam		1e-10		1.000000e-10<br>
-pl_pbeam		1e-10		1.000000e-10<br>
-pl_pip			1.0		1.000000e+00<br>
-pl_weight		3.0		3.000000e+00<br>
-pl_window		5		5<br>
-rawlogdir<br>
-remove_dc		no		no<br>
-remove_noise		yes		yes<br>
-remove_silence		yes		yes<br>
-round_filters		yes		yes<br>
-samprate		16000		1.600000e+04<br>
-seed			-1		-1<br>
-sendump				/var/containers/Bundle/Application/07CBAD80-E067-459E-9522-567D8DF68E72/OpenEarsSampleApp.app/AcousticModelEnglish.bundle/sendump<br>
-senlogdir<br>
-senmgau<br>
-silprob		0.005		5.000000e-03<br>
-smoothspec		no		no<br>
-svspec					0-12/13-25/26-38<br>
-tmat					/var/containers/Bundle/Application/07CBAD80-E067-459E-9522-567D8DF68E72/OpenEarsSampleApp.app/AcousticModelEnglish.bundle/transition_matrices<br>
-tmatfloor		0.0001		1.000000e-04<br>
-topn			4		4<br>
-topn_beam		0		0<br>
-toprule<br>
-transform		legacy		dct<br>
-unit_area		yes		yes<br>
-upperf			6855.4976	6.800000e+03<br>
-uw			1.0		1.000000e+00<br>
-vad_postspeech		50		69<br>
-vad_prespeech		20		10<br>
-vad_startspeech	10		10<br>
-vad_threshold		2.0		2.000000e+00<br>
-var					/var/containers/Bundle/Application/07CBAD80-E067-459E-9522-567D8DF68E72/OpenEarsSampleApp.app/AcousticModelEnglish.bundle/variances<br>
-varfloor		0.0001		1.000000e-04<br>
-varnorm		no		no<br>
-verbose		no		no<br>
-warp_params<br>
-warp_type		inverse_linear	inverse_linear<br>
-wbeam			7e-29		7.000000e-29<br>
-wip			0.65		6.500000e-01<br>
-wlen			0.025625	2.562500e-02</p>
<p>INFO: feat.c(715): Initializing feature stream to type: &#8216;1s_c_d_dd&#8217;, ceplen=13, CMN=&#8217;current&#8217;, VARNORM=&#8217;no&#8217;, AGC=&#8217;none&#8217;<br>
INFO: cmn.c(143): mean[0]= 12.00, mean[1..12]= 0.0<br>
INFO: acmod.c(164): Using subvector specification 0-12/13-25/26-38<br>
INFO: mdef.c(518): Reading model definition: /var/containers/Bundle/Application/07CBAD80-E067-459E-9522-567D8DF68E72/OpenEarsSampleApp.app/AcousticModelEnglish.bundle/mdef<br>
INFO: mdef.c(531): Found byte-order mark BMDF, assuming this is a binary mdef file<br>
INFO: bin_mdef.c(336): Reading binary model definition: /var/containers/Bundle/Application/07CBAD80-E067-459E-9522-567D8DF68E72/OpenEarsSampleApp.app/AcousticModelEnglish.bundle/mdef<br>
INFO: bin_mdef.c(516): 46 CI-phone, 168344 CD-phone, 3 emitstate/phone, 138 CI-sen, 6138 Sen, 32881 Sen-Seq<br>
INFO: tmat.c(206): Reading HMM transition probability matrices: /var/containers/Bundle/Application/07CBAD80-E067-459E-9522-567D8DF68E72/OpenEarsSampleApp.app/AcousticModelEnglish.bundle/transition_matrices<br>
INFO: acmod.c(117): Attempting to use PTM computation module<br>
INFO: ms_gauden.c(198): Reading mixture gaussian parameter: /var/containers/Bundle/Application/07CBAD80-E067-459E-9522-567D8DF68E72/OpenEarsSampleApp.app/AcousticModelEnglish.bundle/means<br>
INFO: ms_gauden.c(292): 1 codebook, 3 feature, size:<br>
INFO: ms_gauden.c(294):  512&#215;13<br>
INFO: ms_gauden.c(294):  512&#215;13<br>
INFO: ms_gauden.c(294):  512&#215;13<br>
INFO: ms_gauden.c(198): Reading mixture gaussian parameter: /var/containers/Bundle/Application/07CBAD80-E067-459E-9522-567D8DF68E72/OpenEarsSampleApp.app/AcousticModelEnglish.bundle/variances<br>
INFO: ms_gauden.c(292): 1 codebook, 3 feature, size:<br>
INFO: ms_gauden.c(294):  512&#215;13<br>
INFO: ms_gauden.c(294):  512&#215;13<br>
INFO: ms_gauden.c(294):  512&#215;13<br>
INFO: ms_gauden.c(354): 0 variance values floored<br>
INFO: ptm_mgau.c(805): Number of codebooks doesn&#8217;t match number of ciphones, doesn&#8217;t look like PTM: 1 != 46<br>
INFO: acmod.c(119): Attempting to use semi-continuous computation module<br>
INFO: ms_gauden.c(198): Reading mixture gaussian parameter: /var/containers/Bundle/Application/07CBAD80-E067-459E-9522-567D8DF68E72/OpenEarsSampleApp.app/AcousticModelEnglish.bundle/means<br>
INFO: ms_gauden.c(292): 1 codebook, 3 feature, size:<br>
INFO: ms_gauden.c(294):  512&#215;13<br>
INFO: ms_gauden.c(294):  512&#215;13<br>
INFO: ms_gauden.c(294):  512&#215;13<br>
INFO: ms_gauden.c(198): Reading mixture gaussian parameter: /var/containers/Bundle/Application/07CBAD80-E067-459E-9522-567D8DF68E72/OpenEarsSampleApp.app/AcousticModelEnglish.bundle/variances<br>
INFO: ms_gauden.c(292): 1 codebook, 3 feature, size:<br>
INFO: ms_gauden.c(294):  512&#215;13<br>
INFO: ms_gauden.c(294):  512&#215;13<br>
INFO: ms_gauden.c(294):  512&#215;13<br>
INFO: ms_gauden.c(354): 0 variance values floored<br>
INFO: s2_semi_mgau.c(904): Loading senones from dump file /var/containers/Bundle/Application/07CBAD80-E067-459E-9522-567D8DF68E72/OpenEarsSampleApp.app/AcousticModelEnglish.bundle/sendump<br>
INFO: s2_semi_mgau.c(928): BEGIN FILE FORMAT DESCRIPTION<br>
INFO: s2_semi_mgau.c(991): Rows: 512, Columns: 6138<br>
INFO: s2_semi_mgau.c(1023): Using memory-mapped I/O for senones<br>
INFO: s2_semi_mgau.c(1294): Maximum top-N: 4 Top-N beams: 0 0 0<br>
INFO: phone_loop_search.c(114): State beam -225 Phone exit beam -225 Insertion penalty 0<br>
INFO: dict.c(320): Allocating 4152 * 32 bytes (129 KiB) for word entries<br>
INFO: dict.c(333): Reading main dictionary: /var/mobile/Containers/Data/Application/CEFFF7C3-6DBE-4C65-83EF-A49F8024AC52/Library/Caches/FirstOpenEarsDynamicLanguageModel.dic<br>
INFO: dict.c(213): Allocated 0 KiB for strings, 0 KiB for phones<br>
INFO: dict.c(336): 47 words read<br>
INFO: dict.c(358): Reading filler dictionary: /var/containers/Bundle/Application/07CBAD80-E067-459E-9522-567D8DF68E72/OpenEarsSampleApp.app/AcousticModelEnglish.bundle/noisedict<br>
INFO: dict.c(213): Allocated 0 KiB for strings, 0 KiB for phones<br>
INFO: dict.c(361): 9 words read<br>
INFO: dict2pid.c(396): Building PID tables for dictionary<br>
INFO: dict2pid.c(406): Allocating 46^3 * 2 bytes (190 KiB) for word-initial triphones<br>
INFO: dict2pid.c(132): Allocated 51152 bytes (49 KiB) for word-final triphones<br>
INFO: dict2pid.c(196): Allocated 51152 bytes (49 KiB) for single-phone word triphones<br>
INFO: ngram_model_trie.c(424): Trying to read LM in bin format<br>
INFO: ngram_model_trie.c(457): Header doesn&#8217;t match<br>
INFO: ngram_model_trie.c(180): Trying to read LM in arpa format<br>
INFO: ngram_model_trie.c(71): No \data\ mark in LM file<br>
INFO: ngram_model_trie.c(537): Trying to read LM in DMP format<br>
INFO: ngram_model_trie.c(632): ngrams 1=49, 2=94, 3=47<br>
INFO: lm_trie.c(317): Training quantizer<br>
INFO: lm_trie.c(323): Building LM trie<br>
INFO: ngram_search_fwdtree.c(99): 8 unique initial diphones<br>
INFO: ngram_search_fwdtree.c(148): 0 root, 0 non-root channels, 49 single-phone words<br>
INFO: ngram_search_fwdtree.c(186): Creating search tree<br>
INFO: ngram_search_fwdtree.c(192): before: 0 root, 0 non-root channels, 49 single-phone words<br>
INFO: ngram_search_fwdtree.c(326): after: max nonroot chan increased to 145<br>
INFO: ngram_search_fwdtree.c(339): after: 8 root, 17 non-root channels, 48 single-phone words<br>
INFO: ngram_search_fwdflat.c(157): fwdflat: min_ef_width = 4, max_sf_win = 25<br>
2016-04-21 15:54:13.162 OpenEarsSampleApp[4162:1540292] There is no CMN plist so we are using the fresh CMN value 40.000000.<br>
2016-04-21 15:54:13.163 OpenEarsSampleApp[4162:1540292] Listening.<br>
2016-04-21 15:54:13.164 OpenEarsSampleApp[4162:1540292] Project has these words or phrases in its dictionary:<br>
___REJ_ZH<br>
___REJ_Z<br>
___REJ_Y<br>
___REJ_W<br>
___REJ_V<br>
___REJ_UW<br>
___REJ_UH<br>
___REJ_TH<br>
___REJ_T<br>
___REJ_SH<br>
___REJ_S<br>
___REJ_R<br>
___REJ_P<br>
___REJ_OY<br>
___REJ_OW<br>
___REJ_NG<br>
___REJ_N<br>
___REJ_M<br>
___REJ_L<br>
___REJ_K<br>
___REJ_JH<br>
___REJ_IY<br>
___REJ_IH<br>
___REJ_HH<br>
___REJ_G<br>
___REJ_F<br>
___REJ_EY<br>
___REJ_ER<br>
___REJ_EH<br>
___REJ_DH<br>
___REJ_D<br>
&#8230;and 17 more.<br>
2016-04-21 15:54:13.164 OpenEarsSampleApp[4162:1540292] Recognition loop has started<br>
2016-04-21 15:54:13.222 OpenEarsSampleApp[4162:1540274] Local callback: Pocketsphinx is now listening.<br>
2016-04-21 15:54:13.226 OpenEarsSampleApp[4162:1540274] Local callback: Pocketsphinx started.<br>
2016-04-21 15:54:13.506 OpenEarsSampleApp[4162:1540293] Speech detected&#8230;<br>
2016-04-21 15:54:13.507 OpenEarsSampleApp[4162:1540274] Local callback: Pocketsphinx has detected speech.<br>
2016-04-21 15:54:13.507 OpenEarsSampleApp[4162:1540293] Pocketsphinx heard &#8221; &#8221; with a score of (-1286) and an utterance ID of 0.<br>
2016-04-21 15:54:13.507 OpenEarsSampleApp[4162:1540293] Hypothesis was null so we aren&#8217;t returning it. If you want null hypotheses to also be returned, set OEPocketsphinxController&#8217;s property returnNullHypotheses to TRUE before starting OEPocketsphinxController.<br>
2016-04-21 15:54:13.646 OpenEarsSampleApp[4162:1540292] Pocketsphinx heard &#8221; &#8221; with a score of (-3591) and an utterance ID of 1.<br>
2016-04-21 15:54:13.791 OpenEarsSampleApp[4162:1540292] Pocketsphinx heard &#8221; &#8221; with a score of (-4690) and an utterance ID of 2.<br>
2016-04-21 15:54:13.914 OpenEarsSampleApp[4162:1540293] Pocketsphinx heard &#8221; &#8221; with a score of (-6414) and an utterance ID of 3.<br>
2016-04-21 15:54:14.028 OpenEarsSampleApp[4162:1540292] Pocketsphinx heard &#8221; &#8221; with a score of (-7325) and an utterance ID of 4.<br>
2016-04-21 15:54:14.179 OpenEarsSampleApp[4162:1540292] Pocketsphinx heard &#8221; &#8221; with a score of (-8217) and an utterance ID of 5.<br>
2016-04-21 15:54:14.303 OpenEarsSampleApp[4162:1540295] Pocketsphinx heard &#8221; &#8221; with a score of (-10011) and an utterance ID of 6.<br>
2016-04-21 15:54:14.411 OpenEarsSampleApp[4162:1540295] Pocketsphinx heard &#8221; &#8221; with a score of (-10959) and an utterance ID of 7.<br>
2016-04-21 15:54:14.580 OpenEarsSampleApp[4162:1540293] Pocketsphinx heard &#8221; &#8221; with a score of (-13033) and an utterance ID of 8.<br>
INFO: ngram_search.c(463): Resized backpointer table to 10000 entries<br>
2016-04-21 15:54:14.686 OpenEarsSampleApp[4162:1540292] Pocketsphinx heard &#8221; &#8221; with a score of (-14072) and an utterance ID of 9.<br>
2016-04-21 15:54:14.813 OpenEarsSampleApp[4162:1540295] Pocketsphinx heard &#8221; &#8221; with a score of (-15302) and an utterance ID of 10.<br>
2016-04-21 15:54:14.952 OpenEarsSampleApp[4162:1540293] Pocketsphinx heard &#8221; &#8221; with a score of (-17031) and an utterance ID of 11.<br>
2016-04-21 15:54:15.078 OpenEarsSampleApp[4162:1540293] Pocketsphinx heard &#8221; &#8221; with a score of (-17955) and an utterance ID of 12.<br>
2016-04-21 15:54:15.180 OpenEarsSampleApp[4162:1540293] Pocketsphinx heard &#8221; &#8221; with a score of (-19053) and an utterance ID of 13.<br>
2016-04-21 15:54:15.348 OpenEarsSampleApp[4162:1540295] Pocketsphinx heard &#8221; &#8221; with a score of (-21171) and an utterance ID of 14.<br>
2016-04-21 15:54:15.472 OpenEarsSampleApp[4162:1540293] Pocketsphinx heard &#8221; &#8221; with a score of (-22181) and an utterance ID of 15.<br>
2016-04-21 15:54:15.563 OpenEarsSampleApp[4162:1540293] Pocketsphinx heard &#8221; &#8221; with a score of (-23068) and an utterance ID of 16.<br>
2016-04-21 15:54:15.719 OpenEarsSampleApp[4162:1540293] Pocketsphinx heard &#8221; &#8221; with a score of (-25022) and an utterance ID of 17.<br>
2016-04-21 15:54:15.840 OpenEarsSampleApp[4162:1540293] Pocketsphinx heard &#8221; &#8221; with a score of (-25955) and an utterance ID of 18.<br>
INFO: ngram_search.c(463): Resized backpointer table to 20000 entries<br>
2016-04-21 15:54:15.961 OpenEarsSampleApp[4162:1540293] Pocketsphinx heard &#8221; &#8221; with a score of (-27724) and an utterance ID of 19.<br>
2016-04-21 15:54:16.067 OpenEarsSampleApp[4162:1540293] Pocketsphinx heard &#8221; &#8221; with a score of (-28966) and an utterance ID of 20.<br>
2016-04-21 15:54:16.219 OpenEarsSampleApp[4162:1540292] Pocketsphinx heard &#8221; &#8221; with a score of (-29769) and an utterance ID of 21.<br>
2016-04-21 15:54:16.355 OpenEarsSampleApp[4162:1540292] Pocketsphinx heard &#8221; &#8221; with a score of (-31651) and an utterance ID of 22.<br>
2016-04-21 15:54:16.454 OpenEarsSampleApp[4162:1540295] Pocketsphinx heard &#8221; &#8221; with a score of (-32748) and an utterance ID of 23.<br>
2016-04-21 15:54:16.621 OpenEarsSampleApp[4162:1540295] Pocketsphinx heard &#8221; &#8221; with a score of (-35159) and an utterance ID of 24.<br>
2016-04-21 15:54:16.731 OpenEarsSampleApp[4162:1540292] Pocketsphinx heard &#8221; &#8221; with a score of (-36272) and an utterance ID of 25.<br>
2016-04-21 15:54:16.870 OpenEarsSampleApp[4162:1540292] Pocketsphinx heard &#8221; &#8221; with a score of (-37163) and an utterance ID of 26.<br>
2016-04-21 15:54:17.003 OpenEarsSampleApp[4162:1540292] Pocketsphinx heard &#8221; &#8221; with a score of (-39099) and an utterance ID of 27.<br>
2016-04-21 15:54:17.120 OpenEarsSampleApp[4162:1540295] Pocketsphinx heard &#8221; &#8221; with a score of (-40206) and an utterance ID of 28.<br>
2016-04-21 15:54:17.244 OpenEarsSampleApp[4162:1540295] End of speech detected&#8230;<br>
2016-04-21 15:54:17.247 OpenEarsSampleApp[4162:1540274] Local callback: Pocketsphinx has detected a second of silence, concluding an utterance.<br>
INFO: cmn_prior.c(131): cmn_prior_update: from &lt; 40.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 &gt;<br>
INFO: cmn_prior.c(149): cmn_prior_update: to   &lt; 40.02 -0.74 -13.25  0.38 -5.00  5.24 -2.31 -1.98 -1.52  1.41 -7.07 -1.30 -3.77 &gt;<br>
INFO: ngram_search_fwdtree.c(1553):    16041 words recognized (42/fr)<br>
INFO: ngram_search_fwdtree.c(1555):    70579 senones evaluated (183/fr)<br>
INFO: ngram_search_fwdtree.c(1559):    24594 channels searched (63/fr), 3048 1st, 18760 last<br>
INFO: ngram_search_fwdtree.c(1562):    17980 words for which last channels evaluated (46/fr)<br>
INFO: ngram_search_fwdtree.c(1564):      241 candidate words for entering last phone (0/fr)<br>
INFO: ngram_search_fwdtree.c(1567): fwdtree 0.52 CPU 0.134 xRT<br>
INFO: ngram_search_fwdtree.c(1570): fwdtree 3.86 wall 1.002 xRT<br>
INFO: ngram_search_fwdflat.c(302): Utterance vocabulary contains 23 words<br>
INFO: ngram_search_fwdflat.c(948):     6335 words recognized (16/fr)<br>
INFO: ngram_search_fwdflat.c(950):    24855 senones evaluated (65/fr)<br>
INFO: ngram_search_fwdflat.c(952):     7343 channels searched (19/fr)<br>
INFO: ngram_search_fwdflat.c(954):     7343 words searched (19/fr)<br>
INFO: ngram_search_fwdflat.c(957):     3010 word transitions (7/fr)<br>
INFO: ngram_search_fwdflat.c(960): fwdflat 0.06 CPU 0.015 xRT<br>
INFO: ngram_search_fwdflat.c(963): fwdflat 0.06 wall 0.016 xRT<br>
INFO: ngram_search.c(1280): lattice start node &lt;s&gt;.0 end node &lt;/s&gt;.381<br>
INFO: ngram_search.c(1306): Eliminated 5 nodes before end node<br>
INFO: ngram_search.c(1411): Lattice has 4227 nodes, 73183 links<br>
INFO: ps_lattice.c(1380): Bestpath score: -41400<br>
INFO: ps_lattice.c(1384): Normalizer P(O) = alpha(&lt;/s&gt;:381:383) = -41884<br>
INFO: ps_lattice.c(1441): Joint P(O,S) = -220787 P(S|O) = -178903<br>
INFO: ngram_search.c(899): bestpath 0.67 CPU 0.176 xRT<br>
INFO: ngram_search.c(902): bestpath 0.67 wall 0.175 xRT<br>
2016-04-21 15:54:17.996 OpenEarsSampleApp[4162:1540295] Pocketsphinx heard &#8221; &#8221; with a score of (-4057) and an utterance ID of 29.<br>
[/spoiler]</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1030147" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 21, 2016 at 2:27 pm</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1030147" class="bbp-reply-permalink">#1030147</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1030147 -->

<div class="loop-item-35 user-id-4 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-36 even  post-1030147 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/halle-2/" title="View Halle Winkler&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Halle Winkler</span></a><div class="bbp-author-role">Politepix</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>&#8220;Creating shared instance of OEPocketsphinxController&#8221; is the second line. But I really don&#8217;t think those logs are from a project successfully linked with a current version of RapidEarsDemo.framework. If the Rejecto 2.5 and OpenEars 2.5 logging work as expected, and all three of them work as expected when I run it locally, and the local behavior when stopping listening is different on your setup, I think the logging problem is most likely somehow connected with linking to an old copy of RapidEars locally. </p>
<p>I believe that it is possible that there is an extraordinary bug of some kind which causes the symptoms of just the RapidEars framework not logging just some of its logging output, so I am not ruling that out, but it seems like a lower probability than linking to an old framework right now. There is some RapidEars logging appearing (&#8220;Starting listening&#8221;), so the issue isn&#8217;t that there is no logging output from RapidEars at all, but that the new logging output that was added to RapidEars 2.5 is not apparent.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1030149" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 21, 2016 at 2:55 pm</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1030149" class="bbp-reply-permalink">#1030149</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1030149 -->

<div class="loop-item-36 user-id-2022 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-43 odd topic-author  post-1030149 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/laurent-oscadi/" title="View Laurent&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Laurent</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>I am trying to get the rapidEars 2.5 logging when I will get it I will tell you!<br>
And then I will be able to do the replication with the songs and share it with you.</p>
<p>Regards,<br>
Laurent</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1030150" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 21, 2016 at 2:59 pm</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1030150" class="bbp-reply-permalink">#1030150</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1030150 -->

<div class="loop-item-37 user-id-4 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-44 even  post-1030150 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/halle-2/" title="View Halle Winkler&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Halle Winkler</span></a><div class="bbp-author-role">Politepix</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Thank you, I look forward to checking it out.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1030164" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 22, 2016 at 10:32 am</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1030164" class="bbp-reply-permalink">#1030164</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1030164 -->

<div class="loop-item-38 user-id-2022 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-49 odd topic-author  post-1030164 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/laurent-oscadi/" title="View Laurent&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Laurent</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>I can&#8217;t replicate the same behavior. But by trying to do it . I can tell you that</p>
<p>when I am profiling with instruments leaks. I don&#8217;t speak at all and when there is some noise in my office Memory size is growing up and when it is rising upper than 60MB if you click on stop listening it doesn&#8217;t fall down to 13MB.<br>
And also I noticed that when it falls down when the memory goes up to 60MB it is not falling down at the same memory size as before the rising.<br>
Example : before it is 13MB there is an increase up to 70MB and falling down at 19Mb&#8230;</p>
<p>Could you please test on your side and check if it is the same behaviour.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1030167" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 22, 2016 at 11:23 am</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1030167" class="bbp-reply-permalink">#1030167</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1030167 -->

<div class="loop-item-39 user-id-4 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-40 even  post-1030167 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/halle-2/" title="View Halle Winkler&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Halle Winkler</span></a><div class="bbp-author-role">Politepix</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Hi,</p>
<p>No, sorry, I don&#8217;t see that behavior when stopping, that is why I have asked for a replication case. I see the behavior I described at the beginning of the discussion, that both the buffer and the hypothesis search can grow to the size needed (which can get pretty large when there is a long utterance that has noise and the presence of words is unclear) and then is eventually released (the hypothesis search memory releases sometime after the search finishes, and the buffer memory releases sometime after stopping listening). It can take a fair amount of time before Instruments shows it as being released, and of course there are also other things happening in the sample app that can make their own use of memory (such as TTS and language model generation file caching).</p>
<p>I also don&#8217;t see leaks in Instruments (other than the very tiny leaks we discussed at the beginning of the discussion, adding up to less than 2k).</p>
<p>When I look at your Instruments example you sent, it doesn&#8217;t have leaks other than the tiny &lt;2k leaks (leaks are orphaned memory). But it has live memory that continues to be live, at a time in which it can be expected to be possible to release, and it is a lot of memory. The memory usage is from a 3rd-pass search that doesn't complete successfully when you stop listening (this is shown in the log when you look at the log timing and compare it to the memory usage).</p>
<p>What is happening in that session is that the 3rd-pass search gets very large (probably due to intermittent background noise that builds up in a long utterance without any easily-found words) and it keeps searching even while the stopListening message is in progress and trying to shut down, and after too much time passes, the stopListening method has to stop attempting to release the search because it will lead to an exception. The reason that I've wanted to debug your RapidEars version install is that this behavior used to be possible with RapidEars during a big unclear 3rd-pass search, but this issue was fixed, so it is unexpected that it is happening in your install, and it is also not possible for me to replicate with the current versions of the frameworks using my own audio files or audio input.</p>
<p>It also seems technically impossible that this happens with 3rd-pass searches turned off, so it's a confusing issue. So these are the reasons I'm now hoping for a replication case from you where it replicates using an audio file from your environment. I would _really_ like to fix it if it is a current problem since I have already worked on this and thought it was fixed, but I can't cause it to happen in my own system. My local install demonstrates the fixes that were made, and also doesn't run 3rd-pass searches like the one shown in the Instruments file when I turn 3rd-pass searches off.</p>
<p>If this is an issue:</p>
<blockquote><p>before it is 13MB there is an increase up to 70MB and falling down at 19Mb</p></blockquote>
<p>It is a different issue – the one I&#8217;ve been trying to replicate is the one that was shown in your Instruments file you sent, where there is a far larger allocation with none of it ever being released when stopping listening fails (this is the old issue that I expected to be fixed). An intermittent usage of a large amount of memory is something that can happen briefly in many kinds of apps that have a temporary need for it without it being a problem, and the search data structures can temporarily get pretty big on a 64-bit platform, although this is something that occurs for a matter of seconds.</p>
<p>I&#8217;m happy to investigate a new issue like &#8220;after a large search is successfully released, there is still 6MB more memory used than expected at the time of restarting listening and it isn&#8217;t clear whether it is due to a new memory need or a bug&#8221;, but first we need to wrap up the reported issue in the file you sent, which is a huge allocation where none of it is released because stopListening is not successful, which is a different scale of problem.</p>
<p>Is it possible that the reported issue doesn&#8217;t replicate for you now because you have done some troubleshooting on your RapidEars install version and you are now linking to RapidEars 2.5, or do you still not see RapidEars 2.5 logging when you try to set up your replication case?</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1030169" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 22, 2016 at 12:43 pm</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1030169" class="bbp-reply-permalink">#1030169</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1030169 -->

<div class="loop-item-40 user-id-2022 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-41 odd topic-author  post-1030169 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/laurent-oscadi/" title="View Laurent&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Laurent</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>I still do not see the rapidEars 2.5 logging.<br>
But I don&#8217;t know why but when I tried with the WAV file the memory is well released. So I think that is with the microphone. I don&#8217;t know why I can give you privately the WAV file that I can&#8217;t replicate just to listen to it if you want.</p>
<p>For the instrument file . I can share it with you privately if you want.</p>
<p>So, when I will be able to replicate I will come back and share it with you.</p>
<p>Regards,<br>
Laurent</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1030171" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 22, 2016 at 4:00 pm</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1030171" class="bbp-reply-permalink">#1030171</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1030171 -->

<div class="loop-item-41 user-id-2022 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-53 even topic-author  post-1030171 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/laurent-oscadi/" title="View Laurent&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Laurent</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>This is a link with the instruments, the project and the dmg framework:<br>
<a href="https://www.dropbox.com/s/so9yfbbv6awwu7j/PP.zip?dl=0" rel="nofollow">https://www.dropbox.com/s/so9yfbbv6awwu7j/PP.zip?dl=0</a></p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1030172" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 22, 2016 at 4:27 pm</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1030172" class="bbp-reply-permalink">#1030172</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1030172 -->

<div class="loop-item-42 user-id-4 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-54 odd  post-1030172 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/halle-2/" title="View Halle Winkler&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Halle Winkler</span></a><div class="bbp-author-role">Politepix</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<blockquote><p>I believe that it is possible that there is an extraordinary bug of some kind which causes the symptoms of just the RapidEars framework not logging just some of its logging output, so I am not ruling that out, but it seems like a lower probability than linking to an old framework right now.</p></blockquote>
<p>It is an extraordinary bug of some kind which causes the symptoms of just the RapidEars framework not logging just some of its logging output :/ . When both OELogging and verbosePocketsphinx are on, verbosePocketsphinx suppresses the earliest part of the RapidEars output. Sorry it was difficult to pin this down and thank you for bringing it to my attention. I will try to fix that in the next version but for now you can easily verify the version by running OELogging with verbosePocketsphinx turned off.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1030173" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 22, 2016 at 4:29 pm</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1030173" class="bbp-reply-permalink">#1030173</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1030173 -->

<div class="loop-item-43 user-id-4 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-55 even  post-1030173 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/halle-2/" title="View Halle Winkler&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Halle Winkler</span></a><div class="bbp-author-role">Politepix</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>OK, the next thing I noticed is that you have </p>
<pre><code>  [OEPocketsphinxController sharedInstance].legacy3rdPassMode = TRUE ;
</code></pre>
<p>set before you call this message:</p>
<pre><code>    [[OEPocketsphinxController sharedInstance] setActive:TRUE error:nil];
</code></pre>
<p>But setActive has to come before any property setting.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1030174" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 22, 2016 at 4:46 pm</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1030174" class="bbp-reply-permalink">#1030174</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1030174 -->

<div class="loop-item-44 user-id-4 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-56 odd  post-1030174 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/halle-2/" title="View Halle Winkler&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Halle Winkler</span></a><div class="bbp-author-role">Politepix</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>OK, is the saved Instruments output file made from running the app you sent when using the WAV test file pocketsphinx_sample_log_201604221141402.wav?</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1030227" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 29, 2016 at 5:52 am</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1030227" class="bbp-reply-permalink">#1030227</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1030227 -->

<div class="loop-item-45 user-id-2022 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-57 even topic-author  post-1030227 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/laurent-oscadi/" title="View Laurent&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Laurent</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Hello Mr Halle,<br>
I had a problem with Xcode which didn&#8217;t link the framework so I had to reset Xcode in order to solve it.<br>
I have checked for the RapidEars 2.5 logging and when I turned off the pocketSphinx verbose I can see the log.<br>
I haven&#8217;t use this wav file to make this instruments.<br>
When I will be able to reproduce the bug I will share it with you.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1030228" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">April 29, 2016 at 7:10 am</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1030228" class="bbp-reply-permalink">#1030228</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1030228 -->

<div class="loop-item-46 user-id-4 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-58 odd  post-1030228 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/halle-2/" title="View Halle Winkler&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Halle Winkler</span></a><div class="bbp-author-role">Politepix</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Hi Laurent,</p>
<p>Thank you, I will look at the new version whenever you have it for me. You can erase the other upload since I have a copy.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1030342" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">May 17, 2016 at 10:28 pm</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1030342" class="bbp-reply-permalink">#1030342</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1030342 -->

<div class="loop-item-47 user-id-4 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-59 even  post-1030342 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/halle-2/" title="View Halle Winkler&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Halle Winkler</span></a><div class="bbp-author-role">Politepix</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>OK, although I wasn&#8217;t able to reproduce this exactly from the sample, I was able to get a similar reproduction on an older device with a different setup, so I am currently investigating this issue, thank you for all of the info.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1030375" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">May 23, 2016 at 7:28 am</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1030375" class="bbp-reply-permalink">#1030375</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1030375 -->

<div class="loop-item-48 user-id-2022 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-60 odd topic-author  post-1030375 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/laurent-oscadi/" title="View Laurent&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Laurent</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Hi, Mr Halle,<br>
I&#8217;m glad to heard that and to help you. Please notify me if you find the problem. Thank you.<br>
Regards,<br>
Laurent.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1030554" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">June 7, 2016 at 6:22 pm</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1030554" class="bbp-reply-permalink">#1030554</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1030554 -->

<div class="loop-item-49 user-id-4 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-50 even  post-1030554 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/halle-2/" title="View Halle Winkler&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Halle Winkler</span></a><div class="bbp-author-role">Politepix</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Hi Laurent,</p>
<p>I&#8217;ve removed a couple of our side discussions in this thread so it&#8217;s easier for later readers who need to get an overview on related issues to get through it quickly – hope you don&#8217;t mind since the extra discussion was my fault. Today there is a new OpenEars and RapidEars version 2.502 (more info at <a href="https://changelogs.politepix.com/" rel="nofollow">https://changelogs.politepix.com</a> and downloadable from <a href="/openears">https://www.politepix.com/openears</a> and your registered framework customer account) which should fix this issue you&#8217;ve reported. Before we talk about it more I wanted to clarify what this update fixes. We&#8217;ve talked about four different things in this discussion:</p>
<p>1. Actual leaks in Sphinx which we&#8217;ve established are very tiny,<br>
2. Normally-increasing memory usage from OpenEars due to a growing buffer size from longer utterances,<br>
3. The usage of much larger amounts of memory which is then normally released after there is a hypothesis,<br>
4. Large amounts of memory not reclaimable after stopping when there is a big search in progress during the attempted stop.</p>
<p>I think we covered 1 &#038; 2 pretty well earlier in the discussion, so let&#8217;s agree to just discuss 3 &#038; 4 now that the updates are out, if that&#8217;s OK.</p>
<p>The OpenEars and RapidEars 2.502 updates should fix #4, which is a serious bug and which I&#8217;m really happy you told me about and showed me an example of, thank you. In general the updates should also allow faster stops when there is a big search in progress at stopping time, even setting aside the memory usage. Please be so kind as to check this out thoroughly and let me know if it stops the bad memory events at stopping time, and also let me know if you see anything bad due to the changes. The one case which was in your replication cases but which isn&#8217;t necessary to test or report is what happens to memory when the app hangs or exits due to the demo framework timing out – this is expected to not be graceful, so the memory usage under those circumstances at the very end of the app session isn&#8217;t an issue. In your case you should be able to test against your registered 2.502 framework instead of the demo.</p>
<p>#3 is a more complicated subject and not really a bug as far as I can see, so I wanted to explain it a little bit. I couldn&#8217;t actually replicate the situation you were seeing with extremely large allocations during an utterance, although I worked very hard to do so, setting up an external speaker system so I could play your office audio out loud into various device microphones since it didn&#8217;t replicate with test file playback. I couldn&#8217;t ever get the big memory usage to replicate, but I could see some smaller allocations which were nonetheless bigger than I would have preferred. I believe this is a bit more of an implementation issue than a bug, with a couple of root causes:</p>
<p>• There are some strange noises with unusual echo and doppler in the recordings. I don&#8217;t know whether there is some kind of industrial noise in the background where you work, whether this is an artifact of the device mic (it could be a strange result of echo cancelling past a certain physical distance from the mic or similar) or even if it is an artifact of SaveThatWave, but I&#8217;ve never heard it before on SaveThatWave recordings so I think it was either really there in the environment or it is a peculiarity of the mic and hardware and usage distance. In any case, this type of audio artifact causes unexpected results with speech recognition and I&#8217;ve had the experience that it adds confusion to word searches.</p>
<p>• In the code you shared, the jobs of vadThreshold and Rejecto weight are reversed. Normally you want the highest possible vadThreshold which still allows intentional speech to be perceived by OpenEars, then you add Rejecto to work against real speech that isn&#8217;t part of your model, and then after adding Rejecto, in relatively uncommon cases, you can increase the weight a little. In this code, the vadThreshold is left at the default although it is resulting in all environmental sounds being treated as speech (leading to all the null hyps in every recognition round), and then there is the maximum possible Rejecto weight so that nearly all of the speech (which is really incidental noise) is first completely processed and then rejected. In RapidEars, this results in very large search spaces, because every noise is a potential word, but every word has to be analyzed using the smallest possible speech units which can occur in any combination, because your actual vocabulary is weighted very low in probability, and reject-able sounds are rated very high due to the weighting. In combination with the odd noises, this leads to the big, slow hypothesis searches as a result of non-speech, which can be seen in the logs and the profile. Although I couldn&#8217;t replicate the memory usage, I believe it is happening, and I think it is due to this circumstance. </p>
<p>It is my expectation that if you turn off Rejecto and first find the right vadThreshold (probably at least 2.5) and then afterwards add in a normally-weighted Rejecto model, you should see more normal memory usage and probably more accuracy. I have made a decision not to make code changes for #3 because it would have big side-effects, and I think it is due to a circumstance which would be better to address via implementation. I am still open to seeing an example which replicates consistently from a test file and giving it more consideration, but so far I haven&#8217;t been able to witness it directly so my sense is that it is bound to the environment and the vadThreshold/weight issue.</p>
<p>Let me know how the new stopping behavior works for you, and thanks again for providing so much info about this bug so I could fix it.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
				
<div id="post-1030563" class="bbp-reply-header">
	<div class="bbp-meta">
		<span class="bbp-reply-post-date">June 8, 2016 at 6:18 am</span>

		
		<a href="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/#post-1030563" class="bbp-reply-permalink">#1030563</a>

		
		<span class="bbp-admin-links"></span>
		
	</div>
<!-- .bbp-meta -->
</div>
<!-- #post-1030563 -->

<div class="loop-item-50 user-id-2022 bbp-parent-forum-9992 bbp-parent-topic-1029883 bbp-reply-position-51 odd topic-author  post-1030563 reply type-reply status-publish hentry">
	<div class="bbp-reply-author">

		
		<a href="/forums/profile/laurent-oscadi/" title="View Laurent&#039;s profile" class="bbp-author-link"><span class="bbp-author-avatar"></span><span class="bbp-author-name">Laurent</span></a><div class="bbp-author-role">Participant</div>
		
		
	</div>
<!-- .bbp-reply-author -->

	<div class="bbp-reply-content">

		
		<p>Hi Mr Halle,</p>
<p>First, I want to thank you for your help.<br>
I didn&#8217;t know the &#8220;vadThreshold&#8221; parameter. I will definitely test it.<br>
I will test the updates and share my report with you as soon as possible.</p>
<p>Regards,<br>
Laurent.</p>

		
	</div>
<!-- .bbp-reply-content -->
</div>
<!-- .reply -->

			
		
	</li>
<!-- .bbp-body -->

	<li class="bbp-footer">
		<div class="bbp-reply-author">Author</div>
		<div class="bbp-reply-content">Posts</div>
<!-- .bbp-reply-content -->
	</li>
<!-- .bbp-footer -->
</ul>
<!-- #topic-1029883-replies -->


			
<div class="bbp-pagination">
	<div class="bbp-pagination-count">Viewing 51 posts - 1 through 51 (of 51 total)</div>
	<div class="bbp-pagination-links"></div>
</div>


		
		

	<div id="no-reply-1029883" class="bbp-no-reply">
		<div class="bbp-template-notice">
			<ul>
				<li>You must be logged in to reply to this topic.</li>
			</ul>
		</div>

		
			
<form method="post" action="https://www.politepix.com/wp-login.php" class="bbp-login-form">
	<fieldset class="bbp-form">
		<legend>Log In</legend>

		<div class="bbp-username">
			<label for="user_login">Username: </label>
			<input type="text" name="log" value="" size="20" maxlength="100" id="user_login" autocomplete="off">
		</div>

		<div class="bbp-password">
			<label for="user_pass">Password: </label>
			<input type="password" name="pwd" value="" size="20" id="user_pass" autocomplete="off">
		</div>

		<div class="bbp-remember-me">
			<input type="checkbox" name="rememberme" value="forever" id="rememberme">
			<label for="rememberme">Keep me signed in</label>
		</div>

		
		<div class="bbp-submit-wrapper">

			<button type="submit" name="user-submit" id="user-submit" class="button submit user-submit">Log In</button>

			
	<input type="hidden" name="user-cookie" value="1">

	<input type="hidden" id="bbp_redirect_to" name="redirect_to" value="https://www.politepix.com/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/?simply_static_page=747"><input type="hidden" id="_wpnonce" name="_wpnonce" value="7d2a08c1e3"><input type="hidden" name="_wp_http_referer" value="/forums/topic/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks/?simply_static_page=747">
		</div>
	</fieldset>
</form>

		
	</div>



	
	

	
</div>

					               	</section><!-- /.entry -->

				                
            </article><!-- /.post -->
            
              
        
		</section><!-- /#main -->
		
		
        	
<aside id="sidebar" class="col-right">

	
	    <div class="primary">
		<div id="bbp_forums_widget-5" class="widget widget_display_forums">
<h3>Forums</h3>
		<ul class="bbp-forums-widget">

			
				<li class="bbp-forum-widget-current-forum">
					<a class="bbp-forum-title" href="/forums/forum/openears-plugins/">
						OpenEars plugins					</a>
				</li>

			
				<li>
					<a class="bbp-forum-title" href="/forums/forum/openearsforum/">
						OpenEars					</a>
				</li>

			
		</ul>

		</div>		           
	</div>        
	   
	
	 
	
</aside><!-- /#sidebar -->

    </div>
<!-- /#content -->
		
		<footer id="footer" class="col-full">
	
			<div id="copyright" class="col-left">
							<p>Politepix &copy; 2024. All Rights Reserved.</p>
						</div>
	
			<div id="credit" class="col-right">
	        <div style="text-align:right;line-height:1.3em">OpenEars® is a registered trademark of Politepix<br>AllHours® is a registered trademark of Politepix<br>The Politepix site uses cookies in order to understand how the website is used by visitors and in order to enable some required functionality. You can learn all about which cookies we use on the <a href="/about/">About</a> page, as well as everything about our privacy policy.<br>
<a href="https://twitter.com/Politepix" rel="me">TWITTER</a> | <a href="/contact" id="impressumlink">CONTACT POLITEPIX</a><a href="/about" id="impressumlink"> | IMPRESSUM | ABOUT | LEGAL | IMPRINT</a>
</div>			</div>
	<a rel="me" href="https://mastodon.social/@Halle">M</a>
		</footer><!-- /#footer  -->

</div>
<!-- /#wrapper -->

<!-- Consent Management powered by Complianz | GDPR/CCPA Cookie Consent https://wordpress.org/plugins/complianz-gdpr -->
<div id="cmplz-cookiebanner-container">
<div class="cmplz-cookiebanner cmplz-hidden banner-1 bottom-right-view-preferences optin cmplz-bottom-right cmplz-categories-type-view-preferences" aria-modal="true" data-nosnippet="true" role="dialog" aria-live="polite" aria-labelledby="cmplz-header-1-optin" aria-describedby="cmplz-message-1-optin">
	<div class="cmplz-header">
		<div class="cmplz-logo"></div>
		<div class="cmplz-title" id="cmplz-header-1-optin">Manage Cookie Consent</div>
		<div class="cmplz-close" tabindex="0" role="button" aria-label="Close dialog">
			<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="times" class="svg-inline--fa fa-times fa-w-11" role="img" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 352 512"><path fill="currentColor" d="M242.72 256l100.07-100.07c12.28-12.28 12.28-32.19 0-44.48l-22.24-22.24c-12.28-12.28-32.19-12.28-44.48 0L176 189.28 75.93 89.21c-12.28-12.28-32.19-12.28-44.48 0L9.21 111.45c-12.28 12.28-12.28 32.19 0 44.48L109.28 256 9.21 356.07c-12.28 12.28-12.28 32.19 0 44.48l22.24 22.24c12.28 12.28 32.2 12.28 44.48 0L176 322.72l100.07 100.07c12.28 12.28 32.2 12.28 44.48 0l22.24-22.24c12.28-12.28 12.28-32.19 0-44.48L242.72 256z"></path></svg>
		</div>
	</div>

	<div class="cmplz-divider cmplz-divider-header"></div>
	<div class="cmplz-body">
		<div class="cmplz-message" id="cmplz-message-1-optin">To provide the best experiences, we use technologies like cookies to store and/or access device information. Consenting to these technologies will allow us to process data such as browsing behavior or unique IDs on this site. Not consenting or withdrawing consent, may adversely affect certain features and functions.</div>
		<!-- categories start -->
		<div class="cmplz-categories">
			<details class="cmplz-category cmplz-functional">
				<summary>
						<span class="cmplz-category-header">
							<span class="cmplz-category-title">Functional</span>
							<span class="cmplz-always-active">
								<span class="cmplz-banner-checkbox">
									<input type="checkbox" id="cmplz-functional-optin" data-category="cmplz_functional" class="cmplz-consent-checkbox cmplz-functional" size="40" value="1">
									<label class="cmplz-label" for="cmplz-functional-optin" tabindex="0"><span class="screen-reader-text">Functional</span></label>
								</span>
								Always active							</span>
							<span class="cmplz-icon cmplz-open">
								<svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 448 512" height="18"><path d="M224 416c-8.188 0-16.38-3.125-22.62-9.375l-192-192c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L224 338.8l169.4-169.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-192 192C240.4 412.9 232.2 416 224 416z"></path></svg>
							</span>
						</span>
				</summary>
				<div class="cmplz-description">
					<span class="cmplz-description-functional">The technical storage or access is strictly necessary for the legitimate purpose of enabling the use of a specific service explicitly requested by the subscriber or user, or for the sole purpose of carrying out the transmission of a communication over an electronic communications network.</span>
				</div>
			</details>

			<details class="cmplz-category cmplz-preferences">
				<summary>
						<span class="cmplz-category-header">
							<span class="cmplz-category-title">Preferences</span>
							<span class="cmplz-banner-checkbox">
								<input type="checkbox" id="cmplz-preferences-optin" data-category="cmplz_preferences" class="cmplz-consent-checkbox cmplz-preferences" size="40" value="1">
								<label class="cmplz-label" for="cmplz-preferences-optin" tabindex="0"><span class="screen-reader-text">Preferences</span></label>
							</span>
							<span class="cmplz-icon cmplz-open">
								<svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 448 512" height="18"><path d="M224 416c-8.188 0-16.38-3.125-22.62-9.375l-192-192c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L224 338.8l169.4-169.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-192 192C240.4 412.9 232.2 416 224 416z"></path></svg>
							</span>
						</span>
				</summary>
				<div class="cmplz-description">
					<span class="cmplz-description-preferences">The technical storage or access is necessary for the legitimate purpose of storing preferences that are not requested by the subscriber or user.</span>
				</div>
			</details>

			<details class="cmplz-category cmplz-statistics">
				<summary>
						<span class="cmplz-category-header">
							<span class="cmplz-category-title">Statistics</span>
							<span class="cmplz-banner-checkbox">
								<input type="checkbox" id="cmplz-statistics-optin" data-category="cmplz_statistics" class="cmplz-consent-checkbox cmplz-statistics" size="40" value="1">
								<label class="cmplz-label" for="cmplz-statistics-optin" tabindex="0"><span class="screen-reader-text">Statistics</span></label>
							</span>
							<span class="cmplz-icon cmplz-open">
								<svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 448 512" height="18"><path d="M224 416c-8.188 0-16.38-3.125-22.62-9.375l-192-192c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L224 338.8l169.4-169.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-192 192C240.4 412.9 232.2 416 224 416z"></path></svg>
							</span>
						</span>
				</summary>
				<div class="cmplz-description">
					<span class="cmplz-description-statistics">The technical storage or access that is used exclusively for statistical purposes.</span>
					<span class="cmplz-description-statistics-anonymous">The technical storage or access that is used exclusively for anonymous statistical purposes. Without a subpoena, voluntary compliance on the part of your Internet Service Provider, or additional records from a third party, information stored or retrieved for this purpose alone cannot usually be used to identify you.</span>
				</div>
			</details>
			<details class="cmplz-category cmplz-marketing">
				<summary>
						<span class="cmplz-category-header">
							<span class="cmplz-category-title">Marketing</span>
							<span class="cmplz-banner-checkbox">
								<input type="checkbox" id="cmplz-marketing-optin" data-category="cmplz_marketing" class="cmplz-consent-checkbox cmplz-marketing" size="40" value="1">
								<label class="cmplz-label" for="cmplz-marketing-optin" tabindex="0"><span class="screen-reader-text">Marketing</span></label>
							</span>
							<span class="cmplz-icon cmplz-open">
								<svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 448 512" height="18"><path d="M224 416c-8.188 0-16.38-3.125-22.62-9.375l-192-192c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L224 338.8l169.4-169.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-192 192C240.4 412.9 232.2 416 224 416z"></path></svg>
							</span>
						</span>
				</summary>
				<div class="cmplz-description">
					<span class="cmplz-description-marketing">The technical storage or access is required to create user profiles to send advertising, or to track the user on a website or across several websites for similar marketing purposes.</span>
				</div>
			</details>
		</div>
<!-- categories end -->
			</div>

	<div class="cmplz-links cmplz-information">
		<a class="cmplz-link cmplz-manage-options cookie-statement" href="#" data-relative_url="#cmplz-manage-consent-container">Manage options</a>
		<a class="cmplz-link cmplz-manage-third-parties cookie-statement" href="#" data-relative_url="#cmplz-cookies-overview">Manage services</a>
		<a class="cmplz-link cmplz-manage-vendors tcf cookie-statement" href="#" data-relative_url="#cmplz-tcf-wrapper">Manage {vendor_count} vendors</a>
		<a class="cmplz-link cmplz-external cmplz-read-more-purposes tcf" target="_blank" rel="noopener noreferrer nofollow" href="https://cookiedatabase.org/tcf/purposes/">Read more about these purposes</a>
			</div>

	<div class="cmplz-divider cmplz-footer"></div>

	<div class="cmplz-buttons">
		<button class="cmplz-btn cmplz-accept">Accept</button>
		<button class="cmplz-btn cmplz-deny">Deny</button>
		<button class="cmplz-btn cmplz-view-preferences">View preferences</button>
		<button class="cmplz-btn cmplz-save-preferences">Save preferences</button>
		<a class="cmplz-btn cmplz-manage-options tcf cookie-statement" href="#" data-relative_url="#cmplz-manage-consent-container">View preferences</a>
			</div>

	<div class="cmplz-links cmplz-documents">
		<a class="cmplz-link cookie-statement" href="#" data-relative_url="">{title}</a>
		<a class="cmplz-link privacy-statement" href="#" data-relative_url="">{title}</a>
		<a class="cmplz-link impressum" href="#" data-relative_url="">{title}</a>
			</div>

</div>
</div>
					<div id="cmplz-manage-consent" data-nosnippet="true">
<button class="cmplz-btn cmplz-hidden cmplz-manage-consent manage-consent-1">Manage consent</button>

</div>
<!--[if lt IE 9]>
<script src="https://www.politepix.com/wp-content//themes/pixelpress/includes/js/respond-IE.js"></script>
<![endif]-->
			<script>jQuery(document).ready(function(){
					jQuery('.images a').attr('rel', 'prettyPhoto[product-gallery]');
				});</script>
			<script type="text/javascript">(function () {
			var c = document.body.className;
			c = c.replace(/woocommerce-no-js/, 'woocommerce-js');
			document.body.className = c;
		})();</script>
	<link rel="stylesheet" id="wc-blocks-style-css" href="https://c0.wp.com/p/woocommerce/8.8.2/assets/client/blocks/wc-blocks.css" type="text/css" media="all">
<script type="text/javascript" src="https://c0.wp.com/p/woocommerce/8.8.2/assets/js/accounting/accounting.min.js" id="accounting-js"></script>
<script type="text/javascript" src="https://c0.wp.com/c/6.5.2/wp-includes/js/jquery/ui/core.min.js" id="jquery-ui-core-js"></script>
<script type="text/javascript" src="https://c0.wp.com/c/6.5.2/wp-includes/js/jquery/ui/datepicker.min.js" id="jquery-ui-datepicker-js"></script>
<script type="text/javascript" id="jquery-ui-datepicker-js-after">
/* <![CDATA[ */
jQuery(function(jQuery){jQuery.datepicker.setDefaults({"closeText":"Close","currentText":"Today","monthNames":["January","February","March","April","May","June","July","August","September","October","November","December"],"monthNamesShort":["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"nextText":"Next","prevText":"Previous","dayNames":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"],"dayNamesShort":["Sun","Mon","Tue","Wed","Thu","Fri","Sat"],"dayNamesMin":["S","M","T","W","T","F","S"],"dateFormat":"MM d, yy","firstDay":1,"isRTL":false});});
/* ]]> */
</script>
<script type="text/javascript" id="woocommerce-addons-js-extra">
/* <![CDATA[ */
var woocommerce_addons_params = {"price_display_suffix":"","tax_enabled":"1","price_include_tax":"","display_include_tax":"","ajax_url":"\/wp-admin\/admin-ajax.php","i18n_validation_required_select":"Please choose an option.","i18n_validation_required_input":"Please enter some text in this field.","i18n_validation_required_number":"Please enter a number in this field.","i18n_validation_required_file":"Please upload a file.","i18n_validation_letters_only":"Please enter letters only.","i18n_validation_numbers_only":"Please enter numbers only.","i18n_validation_letters_and_numbers_only":"Please enter letters and numbers only.","i18n_validation_email_only":"Please enter a valid email address.","i18n_validation_min_characters":"Please enter at least %c characters.","i18n_validation_max_characters":"Please enter up to %c characters.","i18n_validation_min_number":"Please enter %c or more.","i18n_validation_max_number":"Please enter %c or less.","i18n_sub_total":"Subtotal","i18n_remaining":"<span><\/span> characters remaining","currency_format_num_decimals":"2","currency_format_symbol":"€","currency_format_decimal_sep":".","currency_format_thousand_sep":",","trim_trailing_zeros":"","is_bookings":"","trim_user_input_characters":"1000","quantity_symbol":"x ","datepicker_class":"wc_pao_datepicker","datepicker_date_format":"MM d, yy","gmt_offset":"-2","date_input_timezone_reference":"default","currency_format":"%s%v"};
/* ]]> */
</script>
<script type="text/javascript" src="https://www.politepix.com/wp-content/plugins/woocommerce-product-addons/assets/js/frontend/addons.min.js?ver=6.8.2" id="woocommerce-addons-js" defer data-wp-strategy="defer"></script>
<script type="text/javascript" src="https://www.politepix.com/wp-content/plugins/jetpack/jetpack_vendor/automattic/jetpack-image-cdn/dist/image-cdn.js?minify=false&amp;ver=132249e245926ae3e188" id="jetpack-photon-js"></script>
<script type="text/javascript" src="https://www.politepix.com/wp-content/plugins/bbpress/templates/default/js/editor.min.js?ver=2.6.9" id="bbpress-editor-js"></script>
<script type="text/javascript" id="bbpress-engagements-js-extra">
/* <![CDATA[ */
var bbpEngagementJS = {"object_id":"1029883","bbp_ajaxurl":"\/forums\/topic\/rapidears-startrealtimelisteningwithlanguagemodelatpath-memory-leaks\/?bbp-ajax=true","generic_ajax_error":"Something went wrong. Refresh your browser and try again."};
/* ]]> */
</script>
<script type="text/javascript" src="https://www.politepix.com/wp-content/plugins/bbpress/templates/default/js/engagements.min.js?ver=2.6.9" id="bbpress-engagements-js"></script>
<script type="text/javascript" src="https://c0.wp.com/p/woocommerce/8.8.2/assets/js/sourcebuster/sourcebuster.min.js" id="sourcebuster-js-js"></script>
<script type="text/javascript" id="wc-order-attribution-js-extra">
/* <![CDATA[ */
var wc_order_attribution = {"params":{"lifetime":1.0e-5,"session":30,"ajaxurl":"\/wp-admin\/admin-ajax.php","prefix":"wc_order_attribution_","allowTracking":true},"fields":{"source_type":"current.typ","referrer":"current_add.rf","utm_campaign":"current.cmp","utm_source":"current.src","utm_medium":"current.mdm","utm_content":"current.cnt","utm_id":"current.id","utm_term":"current.trm","session_entry":"current_add.ep","session_start_time":"current_add.fd","session_pages":"session.pgs","session_count":"udata.vst","user_agent":"udata.uag"}};
/* ]]> */
</script>
<script type="text/javascript" src="https://c0.wp.com/p/woocommerce/8.8.2/assets/js/frontend/order-attribution.min.js" id="wc-order-attribution-js"></script>
<script data-service="jetpack-statistics" data-category="statistics" type="text/plain" data-cmplz-src="https://stats.wp.com/e-202417.js" id="jetpack-stats-js" data-wp-strategy="defer"></script>
<script type="text/javascript" id="jetpack-stats-js-after">
/* <![CDATA[ */
_stq = window._stq || [];
_stq.push([ "view", JSON.parse("{\"v\":\"ext\",\"blog\":\"206848719\",\"post\":\"1029883\",\"tz\":\"2\",\"srv\":\"www.politepix.com\",\"j\":\"1:13.3.1\"}") ]);
_stq.push([ "clickTrackerInit", "206848719", "1029883" ]);
/* ]]> */
</script>
<script type="text/javascript" id="cmplz-cookiebanner-js-extra">
/* <![CDATA[ */
var complianz = {"prefix":"cmplz_","user_banner_id":"1","set_cookies":[],"block_ajax_content":"","banner_version":"18","version":"7.0.4","store_consent":"","do_not_track_enabled":"1","consenttype":"optin","region":"eu","geoip":"","dismiss_timeout":"","disable_cookiebanner":"","soft_cookiewall":"","dismiss_on_scroll":"","cookie_expiry":"365","url":"\/wp-json\/complianz\/v1\/","locale":"lang=en&locale=en_US","set_cookies_on_root":"","cookie_domain":"","current_policy_id":"16","cookie_path":"\/","categories":{"statistics":"statistics","marketing":"marketing"},"tcf_active":"","placeholdertext":"Click to accept {category} cookies and enable this content","css_file":"\/wp-content\/uploads\/complianz\/css\/banner-{banner_id}-{type}.css?v=18","page_links":{"eu":{"cookie-statement":{"title":"About &#038; Impressum &#038; Privacy Policy","url":"\/about\/"},"privacy-statement":{"title":"About &#038; Impressum &#038; Privacy Policy","url":"\/about\/"},"impressum":{"title":"About &#038; Impressum &#038; Privacy Policy","url":"\/about\/"}},"us":{"impressum":{"title":"About &#038; Impressum &#038; Privacy Policy","url":"\/about\/"}},"uk":{"impressum":{"title":"About &#038; Impressum &#038; Privacy Policy","url":"\/about\/"}},"ca":{"impressum":{"title":"About &#038; Impressum &#038; Privacy Policy","url":"\/about\/"}},"au":{"impressum":{"title":"About &#038; Impressum &#038; Privacy Policy","url":"\/about\/"}},"za":{"impressum":{"title":"About &#038; Impressum &#038; Privacy Policy","url":"\/about\/"}},"br":{"impressum":{"title":"About &#038; Impressum &#038; Privacy Policy","url":"\/about\/"}}},"tm_categories":"","forceEnableStats":"","preview":"","clean_cookies":"","aria_label":"Click to accept {category} cookies and enable this content"};
/* ]]> */
</script>
<script defer type="text/javascript" src="https://www.politepix.com/wp-content/plugins/complianz-gdpr/cookiebanner/js/complianz.min.js?ver=1710283454" id="cmplz-cookiebanner-js"></script>
</body>
</html>